Paper Title ,Abstract,Review Text,Review Type
HFSA: hybrid feature selection approach to improve medical diagnostic system,"Thanks to the presence of artificial intelligence methods, the diagnosis of patients can be done quickly and accurately. This article introduces a new diagnostic system (DS) that includes three main layers called the rejection layer (RL), selection layer (SL), and diagnostic layer (DL) to accurately diagnose cases suffering from various diseases. In RL, outliers can be removed using the genetic algorithm (GA). At the same time, the best features can be selected by using a new feature selection method called the hybrid feature selection approach (HFSA) in SL. In the next step, the filtered data is passed to the naive Bayes (NB) classifier in DL to give accurate diagnoses. In this work, the main contribution is represented in introducing HFSA as a new selection approach that is composed of two main stages; fast stage (FS) and accurate stage (AS). In FS, chi-square, as a filtering methodology, is applied to quickly select the best features while Hybrid Optimization Algorithm (HOA), as a wrapper methodology, is applied in AS to accurately select features. It is concluded that HFSA is better than other selection methods based on experimental results because HFSA can enable three different classifiers called NB, K-nearest neighbors (KNN), and artificial neural network (ANN) to provide the maximum accuracy, precision, and recall values and the minimum error value. Additionally, experimental results proved that DS, including GA as an outlier rejection method, HFSA as feature selection, and NB as diagnostic mode, outperformed other diagnosis models.","Basic reporting
This paper introduces an advanced Diagnostic System (DS) leveraging Artificial Intelligence (AI) techniques for precise and efficient medical diagnostics. The proposed system incorporates a novel Hybrid Feature Selection Approach (HFSA) that combines filter-based and wrapper-based methodologies to optimize feature selection and enhance the performance of the Naive Bayes (NB) classifier. Through its multi-layered structure—Rejection Layer (RL), Selection Layer (SL), and Diagnostic Layer (DL)—the system demonstrates significant improvements in diagnostic accuracy, precision, and recall. The revision should focus on enhancing the scientific rigor of the methodology, clarifying certain ambiguous details, and strengthening the experimental validation.

a. The novelty and computational feasibility of combining Genetic Algorithm (GA) with the Tiki-Taka Algorithm (T²A) in the Accurate Stage (AS) of HFSA should be elaborated. Include an analysis of why this hybrid optimization is superior to existing methods for feature selection in medical diagnostics.

b. Clarify the choice of chi-square as the filter methodology in the Fast Stage (FS). Provide a justification of its suitability compared to other statistical methods, particularly in handling high-dimensional medical datasets.

c. Expand the explanation of the Rejection Layer (RL) to include details about how GA identifies and removes outliers. Specify whether the definition of outliers is domain-specific or generalized across datasets.

d. Incorporate a thorough evaluation of HFSA against state-of-the-art feature selection approaches. Compare their computational complexity, runtime efficiency, and diagnostic performance to substantiate the claim that HFSA outperforms existing methods.

e. The experimental setup requires more details, including dataset characteristics, parameter configurations, and cross-validation techniques used. This ensures reproducibility and provides transparency for performance metrics such as accuracy, precision, recall, and error rates.

f. The integration of the NB classifier within the Diagnostic Layer (DL) should be critically assessed. Discuss the trade-offs in using NB versus more advanced classifiers like Support Vector Machines (SVM) or ensemble methods in the context of medical diagnostics.

g. Add a detailed discussion of potential limitations of the proposed system. For example, how sensitive is HFSA to changes in data distribution, class imbalance, or noisy features in real-world medical datasets?

h. The paper should include a future research direction section that addresses the scalability of the proposed system, particularly when dealing with rapidly growing datasets or integrating multi-modal medical data (e.g., text, images, and numerical records).

The Literature citation is not adequate, and the related work to machine learning should be discussed:
1. Robust semi-supervised multi-label feature selection based on shared subspace and manifold learning
2. Sparse feature selection using hypergraph Laplacian-based semi-supervised discriminant analysis
3. Nonnegative Matrix Factorization in Dimensionality Reduction: A Survey

Experimental design
not

Validity of the findings
not",Authentic
Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation,"In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments.","Basic reporting
The paper is well-written and easy to follow. However, it would benefit from more emphasis on the rationale behind the development of the proposed method. While the paper thoroughly explains ""what"" was done, the reasoning behind the specific design choices remains unclear. Further clarification on this would strengthen the work. See below for additional details.

Experimental design
To summarize the contribution, the paper adapts the method from Kim et al. (2010), originally designed for simulating small bubbles in water, and applies it to simulate fire flakes using a relatively simple fire model from Nguyen et al. (2002). This contribution seems somewhat minimal, especially considering the length of the paper.

The authors make several claims about why the motion of fire flakes is fundamentally different from that of bubbles:
50: ""On the other hand, fire-flake particles, being lighter than air and significantly influenced by air resistance, exhibit relatively more chaotic and disorderly movement.""
135: ""Most existing methods have focused on representing bubbles in water, and there has been relatively less research specifically dedicated to modeling fire-flake particles represented by flames.""
253: ""unlike air bubbles in water, near the flame, there are turbulent interactions between fuel and air that give rise to chaotic movements.""
447: ""Unlike air bubbles, fire-flake particles exhibit highly dynamic movements in chaotic regions.""
However, these assertions are not supported by evidence (citations would be helpful), and yet, they still proceed to apply an existing bubble simulation technique to model fire flakes, which appears contradictory.

Additionally, the paper proposes training a neural network to learn the emission, trajectories, and lifespan of fire flakes. However, the training uses synthetic data generated by the paper's own simulation method, which raises the question of why an already established model is being ""re-learned.""

Validity of the findings
The main result of the paper shows that the proposed method generates more break-up in the motion of fire flake particles compared to previous methods that did not incorporate chaotic advection. While this outcome is expected, it is also unsurprising and aligns with the findings of Kim et al. (2010), who achieved something similar with bubbles.

The machine learning (ML) aspect of the paper is intriguing, but as noted earlier, its purpose remains unclear. The approach essentially re-learns an already established model (which, not surprisingly, yields similar results), and the paper does not demonstrate any performance benefits of using an ML technique over traditional simulation methods.

Additional comments
All of the simulations in the paper are conducted in 2D, and there is no supplemental video provided, making it difficult to evaluate the quality of the results.

Overall, I would consider the paper's contribution to be quite limited. It seems premature for publication and would require significant revisions.",Authentic
Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation,"In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments.","Basic reporting
The overall workflow of the work is appreciable.

Experimental design
Experimental design and the results provided is goo and sufficient.

Validity of the findings
The article meets the standard of the journal due to the experimental results given is good.",Generic
Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation,"In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments.","Basic reporting
This paper proposes an efficient technique to simulate dispersed fire-flake particles that respond to the movement of a flame. The paper is well-organized, and the findings are clearly represented. I would recommend the paper for publication after minor revision.
1. Check the comma and the punctuation in the text and at the ends of equations (Equations 3, 6 and the rest).
2. Check the equation number on line 215.
3. Tell us more about the numerical method used. What are the limitations of the method? Are there other robust methods? What motivates the choice of the current method to present the results.
4. The abstract should contain some obtained results through an ending sentence.
5. On Line 222, replace ‘In this paper’ by ‘In addition’ or any equivalent expression as it is written in the previous sentence.
6. Discuss the implications of your findings for real-world applications.

Experimental design
Yes

Validity of the findings
Yes

Additional comments
NO",Authentic
FLASC: a flare-sensitive clustering algorithm,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc.","Basic reporting
No comment
Experimental design
1. Why was the FLASC algorithm compared with the K-means algorithm, if it is known by the clustering community that this algorithm does not detect groups of irregular figures? I think that being very popular is not a good argument for the comparison.
2. What are the values of the parameters used by the DBSCAN* algorithm.
3. How was the optimal number of groups determined in the K-means algorithm?
4. It is recommended to compare the proposed algorithm with an OPTICS algorithm.
5. Could you explain and give arguments for the “Evaluation and settings” sections of all the cases presented.
6. Could you explain what is written in lines 402-405, why K=5, min cluster size Mc = 100…..
7. How was the efficiency of the clusters obtained by the three algorithms measured?
Validity of the findings
No comment",Authentic
FLASC: a flare-sensitive clustering algorithm,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc.","Basic reporting
1. The selection criteria of the algorithm parameters (e.g., distance threshold, minimum branch size, etc.) are not discussed in detail during the experiment. It is suggested to add the comparison experiment of the performance of the algorithm under different parameters to show the robustness and adaptability of the FLASC algorithm.

2. In the past three years, several studies have addressed the challenge of flare detection. Notable examples include, “Clustering by Measuring Local Direction Centrality for Data with Heterogeneous Density and Weak Connectivity” (Nature Communications), and TWStream (IEEE TFS). These algorithms merit further discussion.

Experimental design
1.The experimental comparison object focuses on a major density clustering algorithm (HDBSCAN*). In order to more fully demonstrate the superiority of FLASC, it is suggested to introduce other density clustering algorithms, such as, Block-Diagonal Guided DBSCAN Clustering(IEEE TKDE github: https://github.com/y66y/TKDE) and M3W (IEEE TNNLS github: https://github.com/Du-Team/M3W).

**PeerJ Staff Note:** It is PeerJ policy that additional references/work suggested during the peer-review process should only be included if the authors are in agreement that they are relevant and useful.

2.The current experimental datasets are mainly simulated data or datasets of specific nature, and lack the testing of real application scenarios. It is suggested to add some datasets from real applications to verify the practicality of FLASC.

Validity of the findings
The time complexity of FLASC algorithm is theoretically analyzed in the paper, so the time comparison experiments are very much expected.",Authentic
FLASC: a flare-sensitive clustering algorithm,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc.","Basic reporting
* There are certain terms that are not clear (at least to this reviewer) in the following lines:

- 14: ""their shape can represent"", please be more explicit in the paper what a shape is, provide a definition or an intuition of it.

- 17: in how far do branches 'describe' within-cluster connectivity? please elaborate on that within the paper

- 19: 'stable outputs', stable in which term/with respect to what? Please elaborate on that

- 20: 'flare sensitive' what is meant by that term? please define or provide intuitions for that within the text

- 25: 'traditional algorithms', what is understood by the authors via the term 'traditional'?

- 31: 'subpopulations': what is meant by that term? do you mean subsets of objects/data points?

- 36: ""Flares are connected in the simplicial complex"", please provide a definition of flares in context of topological data analysis (TDA)

- 55+56: ""branching hierarchies"" and ""branch-based subgroups of clusters"": Please provide a definition of these terms. Furthermore what is the difference in context of your manuscript regarding the terms subgroups and subpopulations?

- 62+63: good! nice reference of existing work for branch detection in 3d models of plants

- 70: ""...intuitive ... and branch sizes..."" in how far are branch sizes intuitive? please elaborate

- The elaboration on HDBSCAN* is very well done and elaborate!

Experimental design
- 155: ""distance between them is smaller than or equal to 1/lambda_t"", why is that the case or thresholded by that? please elaborate on that matter

- 178: ""no cluster selection epsilon"", what is meant by that? do you refer to an epsilon-neighborhood radius? please elaborate

-236: ""The eccentricity function (Equation (4)) is more complex to analyse..."" with respect to what? what makes it so complex? please elaborate

- You compare FLASC against k-means and HDBSCAN*, while the latter makes sense, since your proposed method shares many theoretical and algorithmic foundations, it remains unclear why you do not compare against other methods. Most important this reviewer recommends to compare also against Hierarchical Clustering (e.g. SLINK) and Spectral clustering. Both of them rely on trees/graphs as their underlying structures. Prior works by Hess et. al [The relationship of DBSCAN to matrix factorization and spectral clustering] and Beer et. al [Connecting the Dots--Density-Connectivity Distance unifies DBSCAN, k-Center and Spectral Clustering], also revealed commonalities between DBSCAN, Spectral Clustering.

**PeerJ Staff Note:** It is PeerJ policy that additional references suggested during the peer-review process should only be included if the authors are in agreement that they are relevant and useful.

Furthermore regarding the discovery of branches, this reviewer recommends to compare FLASC against 4C, a density-based clustering algorithm [Computing Clusters of Correlation Connected Objects] that is capable to detect linear correlations (to a certain degree discovering linear branches) within a given dataset.

To test your method against the proposed algorithms has the purpose to substantially underline the power of your method by comparing against other methods that should, in theory, be capable to detect it.

- while the figures with the ARI scores are convincing (cf. Fig. 4), a question that comes up is: how does FLASC behave if the shape of three branches is more like a Y-shape or in 3D two Y-shapes with a certain viccinity to each other. Please provide 2-3 more complex datasets in which FLASC is challenged to convince the readers that it is also capable to deal with different shapes and different proximities of clusters to each other

Validity of the findings
The used methods are well established and based on the datasets on which the experiments have been conducted they seem meaningful, however, as mentioned in 2. Experimental Design, more experiments are needed to investigate the capabilities and limitations of that method.",Authentic
FLASC: a flare-sensitive clustering algorithm,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc.","Basic reporting
This paper was written very well and has a good opportunity to be published in this great journal, after addressing the following concerns in the first round:

- I encourage you to add more detail about your core contributions in the abstract. Abstract has five-section and you should follow the best practices in your area! Please also mention the novelties in the abstract.
- Long paragraphs.
- Please bring some facts and figures in the introduction to support the ideas.

Experimental design
- Literature review is very short and old! We are in jan 2024! You have not covered the knowledge edge! Please clarify the contribution of the paper according to the research gap. Many recent papers in the area can be added to the literature review. You have not referred to the main works in this area. I do not propose you any special reference due to ethical issues. The authors should review the recent works in these areas. I simply search and find following works and similar algorithm like, TGA, SEO, RDA, HBA, FHO, etc. Do and implement your algorithm in professional way by needed experiments... Also, you should cover these algorithms in the LR and refer to the works as samples for their usages.

Also, you should cover these algorithms in the LR and refer to the works as samples for their usages.
HBA, KA, SEO, FHO, TGA, etc.

Validity of the findings
- Please add research gap section.


I go with a major revision in this step and waiting for your corrections. Then, I give you my technical comments.

Additional comments
- Check the English presentation of this paper to remove the typos mistakes.
- Findings, limitations, and recommendations of this paper can be discussed more in the conclusion section.
- Please bring and focus on future research directions.",Authentic
Dynamic prediction of carbon prices based on the multi-frequency combined model,"As a central participant and important leader in the global climate governance system, China is facing the urgent need to predict and regulate the price of carbon emissions to promote the sound development of its carbon market. In this article, a rolling prediction model based on Least Absolute Shrinkage and Selection Operator-cheetah optimization algorithm-extreme gradient boosting (Lasso-COA-XGBoost) carbon price decomposition integration is proposed to address the defects of low prediction accuracy and insufficient model stability of a single machine learning model in the carbon price prediction problem. During the modeling process, the adaptive Lasso method is first employed to select factors from 15 primary indicators of carbon prices, identifying the most important influencing factors. Next, the COA-XGBoost model is built and the parameters of the XGBoost model are optimized using the COA algorithm. Finally, the complete ensemble empirical Mode Decomposition with adaptive noise (CEEMDAM) method is utilized to decompose the residual sequence of the COA-XGBoost model and reconstruct it into high-frequency and low-frequency components. Appropriate frequency models are applied to achieve error correction, thereby constructing the combined Lasso-COA-XGBoost-CEEMDAN model. To further enhance the predictive accuracy and practicality of the model, a rolling time window is introduced for forecasting in the Hubei and Guangzhou carbon emission trading markets, ensuring that the forecasting model can adapt to market changes in real-time. The experimental results show that, taking the carbon price prediction in Hubei as an example, the proposed hybrid model has a significant improvement in prediction accuracy compared with the comparison model (XGBoost model): the RMSE is improved by 99.9987%, the MAE is improved by 99.9039%, the MAPE is improved by 99.9960%, and the R2 is improved by 0.2004%, and the advantages of this hybrid model are also verified in other experiments. The results provide an effective experimental method for future carbon price prediction.","Basic reporting
The topic is interesting and with possible applicability. However, I have some comments and suggestions which could greatly improve the manuscripts quality and these are:
1- The authors should highlight why COA is selected despite here are several other metaheuristic methods were available
2- Environmental setting used to implement the COA, and other machine learning should be added
3- Some new works using and applicable studies should be added.
4- What is the advantage over established techniques.
5- Conclusion is very weak. It should summarize the research, mention the best scores achieved, mention limitations and finally provide clear directions for the future research in this domain

Experimental design
Environmental setting used to implement the COA, and other machine learning should be added

Validity of the findings
The results are good

Additional comments
N/A",Authentic
Dynamic prediction of carbon prices based on the multi-frequency combined model,"As a central participant and important leader in the global climate governance system, China is facing the urgent need to predict and regulate the price of carbon emissions to promote the sound development of its carbon market. In this article, a rolling prediction model based on Least Absolute Shrinkage and Selection Operator-cheetah optimization algorithm-extreme gradient boosting (Lasso-COA-XGBoost) carbon price decomposition integration is proposed to address the defects of low prediction accuracy and insufficient model stability of a single machine learning model in the carbon price prediction problem. During the modeling process, the adaptive Lasso method is first employed to select factors from 15 primary indicators of carbon prices, identifying the most important influencing factors. Next, the COA-XGBoost model is built and the parameters of the XGBoost model are optimized using the COA algorithm. Finally, the complete ensemble empirical Mode Decomposition with adaptive noise (CEEMDAM) method is utilized to decompose the residual sequence of the COA-XGBoost model and reconstruct it into high-frequency and low-frequency components. Appropriate frequency models are applied to achieve error correction, thereby constructing the combined Lasso-COA-XGBoost-CEEMDAN model. To further enhance the predictive accuracy and practicality of the model, a rolling time window is introduced for forecasting in the Hubei and Guangzhou carbon emission trading markets, ensuring that the forecasting model can adapt to market changes in real-time. The experimental results show that, taking the carbon price prediction in Hubei as an example, the proposed hybrid model has a significant improvement in prediction accuracy compared with the comparison model (XGBoost model): the RMSE is improved by 99.9987%, the MAE is improved by 99.9039%, the MAPE is improved by 99.9960%, and the R2 is improved by 0.2004%, and the advantages of this hybrid model are also verified in other experiments. The results provide an effective experimental method for future carbon price prediction.","Basic reporting
no comment

Experimental design
no comment

Validity of the findings
no comment

Additional comments
Authors used alot acroymn first without full meaning ensure that meaning are used before continuing with acryomn e.g COA, XGBoost e.t.c

Authors should review introduction section to include more recent research of Machine learning ehanced with optimization algorithm such as


The image quality require enhancement at least 300dpi

Avoid personal pronouns e.g we or our maintain formal and academic tone. Ensure formal and academic tone in the entire manuscript

Include quantitative description of findings in the abstract

Phrases such as e,g ""As a key player in the global response to climate change"", ""has become a hot topic of research"" do not demonstrate
shcolary articulation correct such sentences

The motivation and contribution is unclear in the introduction

There's no experiment to test COA algorthim before applying to the problem

Discuss the limitation of the study


#


1. The manuscript includes multiple instances where acronyms such as COA and XGBoost are used without first providing their full meanings. The authors must ensure that all acronyms are clearly defined upon first usage before continuing with their abbreviated forms.

2. The introduction should be revised to incorporate more recent research on machine learning enhanced with optimization algorithms.

3. The quality of the images must be improved to a resolution of at least 300 dpi to ensure clarity and compliance with publication standards.

4. The authors should avoid the use of personal pronouns such as ""we"" or ""our."" The entire manuscript should maintain a formal and academic tone.

5. The abstract should include a quantitative description of the findings to provide a clearer and more precise overview of the study's outcomes.

6. Phrases such as ""As a key player in the global response to climate change"" or ""has become a hot topic of research"" lack scholarly articulation. These sentences should be revised to reflect a more academic and professional style.

7. The motivation and contributions of the study are unclear in the introduction. This section should be refined to clearly articulate the novelty and significance of the work.

8. There is no evidence of experiments conducted to validate the COA algorithm before applying it to the problem. The authors should include a dedicated experiment or discussion to justify its application.

9. The manuscript does not address the limitations of the study. A section discussing potential constraints, challenges, or areas for future improvement should be included",Authentic
A genetic programming-based ensemble method for long-term electricity demand forecasting,"This study introduces a novel genetic programming-based ensemble method for forecasting long-term electricity consumption in Ethiopia. The technique utilizes a two-stage ensemble approach to project Ethiopia’s electricity consumption through 2031. In the initial stage, genetic algorithms, particle swarm optimization, and simulated annealing methods are applied to various regression models (linear, quadratic, and exponential). The preliminary forecast values generated in this stage were further refined in the second stage. Here, the genetic programming method was utilized to develop a formula based on the initial forecast values, which then provided the final forecast results. The most accurate predictions in the first stage were obtained using the GA_Quadratic, PSO_Quadratic, and SA_Quadratic methods, resulting in mean absolute percentage error (MAPE) values of 3.61, 3.63, and 4.68, respectively. In the second stage, the GP-based prediction achieved an even lower MAPE value of 2.83. Other error metrics, including MSE, root mean square error (RMSE), and R2, were also evaluated, with the proposed model outperforming all methods from the first stage on these metrics. The study projected Ethiopia’s total annual electricity consumption through 2031 under two different scenarios. Both scenarios indicate that by 2031, electricity consumption will have tripled compared to 2021 levels.","Basic reporting
1. The author may strengthen the introduction by providing an extended explanation of the difficulties involved in long-term electricity demand forecasting in developing countries, especially those with growing economic rates of growth and expanding urbanization such as Ethiopia. More development on the specific challenges that Ethiopian is facing would have given stronger reason for the study. Could you give us more instances on how accurate forecasting will assist in planning, formulation of policies and arrangements for energy facilities in Ethiopia?

2. It may also be useful to draw a more detailed comparison of the proposed method with other ensemble learning models employed in the field of electricity demand forecasting. Moreover, a better understanding of the areas within a more comprehensive literature review where previous research has shortcomings concentrating on the energy demand in Ethiopia and how this paper tries to address those shortages would enhance the paper.

3. Some inconsistency in naming conventions is observed, for instance, different naming for equivalent mathematical operations “GA_Quadratic” and “GA_Squared”. Kindly make sure that the same terms are used throughout the manuscript. Some of them contain unclear constructions which could be formulated better: for instance, the note on page 14, line 364, about the GA_Quadratic method.

4. It will be appreciated if each factual statement made in this report is substantiated by the necessary references. For instance, the points mentioned at page 5, line 35, about electrical consumption in the developed countries and at page 6, line 63, on the expansion of the economy of Ethiopia need citation.

5. Give more information about preprocessing if done on the data alongside with missing values and outliers to make the readers understand from where one is starting the analysis.

Experimental design
1. More justification is required for the incorporation of the selected input variables namely population, GDP, import and export. The reader should explain why these particular variables are chosen before the study and argue any limitation of such variables and other possible variable that could be used.

2. The decision on which specific regression models (linear, quadratic, exponential) to use should be more clearly articulated for the first stage of the ensemble method.

3. What is rationale for using the mean forecast values from first stage as inputs in second stage GP model? What can this approach offer as its benefits?

4. The parameters of the GP model used for training are given in the paper, however there is no detail about the criteria of their selection and how their choice may influence the results of the model. Can you specify further how you came up with your choices of the function set, the terminal set and the crossover/mutation rate?

5. To enhance the credibility of the proposed method additional comparisons with other typical ensemble methods like bagging or boosting should be made.

Validity of the findings
1. It is necessary to analyze how sensitive the model is and analyze the relative importance of input variables. Kindly perform sensitivity analysis to enable determine the impact of changes in the input variables (e.g., the rate of increase in population, the GDP changes).

2. The quadratic regression model fared better than the linear and exponential models. Could you please provide more details about the possible causes of such a superior performance? There is a need to know whether the observed result is a wakeup call due to the nature of data collected, the inherent characteristics of the Ethiopian environment, or any other reasons.

3. Although the paper gives the error values, it is necessary to have a deeper discussion about these values. Examine the findings of the error values in each of the stages and identify any pattern or observation drawn from the study. Is the model good on certain year(s) or certain scenario?

4. For additional measures of uncertainty around the forecasts, it is suggested to present the confidence intervals of the predictions.

5. Create a table or figure that compares the forecast results of the proposed ensemble method with those of the models used in the first stage. This will clearly show the benefit of using the ensemble approach in the current hybrid environment.

Additional comments
1. The discussion section should also be lengthened and contain a broader analysis of the implications of the study’s findings to policy makers and energy planners or any other stakeholder involved in the energy sector in Ethiopia. How does it help decision makers and planners to make decision and allocate resources?
2. It is also recommended to mention the possible difficulties when applying the proposed method in a real environment. As you have pointed out, is it all right to use it or are there any questions concerning the data availability or computation problems to limit its applicability?

3. The ideas for further research are well presented, however they can be expanded. For instance while debating on integrating climate variables, it will be appropriate to identify the most suitable climatic parameters that could be included in the model; and how such parameters can best be incorporated into the model.

4. It may be useful to offer a flowchart or a diagram to represent the general approach in order to shed light on the relationships between different steps and methods. This would improve the understanding of the reader on the proposed ensemble method for developing the model.",Authentic
A genetic programming-based ensemble method for long-term electricity demand forecasting,"This study introduces a novel genetic programming-based ensemble method for forecasting long-term electricity consumption in Ethiopia. The technique utilizes a two-stage ensemble approach to project Ethiopia’s electricity consumption through 2031. In the initial stage, genetic algorithms, particle swarm optimization, and simulated annealing methods are applied to various regression models (linear, quadratic, and exponential). The preliminary forecast values generated in this stage were further refined in the second stage. Here, the genetic programming method was utilized to develop a formula based on the initial forecast values, which then provided the final forecast results. The most accurate predictions in the first stage were obtained using the GA_Quadratic, PSO_Quadratic, and SA_Quadratic methods, resulting in mean absolute percentage error (MAPE) values of 3.61, 3.63, and 4.68, respectively. In the second stage, the GP-based prediction achieved an even lower MAPE value of 2.83. Other error metrics, including MSE, root mean square error (RMSE), and R2, were also evaluated, with the proposed model outperforming all methods from the first stage on these metrics. The study projected Ethiopia’s total annual electricity consumption through 2031 under two different scenarios. Both scenarios indicate that by 2031, electricity consumption will have tripled compared to 2021 levels.","Basic reporting
The manuscript presents a genetic programming-based ensemble method for forecasting Ethiopia's long-term electricity consumption. Overall, the reporting quality is satisfactory with appropriate improvements made based on previous reviewer feedback.

Strengths:

-The manuscript is written in clear, professional English with appropriate technical language.
-The literature review adequately establishes the importance of electricity demand forecasting for Ethiopia and situates the work within existing research.
-The authors have effectively highlighted the knowledge gap in applying optimization methods to Ethiopia's energy forecasting.
-The article structure follows standard scientific presentation with clear sections for methodology, results, and discussion.
-Figures and tables are well-presented, with improvements made as requested by previous reviewers (e.g., consistent y-axis scales in Figure 3).
-The research questions have been added to the introduction as requested.
-The manuscript is self-contained and presents a complete body of work without unnecessary subdivision.

Areas for improvement:

-While many citations are provided, some additional references on recent ensemble forecasting methods could further strengthen the background.
-The authors could more explicitly state which table contains the raw data used in their analysis for better transparency.

Experimental design
The experimental design is sound and has been improved with the addition of statistical tests and synthetic data validation as requested by previous reviewers.

Strengths:

-The research question is clearly defined with two fundamental questions now added to the introduction.
-The methodology is rigorous, using a two-stage ensemble approach with multiple optimization algorithms.
-The authors have added explanations for hyperparameter selection through extensive trial-and-error processes.
-The use of synthetic data to address the small dataset limitation (16 instances) demonstrates good research practice.
-The addition of Wilcoxon signed-rank test provides statistical validation of model performance differences.
-The methods are described with sufficient detail to allow replication by other researchers.

Areas for improvement:

-While the authors now mention testing with synthetic data, more details on the specific Gaussian Copula-based method used to generate this data would be helpful.
-The rationale for specific algorithm selection (GA, PSO, SA, GP) has been addressed, but could be further strengthened with justification from performance benchmarks in similar domains.

Validity of the findings
The manuscript presents valid findings with appropriate limitations now acknowledged.

Strengths:

-The data analysis is sound and the results are clearly presented.
-The authors now explicitly address the small dataset size limitation (16 instances) and its potential impact on generalization ability.
-Test data validation using synthetic data strengthens the robustness of the findings.
-Error metrics (MSE, RMSE, R², MAPE) are appropriate for evaluating the model performance.
-Statistical tests have been added to validate performance differences between models.
-The conclusions are appropriately connected to the original research questions.
-The authors have revised overstated conclusions to better reflect the methodological approach and limitations.

Areas for improvement:

-While the synthetic data testing is a positive addition, the authors could provide more details on how well the model performs on completely new, unseen data patterns.
-The discussion on quadratic models' superior performance could further explore theoretical explanations beyond empirical results.

Additional comments
This manuscript makes a valuable contribution to the field of electricity demand forecasting for Ethiopia. The authors have been responsive to previous reviewer feedback and have substantially improved their work accordingly.

Particularly commendable aspects include:

-The clear separation of Results and Discussion sections as requested by reviewers.
-The addition of statistical tests to validate model performance differences.
-The use of synthetic data to address the small dataset limitation.
-More tempered conclusions that acknowledge the limitations of the study.
-The detailed explanation of hyperparameter selection.

The authors' GP-based ensemble approach demonstrates promising results for improving forecast accuracy. The model's ability to project Ethiopia's electricity consumption up to 2031 under two different scenarios provides valuable insights for energy planning and policy-making.

Future research might benefit from:

-Expanding the dataset with additional historical data or more input variables.
-Exploring other ensemble techniques beyond GP.
-Developing methods to better handle rapid changes in economic factors affecting electricity consumption.

Overall, the manuscript represents a solid contribution to the field and, with the improvements made in response to reviewer feedback, is suitable for publication.",Authentic
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios.","Basic reporting
The paper is well written and relatively easy to follow. The quality of English is satisfactory. Figures are of adequate quality and the data set is supplied.

Experimental design
The research question is formulated well. The authors propose a novel model for defect prediction and compare it to some of the existing ones.

Validity of the findings
There are several issues with this paper as it appears that the authors are trying to add a lot of content while not clearly reporting all the necessary details. Firstly, it is not clear how the MEPO and PO differ when it comes to computational efficiency. Some results regarding this should be given. Secondly, BMEPO, the purposed algorithm at the center of the stated contributions, is never clearly defined in the paper. The authors need to clarify all the metrics used during evaluation.",Authentic
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios.","Basic reporting
• The manuscript is well-structured, with a logical flow from introduction to conclusion. The sections are clearly and appropriately titled.
• The language used is clear and precise, despite minor grammatical errors.
• The literature review provides sufficient context for the problem and appropriately references relevant studies, such as existing feature selection methods and ensemble learning techniques.
• Figures and tables are relevant and well-labeled. The inclusion of diagrams for algorithms and frameworks aids comprehension. However, Figure resolutions should be improved for clarity.

Experimental design
• The objectives of the study are well-defined, with a focus on improving software defect prediction using feature selection via a novel multi-strategy enhanced parrot optimization algorithm (MEPO).
• The methodology is robust, employing standard metrics (AUC) for performance evaluation and multiple comparative baselines (e.g., BPO, BWO, BCPO).
• The dataset preprocessing steps, including SMOTE for addressing class imbalance, are adequately detailed. However, additional explanation on why certain datasets were chosen (e.g., PROMISE repository datasets) would strengthen the rationale.
• The parameters of the algorithm are clearly described, but sensitivity analyses for key parameters like mutation rate or population size are missing.

Validity of the findings
• The findings are well-supported by experimental results, demonstrating the superiority of MEPO in optimization tasks and feature selection for defect prediction.
• Statistical analysis is sufficient; standard deviation and multiple independent runs ensure reliability. However, the manuscript lacks confidence intervals or statistical tests to compare algorithm performance rigorously.
• The heterogeneous data stacking ensemble (HEDSE) framework’s results outperform homogeneous models, providing a strong argument for its use. Yet, an ablation study isolating the effect of HEDSE would strengthen claims.

Additional comments
• The manuscript makes a valuable contribution by integrating a novel optimization algorithm with ensemble learning for a critical problem in software engineering.
• The writing can benefit from minor revisions for fluency and consistency. For example, terms like ""heterogeneous data stacking ensemble"" should be abbreviated consistently throughout.
• The introduction sets the stage well but could more directly highlight the practical implications of defect prediction improvements.
• Future work directions are insightful but would benefit from more specific suggestions, such as real-world deployment challenges or computational cost comparisons.

Specific Suggestions for Improvement:

1. Include a sensitivity analysis for MEPO parameters.
2. Provide statistical tests for performance comparisons.
3. Improve the quality of figures and ensure consistent formatting throughout.
4. Revise certain sentences for grammatical clarity and conciseness.
5. Expand on practical implications and limitations in the discussion.",Authentic
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios.","Basic reporting
1. A few grammar corrections, spelling error, and acronyms must be looked at carefully throughout the manuscript
2. Equations citations should be done properly

Experimental design
Original primary research within the Aims and Scope of the journal.
The research question is well-defined, relevant, and meaningful. It states how research fills an identified knowledge gap and is well-demonstrated.
In the Results and discussion, details of the implementation environment and overview of the dataset are required

Validity of the findings
All underlying data have been provided; they are robust, statistically sound, & controlled.
Explanations for the dataset should be given
Conclusions are well stated, linked to original research question & limited to supporting results

Additional comments
First and foremost, I would like to appreciate the authors for the good and extensive work in evaluating the performance of their proposed model. The paper was organized well. The following suggestions would help the readers to enhance the readability:

1. The abstract is not clear. For instance, lines 17 to 19 could be rewritten. And what does the phrase” most datasets convey” mean? Furthermore, the abstract should answer the question: What is the significance of your work for researchers in the same field?”
2. In line 29, give valid reference for the Pareto principle
3. In line 38, “defectsSong et al.” leave a space between these words, and check for the same while citing references throughout the manuscript
4. All the acronyms should be abbreviated on their first usage, and the acronym should be referred to on its subsequent usage. Some of the acronyms have been abbreviated multiple times and few weren’t. Eg: line no. 234,238, 240, 241, 304,311 etc..
5. In line 48, change “The paper” to “This work” or “This paper”
6. In the significant contributions presented in the introduction section, there are some repeated statements from the previous paragraph:” Our proposed model has proven to outperform homogeneous data stacking ensemble learning models.” Also, in some places, you have used this model, and in other places, you have used our proposed model, so maintain uniformity in this usage.
7. What is this “BMCPO”?? If this refers to your proposed model, abbreviate it and use it throughout the paper. This would improve the readability
8. In line no.91 “Arar et al.Arar and Ayan (2015)” check this, and same kind of typo errors with reference management software occur at different places (For instance, in line no: 98, 102,104)
9. In line 120, what does “ each individual” refer to? And PAO can be explained simply to enhance readability and understandability
10. While using comma, leave a space after that: line no 124 – 127
11. Some special characters were inserted in line no.129
12. In stacked ensemble learning: line nos: 135 to 137, predictions + input data is fed as input to metamodel (as per explanation), but it is not reflected in Figure 1. Will the meta model in a stacked ensemble take only predictions as features or the input data? Clarify
13. In 3.2, 3.2.2, 3.2.3 capitalise the first letter of the section title
14. Why is the chaotic state parameter set to 0.5??
15. Usually equation is denoted as Eqn. check this in line no. 193
16. Change the 4.1 section title to a Result Analysis of MEPO and Experimental Comparison with other Models. Similarly, change 4.2, 4.3
17. In line no. 249, it is Table 5 I think, change it
18. In table 10. Check the column names, and NGBoost was not abbreviated
19. Why were specific models chosen for stacking, the justification for this is missing
20. In the Results and discussion, details of the implementation environment and overview of the dataset are required",Authentic
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios.","Basic reporting
The paper presents a novel, well-structured, and practically significant methodology with clear experimental results. It is suitable for publication with minor changes to enhance clarity and comprehensiveness.

Experimental design
No Comment

Validity of the findings
No Comment",Generic
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios.","Basic reporting
The paper is well written and easy to follow. It conforms to the journal's standards.

Experimental design
The research is within the aims of the journal.

Validity of the findings
The research findings are valid.

Additional comments
The authors have addressed my concerns and the paper can be accepted",Generic
A rapid method for methanol quantification in spirits using UV-visible spectroscopy and FTIR,"Although standards methods of food safety assessment are important, these methods are relatively expensive and require intensive work and time. In alcohol beverage industries, ultraviolet visible (UV-Vis) spectroscopy and Fourier transform infrared (FTIR) spectroscopy are reliable techniques for quality assessment of alcohol, however, testing methods are often varying with calibration techniques and instrument specification. In this work, methanol content in ethanol was assessed in two approaches using UV-Vis with a developed calibration technique and FTIR spectroscopy with a factory default scan function at every 2 nanometer (nm). For UV-Vis method, potassium dichromate was used as the chromogenic reagent, tested with methanol concentration ranging from 0.12% to 1% (mV−1). For FTIR method, spectra data was collected every 2 nm interval and calibration curve was built by increasing methanol ratio from 0% to 40% (mV−1) at the expense of ethanol while keeping deionised (DO) water constant at 5% (mV−1) concentration. This helps gauge the change in methanol concentration relative to ethanol. Results of analysis using UV-Vis showed a strong negative correlation for methanol concentration and absorbance value at UV region from 900 to 1,100 cm−1 (r = 98.00, RMSE = 0.023) relative to increasing ethanol concentration. A strong peak was observed for methanol concentration at spectral region of 975 cm−1 which is related to the methanoic acid C-O bond. The FTIR spectra region at 900 to 1,050 cm−1 was used for observing methanol concentration with absorbance. A strong correlation was established from spectral region of 1,010 to 1,026 cm−1, enabling quantification of methanol (r = 0.99, RMSEC = 0.55). Methanol peak was observed at 1,020 cm−1 region of the spectrum. A set of experimental repetition was made with methanol concentration of 0.02% to 0.5% and 0.1% to 5% for UV-Vis and FTIR, respectively, to determine the limit of detection (LOD) and limit of quantification (LOQ). The observation showed a 0.04% and 0.29% (mV−1) LOD for UV-Vis and FTIR method, respectively. The LOQ was 0.12% and 0.89% (mV−1) for UV-Vis and FTIR respectively. The integration of UV-Vis with potassium dichromate as chromogenic reagent and FTIR spectroscopy with comparatively 50% less data point still present a significant advancement in the test method for safety and quality control of alcohol beverage products. These techniques not only enhance the ability to detect harmful substances but also provide a cost-effective and rapid alternative to traditional methods, making them invaluable tools for distilleries aiming to uphold high standards of quality.","Basic reporting
1. This manuscript should be submitted to a professional research paper writing service to improve its language quality and address any linguistic inaccuracies.
Examples:
I. The term ""ultraviolet-visible spectrometry"" is abbreviated as both ""UV Vis"" and ""UV VIS"" in various sections. To enhance clarity for readers, it is recommended to adhere to ISO standards and maintain a consistent abbreviation throughout the article.
II. There are capitalization errors in the text, such as in line 31 where ""Methanol"" is capitalized in the middle of a sentence. Chemical names should not be capitalized unless they appear at the beginning of a sentence.
",Authentic
A rapid method for methanol quantification in spirits using UV-visible spectroscopy and FTIR,"Although standards methods of food safety assessment are important, these methods are relatively expensive and require intensive work and time. In alcohol beverage industries, ultraviolet visible (UV-Vis) spectroscopy and Fourier transform infrared (FTIR) spectroscopy are reliable techniques for quality assessment of alcohol, however, testing methods are often varying with calibration techniques and instrument specification. In this work, methanol content in ethanol was assessed in two approaches using UV-Vis with a developed calibration technique and FTIR spectroscopy with a factory default scan function at every 2 nanometer (nm). For UV-Vis method, potassium dichromate was used as the chromogenic reagent, tested with methanol concentration ranging from 0.12% to 1% (mV−1). For FTIR method, spectra data was collected every 2 nm interval and calibration curve was built by increasing methanol ratio from 0% to 40% (mV−1) at the expense of ethanol while keeping deionised (DO) water constant at 5% (mV−1) concentration. This helps gauge the change in methanol concentration relative to ethanol. Results of analysis using UV-Vis showed a strong negative correlation for methanol concentration and absorbance value at UV region from 900 to 1,100 cm−1 (r = 98.00, RMSE = 0.023) relative to increasing ethanol concentration. A strong peak was observed for methanol concentration at spectral region of 975 cm−1 which is related to the methanoic acid C-O bond. The FTIR spectra region at 900 to 1,050 cm−1 was used for observing methanol concentration with absorbance. A strong correlation was established from spectral region of 1,010 to 1,026 cm−1, enabling quantification of methanol (r = 0.99, RMSEC = 0.55). Methanol peak was observed at 1,020 cm−1 region of the spectrum. A set of experimental repetition was made with methanol concentration of 0.02% to 0.5% and 0.1% to 5% for UV-Vis and FTIR, respectively, to determine the limit of detection (LOD) and limit of quantification (LOQ). The observation showed a 0.04% and 0.29% (mV−1) LOD for UV-Vis and FTIR method, respectively. The LOQ was 0.12% and 0.89% (mV−1) for UV-Vis and FTIR respectively. The integration of UV-Vis with potassium dichromate as chromogenic reagent and FTIR spectroscopy with comparatively 50% less data point still present a significant advancement in the test method for safety and quality control of alcohol beverage products. These techniques not only enhance the ability to detect harmful substances but also provide a cost-effective and rapid alternative to traditional methods, making them invaluable tools for distilleries aiming to uphold high standards of quality.","Validity of the findings
1. The manuscript titled “A Rapid Method for Methanol Quantification in Spirits Using UV-Visible Spectroscopy and FTIR” (#104989) presents techniques for determining methanol content in spirits through the use of UV-Vis and FTIR spectroscopy. The UV-Visible spectrometry method described involves the use of potassium dichromate in acidic conditions to oxidize alcohols, leading to the reduction of chromium from the Cr (VI) (yellow color) oxidation state to Cr (III) (green color). It is worth noting that this method has been widely recognized since the 1990s. (Hari, M.; Deoki, N. Spectrophotometric Determination of Some Monohydric Alcohols Based on Their Oxidation by K2CrO4-HNO3 System. Indian J. Chem 1994, 33, 359–361. ; Tgd, S.; Rsrd, S.; Fgd, C.; Dda, S. ATR-FTIR and UV-Vis as Techniques for Methanol Analysis in Biodiesel-Washing Wastewater. QuÌmica Nova 2023, 46, 698–705.). Previously published methods have reported alcohols (ethanol and methanol) at a wavenumber of 600 cm⁻¹. The current manuscript provides a clarification regarding the correlation of methanol at 970 cm⁻¹, which is considered one of the novel aspects of this study. To enhance clarity, it would be beneficial to highlight this correlation by zooming in on the 970 cm⁻¹ region in Figure 2.

2. Another commonly reported method, FTIR, is widely recognized for its effectiveness in quantifying ethanol and methanol content in various samples (Pérez-Ponce, A.; de la Guardia, M. Partial Least-Squares–Fourier Transform Infrared Spectrometric Determination of Methanol and Ethanol by Vapour-Phase Generation. Analyst 1998, 123, 1253–1258. ; Coldea, T. E.; Socaciu, C.; Fetea, F.; Ranga, F.; Pop, R. M.; Florea, M. Rapid Quantitative Analysis of Ethanol and Prediction of Methanol Content in Traditional Fruit Brandies from Romania, Using FTIR Spectroscopy and Chemometrics. Not. Bot. Horti Agrobot. Cluj Napoca 2013, 41, 143.). Could you kindly compare the previously established FTIR methods for ethanol and methanol quantification with the current method you have developed, and emphasize the significant improvements? It is important to clearly demonstrate the advancements made, as presenting a method without novelty or improvement may not add substantial value to the literature.",Authentic
A rapid method for methanol quantification in spirits using UV-visible spectroscopy and FTIR,"Although standards methods of food safety assessment are important, these methods are relatively expensive and require intensive work and time. In alcohol beverage industries, ultraviolet visible (UV-Vis) spectroscopy and Fourier transform infrared (FTIR) spectroscopy are reliable techniques for quality assessment of alcohol, however, testing methods are often varying with calibration techniques and instrument specification. In this work, methanol content in ethanol was assessed in two approaches using UV-Vis with a developed calibration technique and FTIR spectroscopy with a factory default scan function at every 2 nanometer (nm). For UV-Vis method, potassium dichromate was used as the chromogenic reagent, tested with methanol concentration ranging from 0.12% to 1% (mV−1). For FTIR method, spectra data was collected every 2 nm interval and calibration curve was built by increasing methanol ratio from 0% to 40% (mV−1) at the expense of ethanol while keeping deionised (DO) water constant at 5% (mV−1) concentration. This helps gauge the change in methanol concentration relative to ethanol. Results of analysis using UV-Vis showed a strong negative correlation for methanol concentration and absorbance value at UV region from 900 to 1,100 cm−1 (r = 98.00, RMSE = 0.023) relative to increasing ethanol concentration. A strong peak was observed for methanol concentration at spectral region of 975 cm−1 which is related to the methanoic acid C-O bond. The FTIR spectra region at 900 to 1,050 cm−1 was used for observing methanol concentration with absorbance. A strong correlation was established from spectral region of 1,010 to 1,026 cm−1, enabling quantification of methanol (r = 0.99, RMSEC = 0.55). Methanol peak was observed at 1,020 cm−1 region of the spectrum. A set of experimental repetition was made with methanol concentration of 0.02% to 0.5% and 0.1% to 5% for UV-Vis and FTIR, respectively, to determine the limit of detection (LOD) and limit of quantification (LOQ). The observation showed a 0.04% and 0.29% (mV−1) LOD for UV-Vis and FTIR method, respectively. The LOQ was 0.12% and 0.89% (mV−1) for UV-Vis and FTIR respectively. The integration of UV-Vis with potassium dichromate as chromogenic reagent and FTIR spectroscopy with comparatively 50% less data point still present a significant advancement in the test method for safety and quality control of alcohol beverage products. These techniques not only enhance the ability to detect harmful substances but also provide a cost-effective and rapid alternative to traditional methods, making them invaluable tools for distilleries aiming to uphold high standards of quality.","Basic reporting
Clear and unambiguous, professional English used throughout.

Literature references, sufficient field background/context provided.

Professional article structure, figures, tables. Raw data shared.

Self-contained with relevant results to hypotheses.

Experimental design
Original primary research within Aims and Scope of the journal.

Research question well defined, relevant & meaningful. It is stated how research fills an identified knowledge gap.

Rigorous investigation performed to a high technical & ethical standard.

Methods described with sufficient detail & information to replicate.

Validity of the findings
Meaningful replication encouraged where rationale & benefit to literature is clearly stated.

All underlying data have been provided; they are robust, statistically sound, & controlled.

Conclusions are well stated, linked to original research question & limited to supporting results.

Additional comments
The manuscript has been revised appropriately. The revised manuscript is acceptable.",Generic
Authentication of Jarrah (Eucalyptus marginata) honey through its nectar signature and assessment of its typical physicochemical characteristics,"Jarrah (Eucalyptus marginata) is a dominant forest tree endemic to the southwest of Western Australia. Its honey is appreciated for its highly desirable taste, golden colour, slow crystallisation, and high levels of bioactivity, which have placed Jarrah in the premium product range. However, whilst customers are willing to pay a high price for this natural product, there is currently no standard method for its authentication. As honey is naturally sourced from flower nectar, a novel route of authentication is to identify the nectar signature within the honey. This study reports on a high-performance thin layer chromatography (HPTLC)-based authentication system which allows the tracing of six key marker compounds present in Jarrah flower nectar and Jarrah honey. Four of these markers have been confirmed to be epigallocatechin, lumichrome, taxifolin and o-anisic acid with two (Rf 0.22 and 0.41) still chemically unidentified. To assist with the characterisation of Jarrah honey, a range of physicochemical tests following Codex Alimentarius guidelines were carried out. A blend of authenticated Jarrah honey samples was used to define the properties of this honey type. The blend was found to have a pH of 4.95, an electric conductivity of 1.31 mS/cm and a moisture content of 16.8%. Its water-insoluble content was 0.04%, its free acidity 19 milli-equivalents acid/kg and its diastase content 13.2 (DN). It also contains fructose (42.5%), glucose (20.8%), maltose (1.9%) and sucrose (<0.5%). The HPTLC-based authentication system proposed in this study has been demonstrated to be a useful tool for identifying Jarrah honey and might also act as a template for the authentication of other honey types.","Basic reporting
English is ok.
Literature is sufficiently sampled.
The structure of the article is ok.

Experimental design
The experimental design is not complete.
- A first question is concerning the evaluation of authentic Jarrah honey and nectar from other geographical origins (whenever possible). How the authors evaluated this point?
- Additionally, did the authors evaluated mixtures of honey from WA and Jarrah to evaluate the LOD of the approach for detecting mixtures/dilutions practices? Eventually other non-WA honeys.
- In the present work the authors evaluated different HPTLC bands attributed to co-flowering, therefore blended samples. How is the allowed limit of blends for a pure Jarrah honey?
- Which are the values of reproducibility and accuracy of the rf values of bands for the proposed HPTLC approach?
- How many replicates have been considered in this study?

Validity of the findings
The novelty is not present.
The approach includes an HPTLC analysis combined with physicochemical characterizations. The authors also stated that this authentication approach allows, for the first time, the characterization of physicochemical characteristics of this honey. The present manuscript is an extension of a previous work from the same group, in which the same approach was applied to a restricted samples set of Jarrah honey (https://doi.org/10.1016/j.crfs.2022.02.014), while 31 were considered here.

Additional comments
The proposed approach resulted fruitful in determining compositional differences, typically in botanical assessments, or in the profiling of polyphenols compositions etc.. In fact the authors already published compositional differences between Manuka and Jarrah honeys in their recent work (https://doi.org/10.7717/peerj.12186). The main result of these investigations is concerning a qualitative evaluation of HPTLC bands, whereas the quantitative evaluation (e.g. saccharides content) could be achieved by other methods. The authentication process needs more accurate determinations.",Authentic
Baltic Sea shipwrecks as a source of hazardous pollution,"Shipwrecks on the Baltic Sea seabed pose a serious threat to the marine environment. Fuel, ammunition and chemicals in their holds can enter the ecosystem at any time, causing an ecological disaster. It is known that oil spills from ship accidents can affect life and health of different species of animals, both immediately after catastrophe and for many years thereafter. This article discusses the negative impact of shipwrecks on the ecological status of the Baltic Sea and presents the contamination status of bottom sediment core samples taken in the vicinity of shipwrecks located in the South Baltic, i.e., S/s Stuttgart, t/s Franken, S/T Burgmeister Petersen and m/s Sleipner. It is based on the results of research carried out by the Maritime Institute between 2011 and 2016.","Dear Authors,
Reviewers have evaluated your manuscript. it is recommended that you revise the manuscript to polish the draft to a standard that is acceptable in a journal with international audience. In addition to this, please consider the following additional EDITOR comments:

1. The abstract needs to be carefully revised following the journal guidelines found in the template at https://peerj.com/about/author-instructions/chemistry. Specifically, you need to properly introduce the study background (problem), present the methods used and the key findings (results).

2. Some of the literature cited are too old. For example, Andrulewicz et al. (1994, 1998), (HELCOM, 1974), and (Jeffery, 1990). Although some of these may be important in this study, such literature rarely present the current status of knowledge and should only be cited in exceptional cases. There is very recent relevant literature in this area.

https://doi.org/10.3390/w14223772
https://doi.org/10.1016/j.marpolbul.2021.112747
https://doi.org/10.1016/j.marenvres.2020.105036
https://doi.org/10.1016/j.marpolbul.2022.114426

3. The introduction needs to be rewritten altogether so that it does not merely present the history of the ship wreckages that occurred in the study area. This section should introduce the potential of ship wrecks as sources of pollution (see for example, https://doi.org/10.3390/jmse11020276), which should better position your study and justify why you need to report on the study area at this point in time.

4. Thoroughly check the manuscript for typos and grammar. For example, in L157, ''or'' should be ‘‘for’’.

5. Methodology (L156-178) needs to be elaborated. The analytical conditions used should be clearly indicated, including any quality control and quality assurance activities undertaken. See https://doi.org/10.1007%2Fs40201-019-00356-z for a similar reporting.

6. Your study does not have any dedicated section on how the data were analyzed or visualized. I believe that your study could benefit from health risk assessment, which should give a better view as to whether the current pollution levels pose any significant health threats to the local pollution.

7. In Table 3, it is not possible that Pazdro (2004) did their study in 2022. The ‘‘n.d.’’ in this Table should be replaced with the LOD or LOQ. For easy comparison, put the results of your study first, followed by previous studies.

**PeerJ Staff Note:** It is PeerJ policy that additional references suggested during the peer-review process should only be included if the authors agree that they are relevant and useful.

**PeerJ Staff Note:** Please ensure that all review, editorial, and staff comments are addressed in a response letter and that any edits or clarifications mentioned in the letter are also inserted into the revised manuscript where appropriate.",Authentic
Effect of acidity/alkalinity of deep eutectic solvents on the extraction profiles of phenolics and biomolecules in defatted rice bran extract,"This study investigated the influence of deep eutectic solvent (DES) acidity/alkalinity on the extraction profiles of phenolics and other biomolecules (phytic acid, reducing sugar, and protein) in defatted rice bran (DFRB). The DES with varying pH levels were prepared using different hydrogen bond acceptors (choline chloride (ChCl) and potassium carbonate (K2CO3)) and hydrogen bond donors (lactic acid, urea, and glycerol). The results reveal that the acidic DES (ChCl-lactic acid; pH 0.42) demonstrated superior extraction efficiency for total phenolic acids (4.33 mg/g), phytic acid (50.30 mg/g), and reducing sugar (57.05 mg/g) while having the lowest protein content (5.96 mg/g). The alkaline DES (K2CO3-glycerol; pH 11.21) showed the highest levels of total phenolic acid (5.49 mg/g) and protein content (12.81 mg/g), with lower quantities of phytic acid (1.04 mg/g) and reducing sugar (2.28 mg/g). The weakly acidic DES (ChCl-glycerol; pH 4.72) exhibited predominantly total phenolics (3.46 mg/g) with lower content of protein (6.22 mg/g), reducing sugar (1.68 mg/g) and phytic acid (0.20 mg/g). The weakly alkaline DES (ChCl-urea; pH 8.41) resulted in lower extraction yields for total phenolics (2.81 mg/g), protein (7.45 mg/g), phytic acid (0.10 mg/g), and reducing sugar (7.36 mg/g). The study also explored the distribution of phenolics among various DESs, with the alkaline DES (K2CO3-glycerol) containing the highest concentration of free phenolics. Notably, ChCl-based DESs predominantly contained soluble esterified bound phenolics and soluble glycosylated bound phenolics. Furthermore, a significant correlation between antioxidant activities and phenolic contents was observed. In conclusion, this study has revealed that the acidity and alkalinity of a DES significantly impact the extraction of phenolics and other value-added biomolecules in DFRB. These findings highlight the potential for manipulating the properties of DESs through pH variation, making them versatile solvents for extracting and isolating valuable compounds from agricultural by-products like DFRB and offering opportunities for sustainable utilization and value addition in various industries.","Basic reporting
The manuscript presents a thorough investigation into the effect of acidity and alkalinity in deep eutectic solvents (DES) on the extraction of phenolics and other biomolecules from defatted rice bran (DFRB), a by-product of rice bran oil production. The text is well-structured and unveils intriguing findings regarding the utilization of DES for extracting phenolics and other biomolecules from DFRB. However, certain aspects of the study require further clarification and enhancement to elevate the manuscript's quality for publication.

A thorough proofreading is advised to enhance the manuscript’s quality. For example, Section 2.6.2 is missing a subtitle, and in Line 301, 'NR' should be subscripted as 'ENR'.

Experimental design
The rationale behind choosing the four DES (ChCl-lactic acid, K2CO3-glycerol, ChCl-glycerol, and ChCl-urea) needs to be explicitly delineated. Are there other DES that could have been considered? A comparative analysis regarding the cost, availability, environmental impact, and biocompatibility of these DES compared to conventional solvents would provide a more comprehensive understanding.

More granular information on the analysis of the phenolic extracts is desired. It is advisable to include data on the replicates for each extraction experiment and to report the standard deviation or standard error of the mean for all data exhibited in tables and figures.

Validity of the findings
A more in-depth exploration of the extraction mechanism of phenolics and other biomolecules by the DES is necessary. The impact of the pH environment of the DES on the solubility and stability of the extracted compounds should be discussed. Additionally, elucidating the interactions between the DES and DFRB components, along with the effect of extraction time and temperature on the yield and quality, would be beneficial.

A discussion on the molecular interactions that might account for the differential extraction efficiency observed would be insightful. This aspect could significantly enrich the manuscript.

Investigating and discussing the effect of DES composition on the stability and functionality of the extracted biomolecules, particularly focusing on proteins and phenolics, would be worthwhile.",Authentic
"Evaluating soil salinity dynamics under drip irrigation in the Manas River Basin, Xinjiang: a long-term analysis (1996–2019)","The Manas River Basin, located in Xinjiang, China, is one of the province’s four major agricultural irrigation regions and the first in the country to implement large-scale drip irrigation. While drip irrigation has enhanced water use efficiency, it has also contributed to soil salinization, negatively impacting crop yields and soil health. This study examines the spatial and temporal evolution of soil salinity in the oasis area of the basin from 1996 to 2019. The study evaluates salinization dynamics under long-term irrigation practices using soil salinity inversion models, regression analysis, water-salt balance calculations, geostatistical techniques, and ArcGIS. The results reveal significant improvements in soil salinity conditions, with 78.02% of the region experiencing reduced salinity and 10.09% exhibiting deterioration. From 1996 to 2019, non-salinized soil increased by 1,403.46 km2, mildly salinized soil expanded by 3,702.28 km2, while saline soils decreased by 7,685.6 km2. Statistical analysis indicates that soil salinity followed normal or logarithmic-normal distributions, with higher variability observed in 2016 and 2019. Despite these positive trends, challenges remain, particularly in the Shihezi, Manas, and Mosuowan irrigation zones, which still exhibit moderate to severe salinity. This study highlights the effectiveness of drip irrigation combined with improved management practices in mitigating soil salinity and enhancing soil quality. However, it emphasizes the need for targeted strategies to address residual salinization risks, ensuring sustainable agricultural development and ecological balance in arid regions.","Basic reporting
Overall paper is written in clear and in professional English language with the provision of sufficient background knowledge, enough literature review, good illustrated figures and tables etc. However some suggestions are given in separate file to be considered.

Experimental design
Study experiments are well designed however gaps can be filled considering the minor suggestion attached file.

Validity of the findings
This study is based on older methods with the lake of novelty but still have sound knowledge.

Additional comments
This paper offers a valuable contribution to soil salinity research through its integration of advanced techniques and long-term data. However, addressing the methodological gaps, improving clarity and organization, and strengthening actionable recommendations would significantly enhance its impact and utility for both scientific and practical purposes.",Generic
"Comprehensive analysis of groundwater hydrochemistry and nitrate health risks in the Baiquan basin, Northern China","
Groundwater is a crucial water source and strategic resource, essential for sustaining both urban and rural livelihoods, supporting economic and social development, and maintaining ecological balance. This study investigates the hydrochemical properties and controlling factors of groundwater in the Baiquan basin (BQB) by analyzing water quality data collected during both dry and wet periods. Additionally, the suitability of groundwater for drinking and agricultural irrigation was evaluated. The findings reveal that groundwater in BQB is generally weakly alkaline and primarily consists of hard-fresh water. Although there are seasonal variations in the main ion concentrations, HCO3− and Ca2+ are the predominant anions and cations, respectively. Consequently, the hydrochemical type is mainly HCO3-Ca⋅Mg type, with a secondary classification of SO4⋅Cl-Ca ⋅ Mg. The hydrochemical composition is primarily influenced by the dissolution of carbonate and silicate minerals, as well as cation exchange processes. Additionally, it is affected by anthropogenic inputs, particularly from the use of agricultural fertilizers. The water quality assessment results indicated that all water samples are classified as either good or moderate, with a significant majority falling into the good category. Additionally, the northern section of the BQB exhibited lower entropy weight water quality index (EWQI) values during the dry season in comparison to the wet season. For irrigated agriculture, groundwater in the BQB serves as a high-quality water source for irrigation throughout both the dry and rainy seasons. Furthermore, non-carcinogenic risks are notably concentrated in the north-western and south-eastern regions of the study area. Health risks associated with nitrates in groundwater are elevated during the rainy season. Notably, non-carcinogenic risks for infants were significantly high across both seasons and substantially exceeded those for children and adults. These results provide valuable scientific insights for the management and development of groundwater resources in the BQB.","Basic reporting
The manuscript is generally clear and well-organized. However, some sections could benefit from more concise language to improve readability. Somes suggestions have been made in the document.

The study is well-contextualized within the existing literature, with appropriate references to previous research. Figures and tables are well-designed but could benefit from informative captions. The references are relevant and up to date, supporting the study’s background and rationale effectively.",Authentic
"Nutrient cycling characteristics along a chronosequence of forest primary succession in the Hailuogou Glacier retreat area, eastern Tibetan Plateau","The Hailuogou Glacier has been continuously retreating since the end of the Little Ice Age, resulting in a 125-year soil chronosequence and a complete primary forest succession sequence. Nutrient cycling and utilization are the foundation to forest succession processes and dynamic changes, directly influencing the structure and stability of ecosystems. However, our understandings on the characteristics of ecosystem nutrient accumulation and recycling during succession, especially in the context of primary succession within glacier retreat areas, remain limited. To address this, we investigated nutrient characteristics across six forest primary succession sites in the Hailuogou Glacier retreat area.","Basic reporting
Some sections, particularly in the ""Materials & Methods"" and ""Results"" sections, contain lengthy, complex sentences that can be difficult to follow. Simplify and break down long sentences into shorter, more concise statements. Ensure each paragraph focuses on a single idea. While the introduction provides some context, it lacks a clear statement of the research gap and the significance of the study. It should better articulate the importance of the study and how it contributes to the field of nutrient cycling and forest succession. Some methodological details are insufficiently described, such as the specific techniques used for sample collection and analysis. Figures and tables are relevant but need clearer labeling and more descriptive captions. All visual data should be easily interpretable without referring back to the text. The literature cited is relevant and generally well-integrated into the manuscript. However, there could be a more comprehensive comparison of the study's findings with existing literature, especially in the discussion section.

Experimental design
While the methods are generally well-described, some aspects, such as the specific statistical methods used for data analysis, are not detailed enough. This lack of detail can hinder the replication of the study. Provide more comprehensive descriptions of the statistical analyses, including the software used, specific tests applied, and criteria for significance. The study does not include longitudinal data collected over multiple years, which could provide deeper insights into temporal variations in nutrient cycling. The methods section provides detailed descriptions of sample collection and analysis techniques for soil and vegetation. This includes specific information on the tools and procedures used, enhancing the reproducibility of the study. The discussion section provides a broad overview of the findings but lacks depth in interpreting the implications of the results. The discussion could be more thorough in explaining the significance of the findings in relation to existing literature. Deepen the analysis by comparing and contrasting the study’s results with previous research. Discuss the broader ecological implications and potential mechanisms underlying the observed nutrient cycling patterns.

Validity of the findings
The paper does not explicitly assess the impact and novelty of its findings within the broader context of ecological research. While the study presents original data and insights into nutrient cycling during forest succession in a glacial retreat area, it could benefit from a more explicit discussion of how these findings advance current knowledge and their potential implications for future research. you can include a dedicated section that explicitly discusses the impact and novelty of the study’s findings. Highlight how the research contributes new insights to the field and its potential implications for ecological theory and practice. Provide more detailed information on the statistical analyses performed, including the software used, specific tests applied, and criteria for significance. This transparency will enhance the credibility and reproducibility of the findings.

Additional comments
Overall, the paper presents a valuable and original contribution to the understanding of nutrient cycling in forest primary succession within a glacial retreat area. The study is well within the aims and scope of the journal, with a clear and well-defined research question. the discussion does not consider alternative explanations or potential limitations of the study, such as sampling biases or the constraints of the chrono sequence approach. There is also a tendency to overgeneralize the findings, which may not be fully supported by the data, especially concerning their applicability to other glacial retreat areas. The paper would benefit from a more detailed discussion of methodological limitations and a more cautious approach to generalizing results. Furthermore, the absence of explicit future research directions weakens the potential for guiding subsequent studies. The statistical analysis section needs more detailed explanations of the methods used and their appropriateness, enhancing the transparency and reproducibility of the findings. Addressing these weaknesses through a more thorough and critical discussion, better integration with existing research, and detailed methodological transparency would significantly improve the paper's quality and contribution to the field.",Authentic
Optimization and action mechanism of pollutant removal performance of unsaturated vertical flow constructed wetland (UVFCW) driven by substained-release carbon source,"Constructed wetland (CW) technology has attracted much attention due to its economical and environmentally friendly features. The low dissolved oxygen (DO) and low carbon/nitrogen (C/N) ratio in the wetland influent water affect the treatment performance of CW, resulting in a decrease in the removal efficiency of ammonia nitrogen (NH4+-N) and nitrate nitrogen (NO3−-N). In order to address this problem, this study optimized the pollutants removal performance of unsaturated vertical flow constructed wetland (UVFCW) by adding sustained-release carbon sources (corn cobs + polybutylene adipate terephthalate (PBAT)). The results showed that the sustained-release of carbon source increased the carbon source in UVFCW, thus increasing the abundance and activity of denitrifying microorganisms and enhancing the denitrification reaction, ultimately improving the removal of NO3−-N, with its removal efficiency reaching up to 95.50%. The placement method of sustained-release carbon source mainly affected the distribution of carbon source and DO in water body, thus influencing the relative abundance of microorganisms, finally affecting the removal of pollutants. Among them, the removal efficiency of total nitrogen (TN), NO3−-N, and total phosphorus (TP), and the relative abundance of denitrifying microorganisms in the CWR-Cu (uniform placement of sustained-release carbon source) were significantly higher than those in the CWR-Ca (centralized placement above) and CWR-Cb (centralized placement below) (p < 0.05). The surface C:O (carbon:oxygen) ratio of sustained-release carbon source after water treatment showed a decreasing trend, and CWR-Cu exhibited the greatest decrease in C:O ratio. In summary, CWR-Cu achieved the highest utilization of the carbon source and produced the largest number of heterotrophic microorganisms. This study reveals that CWR-Cu is a structural process for the efficient removal of nitrogen and phosphorus pollutants, and our findings provide theoretical basis and technical support for actual projects.","Basic reporting
The article is very well written and with impactful research contributions in the field of COD, TN, and TP removal from wastewater. The use of UVFCW delivers excellent performance. However, the authors need to address few queries before the article can be accepted.
The authors need to justify the use of UVFCW? What do they mean by unsaturated and how it helps in creating the DO gradient needs to be justified.
The use of sustained release of carbon source needs to be justified.
Also, in the article it is written substained. Why is it so? It should be sustained right?

Experimental design
How is the influent concentration for COD, TP, ammonia are constant?
DO, ORP, and pH of influent and effluent and inside the reactor needs to be reported to establish nitrogen removal.
An actual figure of the reactor is required for the readers to better understand the configuration.
From Fig. 1 it is not clear how the carbon source is below or above the device? What do you mean by device?

Validity of the findings
No comment

Additional comments
No comment",Authentic
Time series (ARIMA) as a tool to predict the temperature-humidity index in the dairy region of the northern desert of Mexico,"The environment in which an animal is situated can have a profound impact on its health, welfare, and productivity. This phenomenon is particularly evident in the case of dairy cattle, then, in order to quantify the impact of ambient temperature (°C) and the relative humidity (%) on dairy cattle, the temperature-humidity index (THI) is employed as a metric. This indicator enables the practical estimation of the stress imposed on cattle by ambient temperature and humidity. A seasonal autoregressive integrated moving average (SARIMA) (4,1,0)(0,1,0)365 model was estimated using daily data from the maximum daily THI of 4 years (2016–2019) of the Comarca Lagunera, an arid region of central-northern Mexico. The resulting model indicated that the THI of any given day in the area can be estimated based on the THI values of the previous four days. Furthermore, the data demonstrate an annual increase in the number of days the THI indicates a risk of heat stress. It is essential to continue building predictive models to develop effective strategies to mitigate the adverse effects of heat stress in dairy cattle (and other species) in the region.","Basic reporting
The study is of general importance for livestock production, health and welfare. The study examines the predictive capacity of the THI in relation to potential heat stress by using the time series method (ARIMA MODEL). They have predicted THI using four previous days climatic data of previous four years. Accuracy of model prediction for THI has not discussed in detail ?. Results and discussion part is weak in the manuscript. Predictive capacity of model has to be discussed in detail.

Experimental design
Experimental design is okay

Validity of the findings
Validity of the findings is not explained properly.
Furthermore, the data demonstrates an annual increase in the number of days the THI indicates a risk of heat stress. This line is written in abstract, but not discussed in the results and discussion part.

Clearly state what is the gap, hypothesis of study?

Accuracy of model prediction for THI has not discussed in detail ?. Predictive capacity of model with results of model has to be discussed in detail.

Discussion part is not written well. Some points in the Paper has been written very technically, difficult for the readers to comprehend the interpretation.

Generally, it is a very well established fact that Increase in THI can be stressful for the animals. Repeatedly these points in the paper have been written. While the specific ARIMA model utility and its accuracy has not been written at all.

Additional comments
The paper is recommended for major revision.
Many mistakes in reference writing through out the text.",Authentic
Turbidivision: a machine vision application for estimating turbidity from underwater images,"The measurement of turbidity serves as a key indicator of water quality and purity, crucial for informing decisions related to industrial, ecological, and public health applications. As existing processes require both additional expenses and steps to be taken during data collection relative to photography, we seek to generate accurate estimations of turbidity from underwater images. Such a process could give new insight to historical image datasets and provide an alternative to measuring turbidity when lower accuracy is acceptable, such as in citizen science and education applications. We used a two-step approach to a machine vision model, creating an image classification model trained on image data and their corresponding turbidity values recorded from a turbidimeter that is then used to generate continuous values through multiple linear regression. To create a robust model, we collected data for model training from a combination of in situ field sites and lab mesocosms across suspended sediment and colorimetric profiles, with and without a Secchi disk for visual standard, and binned images into 11 classes 0–55 Formazin Nephelometric Units (FNU). Our resulting classification model is highly accurate with 100% of predictions within one class of the expected class, and 84% of predictions matching the expected class. Regression results provide a continuous value that is accurate to ±0.7 FNU of true values below 2.5 FNU and ±33% between 2.5 and 55 FNU; values that are less accurate than conventional turbidimeters but comparable to field-based test kits frequently used in classroom and citizen science applications. To make the model widely accessible, we have implemented it as a free and open-source user-friendly web, computer, and Google Play application that enables anyone with a modern device to make use of the tool, the model, or our repository of training images for data collection or future model development.","Basic reporting
In this article, the authors present a model they developed to estimate turbidity from underwater images. Their findings are robust, with an extensive set of data, good model performance, and well-explained methods. This paper's content is relevant to the field of water monitoring. However, there is substantial room for improvement in the paper's form. Many sentences are ambiguous, redundant, and do not follow scientific writing standards. Therefore, I do not recommend publishing the article as it is, but I encourage the authors to carefully revisit the form, as I believe that their findings are very promising. In this review, I used the structure provided by the PeerJ editors, and used a red/orange/green colors to evaluate each point. Each evaluation is followed by a detailed explanation.

RED: The article must be written in English and must use clear, unambiguous, technically correct text. The article must conform to professional standards of courtesy and expression.

Explanation: see comments in the document. I primarily concentrated on the abstract and the introduction to provide detailed comments, but the criticism is applicable to the whole document.

ORANGE: The article should include sufficient introduction and background to demonstrate how the work fits into the broader field of knowledge. Relevant prior literature should be appropriately referenced.

Explanation: The authors defined in the first paragraph of the introduction the relevance of turbidity in water quality assessment. However, despite using in the article statements such as line 47 “High turbidity can have a negative impact on aquatic life” or line 53 “overly turbid water is unsuitable for consumption by humans”, they did not provide any information on what is considered a high/normal/low turbidity value in the context of natural waters and drinking waters. This is a problem because the reader is not able to assess whether the measurement range (0-55NTU) is relevant in this context.

GREEN: The structure of the article should conform to an acceptable format of ‘standard sections’ (see our Instructions for Authors for our suggested format). Significant departures in structure should be made only if they significantly improve clarity or conform to a discipline-specific custom.

Comment: the article is brief, well-structured and information are presented where the reader expect them.

ORANGE: The submission should be ‘self-contained,’ should represent an appropriate ‘unit of publication’, and should include all results relevant to the hypothesis.

Explanation: The authors used the argument that historical dataset of underwater images could be analyzed with their method (line 75: “historical data from underwater images often lacks […]” or line 96 “gaining insights into historical data”). I agree that this is a very strong argument for the relevance of their model. Yet they did not provide any evidence that such data are available. I suggest citing existing dataset, and even (if possible) analyzing some of historical data with their algorithm, which would with little effort greatly improve the strength of the article.

Experimental design
The submission should clearly define the research question, which must be relevant and meaningful. GREEN: The knowledge gap being investigated should be identified, and statements should be made as to how the study contributes to filling that gap.

Comment: the introduction was well structured and highlights the importance of turbidity for water monitoring, as well as the limitations of currently available techniques for turbidity measurement, leading to the research gaps.

ORANGE: The investigation must have been conducted rigorously and to a high technical standard. The research must have been conducted in conformity with the prevailing ethical standards in the field.

Explanation:
There is, at difference instance, a lack of justification to explain why the authors choose certain methods over others:
• Line 122: “For 38% of all photos, we included the 4.2cm Secchi”. Why did you do that? What is the impact of the Secchi disk on the model performance? Did you try to train the model with images without Secchi disk? (This is relevant because for future use of the model most users will not have access to a Secchi disk).
• Line 148: “We used Ultralytics YOLOv8 classification”. Why did you selected this method? Did you try other approaches? Why do you believe that this method is suited for this task?
• Line 136: “the images were broken into 11 groups”. Why eleven? Why not 5? Did you optimite this number, or is it for practical reasons?

ORANGE: Methods should be described with sufficient information to be reproducible by another investigator.

Explanation: In general, the description about the data collection lacks information, which hinders reproducibility:
• Line 106: “whether the water was flowing”. Under which criteria was the water flowing? Did you use a flowmeter? Did you use this information in the analysis?
• What about the illumination? Did you use sunlight? Did you use the camera flash? Did you collect information about it?
• Line 115: “Images were collected by taking individual photographs […] and by saving a selection of frames from a video”. Why to acquisitions methods ? How many images are taken from each mode?

Validity of the findings
ORANGE: The data on which the conclusions are based must be provided or made available in an acceptable discipline-specific repository. The data should be robust, statistically sound, and controlled.

Explanation: I am concerned about the validity of the findings because some information about the dataset are missing. The authors used images takes from different sources, which is improving the range of application of their model. My concern is: is it possible that the model is not actually predicting turbidity, but the origin of the image?
Here is a possible scenario: all samples from rivers have a turbidity between 0 and 5 NTU, all samples from the lab experiment have a turbidity between 10 and 15 NTU, etc. In that scenario, the classification model could be predicting the origin of the image, which is itself correlated with turbidity. The problem would be that if one provide a new image to the model, it will not be able to predict turbidity accurately.
Solution: 1) provide information, for each sub-parts of the dataset, about the range of turbidity. What about, in Figure X_REF, using a color-code for each dot representing the origin of the sample? 2) provide information about the train-validate-test splitting of the data, regarding whether images from each source is present in each one of them. 3) Even better: keep a whole section of the dataset, for instance river images, from the training and validating set. If the model still performs well on this totally new images, it means it learned how to estimate turbidity from any images. This is important because future users of your app need to verify that the model will still perform if they take a picture in a totally different context.

ORANGE: The conclusions should be appropriately stated, should be connected to the original question investigated, and should be limited to those supported by the results. In particular, claims of a causative relationship should be supported by a well-controlled experimental intervention. Correlation is not causation.

Comment: see comment above.

Additional comments
I have two additional remarks/questions about the result analysis.

First, why did you present the model accuracy in percentage (above 2.5 FNU)? In my opinion, the main message of this article is that with a properly trained model, any camera can give a +/- 5 FNU estimation of the turbidity, no matter the range. By giving accuracy in %, you are confusing the reader who could believe that the accuracy increases with increasing turbidity (heteroscedasticity), which is apparently not the case (I am using Figure 2, but a residual plot would enable the reader to judge heteroscedasticity more clearly).

Second, why did you analyze the results differently below and above 2.5FNU? You can consider whether this is necessary, or if it is not overcomplicating things. In my opinion, this is not very relevant, as a natural water below 2.5 FNU can already be considered as very clean, not matter if the turbidity is 1.9 or 1.8 FNU. Again, in my understanding, your approach is not aiming at being as accurate as possible, but instead to be accurate enough to provide a cheap, accessible and quick way to estimate turbidity.",Authentic
Towards sustainable coastal management: aerial imagery and deep learning for high-resolution Sargassum mapping,"The massive arrival of pelagic Sargassum on the coasts of several countries of the Atlantic Ocean began in 2011 and to date continues to generate social and environmental challenges for the region. Therefore, knowing the distribution and quantity of Sargassum in the ocean, coasts, and beaches is necessary to understand the phenomenon and develop protocols for its management, use, and final disposal. In this context, the present study proposes a methodology to calculate the area Sargassum occupies on beaches in square meters, based on the semantic segmentation of aerial images using the pix2pix architecture. For training and testing the algorithm, a unique dataset was built from scratch, consisting of 15,268 aerial images segmented into three classes. The images correspond to beaches in the cities of Mahahual and Puerto Morelos, located in Quintana Roo, Mexico. To analyze the results the fβ-score metric was used. The results for the Sargassum class indicate that there is a balance between false positives and false negatives, with a slight bias towards false negatives, which means that the algorithm tends to underestimate the Sargassum pixels in the images. To know the confidence intervals within which the algorithm performs better, the results of the f0.5-score metric were resampled by bootstrapping considering all classes and considering only the Sargassum class. From the above, we found that the algorithm offers better performance when segmenting Sargassum images on the sand. From the results, maps showing the Sargassum coverage area along the beach were designed to complement the previous ones and provide insight into the field of study.","Basic reporting
All comments have been added in detail to the last section.

Experimental design
All comments have been added in detail to the last section.

Validity of the findings
All comments have been added in detail to the last section.

Additional comments
Review Report for ""Towards sustainable coastal management: Aerial imagery and deep learning for high-resolution Sargassum mapping"" in PeerJ

1. Within the scope of the study, classification and semantic segmentation processes were carried out using deep learning to detect Sargassum from aerial images.

2. In the Introduction section, what Sargassum is, its importance, its difficulties and basically semantic segmentation are mentioned by giving Figure-1. In this section, it is recommended to add a literature table consisting of columns such as dataset, model used, results, advantages/disadvantages, etc., in order to present the literature more clearly. Thus, the current topic and the literature can be compared better. In addition, it is recommended that the difference of the study from the literature and its main contributions to the literature be added more clearly at the end of the Introduction section.

3. In the Materials and Methods section, the semantic segmentation and dataset used in the study are basically mentioned. In the semantic segmentation section, first the u-net architecture, then the Pix2pix deep learning, a conditional generative adversarial network based on u-net, and the customizations made are explained. Although there are many different deep learning models that can be used for semantic segmentation when the literature is examined, it should be explained more clearly why the Pix2pix model is preferred. In the study of this model, the developed originality point, algorithm and hyperparameters used need to be explained in detail.

4. 15,268 aerial images with a 3-class structure consisting of ""Sargassum, Sand and Other"" classes were used as the dataset. The dataset used in the study is sufficient in quantity, size and quality. It has been stated that the dataset distribution is randomly divided into 80% training and 20% testing. The results obtained in classification and segmentation problems are very dependent on the dataset. For this reason, it is very important how the dataset distribution is made and how the training, validation and/or testing sections are determined. In order to analyze the results more accurately, it would be better to perform cross-validation instead of random distribution of the dataset. Based on this, it should be stated more clearly why cross-validation is not preferred and/or the dataset distribution ratio (80:20) and the basis on which the images are selected.

5. There are serious deficiencies in the evaluation metrics regarding classification and semantic segmentation in the results section. For example, metrics such as ROC curve, AUC score, precision-recall curve for both the triple class (Sargassum, Sand and Other) and binary class (Sargassum and Other) that are required to analyze the classification results correctly are not included.

As a result, although the study is important in terms of the dataset used and the problem addressed, it is recommended to pay attention to the parts explained in detail above.",Authentic
"Comprehensive analysis of bioplastics: life cycle assessment, waste management, biodiversity impact, and sustainable mitigation strategies","Coral resilience varies across species, with some exhibiting remarkable stability and adaptability, often mediated by their associated microbiomes. Given the species-specific nature of coral-microbiome interactions, investigating the microbiomes of urban-adapted corals provides critical insights into the health, dynamics, and functioning of coral holobionts. In this study, we examined the microbiome of Madracis auretenra, a Caribbean coral from Santa Marta, Colombia, across contrasting environmental conditions. Over two years, we compared the microbiomes of healthy and stressed coral colonies from two distinct reef habitats—urban and protected—using 16S rRNA gene sequencing (V4 region) to assess microbial diversity. Our findings revealed microbial richness and diversity were primarily influenced by seasonal and local factors rather than host-specific traits such as interaction with algae, health status, or microhabitat. These variations were not substantial enough to disrupt the overall microbial community structure, which remained stable across temporal and spatial scales. Dominant taxa included Endozoicomonas, along with Vibrionaceae and Rhodobacteraceae, which form dense ecological interaction networks. Notably, nutrient and oxygen levels emerged as key drivers of microbiome fluctuations, yet Vibrionaceae populations exhibited exceptional temporal stability. These findings highlight the presence of a well-structured and resilient coral microbiome with minimal seasonal variability, even in urban-influenced environments. We propose that the dominance of Endozoicomonas and the stability of Vibrionaceae populations play a pivotal role in maintaining microbiome balance, ultimately contributing to the ecological resilience of M. auretenra in dynamic reef habitats.","Basic reporting
Clear writing and good structure. Sufficient references used correctly in context. Figures included are relevant, contributing to a self-contained study.

Experimental design
This is an original contribution, with a research question well defined; the methodology is detailed and well explained.

Validity of the findings
The discussion has been rewritten to present the main findings clearly. The information presented is of great interest, highlighting resilient corals that grow in urban sites, yet having a stable microbial community. The dominance of Endozoicomonas is also observed in these urban reefs.

Additional comments
The authors have done a great job in the resubmitted manuscript. Suggestions by reviewers have been considered and now the ms. is more concise, better structured, easy to read and follow, and highlights the main findings of the study.
I think the manuscript is ready for acceptance.",Generic
Endozoicomonas dominance and Vibrionaceae stability underpin resilience in urban coral Madracis auretenra,"Coral resilience varies across species, with some exhibiting remarkable stability and adaptability, often mediated by their associated microbiomes. Given the species-specific nature of coral-microbiome interactions, investigating the microbiomes of urban-adapted corals provides critical insights into the health, dynamics, and functioning of coral holobionts. In this study, we examined the microbiome of Madracis auretenra, a Caribbean coral from Santa Marta, Colombia, across contrasting environmental conditions. Over two years, we compared the microbiomes of healthy and stressed coral colonies from two distinct reef habitats—urban and protected—using 16S rRNA gene sequencing (V4 region) to assess microbial diversity. Our findings revealed microbial richness and diversity were primarily influenced by seasonal and local factors rather than host-specific traits such as interaction with algae, health status, or microhabitat. These variations were not substantial enough to disrupt the overall microbial community structure, which remained stable across temporal and spatial scales. Dominant taxa included Endozoicomonas, along with Vibrionaceae and Rhodobacteraceae, which form dense ecological interaction networks. Notably, nutrient and oxygen levels emerged as key drivers of microbiome fluctuations, yet Vibrionaceae populations exhibited exceptional temporal stability. These findings highlight the presence of a well-structured and resilient coral microbiome with minimal seasonal variability, even in urban-influenced environments. We propose that the dominance of Endozoicomonas and the stability of Vibrionaceae populations play a pivotal role in maintaining microbiome balance, ultimately contributing to the ecological resilience of M. auretenra in dynamic reef habitats.","Basic reporting
The manuscript has been carefully reviewed and meets the scientific and editorial standards of PeerJ. The suggested revisions have been successfully incorporated, resulting in a clear and well-structured document with solid methodological and bibliographic support. The tables and figures have been improved for better clarity, enhancing the presentation of the results. Additionally, the revised title more accurately reflects the study’s content and scope.

The writing is professional, clear, and appropriate for a scientific journal, with no major grammatical errors. The manuscript includes relevant and up-to-date references, following PeerJ’s guidelines. The figures and tables are well-organized, and the availability of raw data in public databases ensures compliance with the journal’s requirements.

Experimental design
The research question is clear and relevant, with an improved introduction that follows previous recommendations and highlights the importance of studying microbiomes in urban corals. The methodology has been refined for greater clarity and reproducibility, providing a more precise description of data processing and the controls implemented to prevent contamination and sampling biases.",Authentic
Advancing molecular macrobenthos biodiversity monitoring: a comparison between Oxford Nanopore and Illumina based metabarcoding and metagenomics,"DNA-based methods and developments of sequencing technologies are integral to macrobenthos biodiversity studies, and their implementation as standardized monitoring methods is approaching. Evaluating the efficacy and reliability of these technological developments is crucial for macrobenthos biodiversity assessments. In this study, we compared three DNA-based techniques for assessing the diversity of bulk macrobenthos samples from the Belgian North Sea. Specifically, we compared amplicon sequencing using Illumina MiSeq and portable real-time sequencing of Oxford Nanopore versus shotgun sequencing using Illumina NovaSeq sequencing. The 313 bp mitochondrial cytochrome c oxidase subunit I (COI) metabarcoding fragment served as the target region for the metabarcoding analysis. Our results indicate that Oxford Nanopore and MiSeq metabarcoding had similar performances in terms of alpha and beta diversity, revealing highly similar location-specific community compositions. The NovaSeq metagenomics method also resulted in similar alpha diversity, but slightly different community compositions compared to the metabarcoding approach. Despite these differences, location-specific community compositions were maintained across all platforms. Notably, read counts from the NovaSeq metagenomic analysis showed the weakest correlation to size corrected morphological abundance and there were mismatches between morphological identification and all DNA based findings which are likely caused by a combination of factors such as primer efficiency and an incomplete reference database. Our findings underscore the critical importance of database completeness prior to implementing DNA-based techniques as standardized monitoring method, especially for metagenomics. Nevertheless, our findings emphasize that Oxford Nanopore metabarcoding proves to be a viable alternative to the conventional Illumina MiSeq metabarcoding platform for macrobenthos biodiversity monitoring.","Additional comments
This manuscript by Doorenspleet et al. is a timely comparison among three molecular methods and the morphological approach for molecular assessment of marine benthic metazoan biodiversity. Their methods are sound and their results are clear and conclusive, and they will be of interest for many ecological researchers that are currently wondering which molecular method is best for biodiversity assessment of marine eukaryotic communities, and possibly other ecosystems, since their results are probably translatable to other eukaryotic communities such as freshwater or terrestrial soils.
The manuscript is in general, well written, and provided that some minor corrections are addressed, which I detail below, I think that it can be publishable in PeerJ.

Minor corrections:

-Abstract:
L36: Correct the length of the fragment to ""The 313 bp COI Leray region""
L47: Remove duplicated words: ""standardized monitoring method.""

-Introduction:
L87 and L89: Correct ""Illumina MiSeq"" to ""Illumina technologies"". I do not think that ""Illumina MiSeq is currently the standard platform, since many laboratories have moved to Illumina NovaSeq and they are not using the MiSeq anymore. ""Illumina technologies are currently the standard platform"" and ""In comparison to Illumina technologies, the Oxford Nanopore…""
L116: Correct to: ""paired-end Illumina MiSeq metabarcoding""

-Materials and methods
L137: What was the full volume (size) of the Van Veen grab? Please specify.
L144: Anthozoa is currently considered a subphylum (McFadden et al., 2022), with two classes: Hexacorallia and Octocorallia. If anthozoans were not further classified in this study, then you should correct the word ""class"" to ""subphylum"". If they were classified either as Hexacorallia or Octocorallia, then it is fine to keep the rank ""class"", even though it can be a little misleading in this context. [McFadden, C. S., van Ofwegen, L. P., & Quattrini, A. M. (2022). Revisionary systematics of Octocorallia (Cnidaria: Anthozoa) guided by phylogenomics. Bulletin of the Society of Systematic Biologists, 1(3), 1–79.]
L162: Correct the details of the incubation instructions: ""were incubated overnight at 56 °C in the power-beads tube of the DNeasy PowerSoil Kit (QIAGEN), supplemented with 10 µL of proteinase K (20 mg/ml).""
L169. With the current description, it is ambiguous and difficult to know which primers were used for the amplification. I can see from Van den Bulcke et al. (2021) that the original Leray primer set were not used, but a modified version, produced by replacing the deoxy-inosines (I) by totally degenerated bases (N). These are not the primers from Leray et al. and you should specify this here clearly. Specially, since you used the KAPA HiFi Taq polymerase, which does not work properly with deoxy-inosines. So, please rewrite these sentences.
L268: Change ""normalized"" to ""transformed"" in ""Therefore, the data were not rarefied but transformed using a log10 transformation"".

-Results:
L290: ""3,060,417,120 data"" is ambiguous. Change to ""shotgun reads""
L349: Correct the name: ""Crepidula fornicata""
L354: Correct the name: ""Nephtys cirrosa""
L369: Delete the first ""the"" from ""the both the""
L375: Correct the name ""Scolelepis bonnieri""

-Discussion:
L466: Remove ""rRNA"" from ""contained only COI sequences"" or explain it more clearly, in the case that the database contained sequences from COI and other rRNA markers.
L486: Correct ""the mitogenomes of mock community were available""
L522: Correct the name: ""the Anthozoa Cylista""

-Data Availability:
L567-569: Improve the syntax of the data availability statement. Remove the first ""are available"", remove the s from ""Custom""",Authentic
"Diversity of lanternfish (Myctophidae) larvae along the Ninety East Ridge, Indian Ocean","Since the 19th century, the impact of seamounts on the distribution of plankton has been a topic of considerable interest. The influence of seamounts on the biogeographic patterns of marine organisms is complex, with some aspects still under debate. It is generally accepted that seamounts can drive the upwelling of nutrient-rich deep waters. Tidal amplification, flow acceleration, and internal waves can further enhance vertical mixing, leading to increased primary productivity near seamounts. Seamounts may also act as barriers to the migration of marine organisms, affecting gene flow. Research on Pacific seamounts suggests these features might serve as “stepping stones” for the dispersal of marine species across the ocean. However, investigations of seamounts in the eastern Indian Ocean remain limited. Focusing on the Ninety East Ridge region in the eastern Indian Ocean, this study collected zooplankton samples using horizontal (surface) and vertical (0–200 m) plankton nets and measured temperature and salinity profiles with a conductivity, temperature, and depth (CTD) sensor. A total of 544 fish larvae were identified, including 260 lanternfish larvae, representing 38 species across 12 genera, determined through COI DNA barcoding. Phylogenetic trees and haplotype networks were constructed to analyze genetic distances and population structures of lanternfish species. Among the samples, intra-specific genetic distances ranged from 0% to 2.99%, while inter-specific distances ranged from 1.88% to 25.71%. Except for Notolychnus valdiviae (Brauer, 1904), the maximum intra-specific distances were lower than the minimum inter-specific distances for all species. Haplotype analysis of nine species revealed significant variations in haplotype number, structure, and spatial distribution. Specifically, Ceratoscopelus warmingii (Lütken, 1892) and N. valdiviae exhibited a notable north-south divergence pattern, consistent with the temperature and salinity distribution of the region’s water masses. This conclusion was supported by analysis of molecular variance analysis, suggesting that larval stages of certain lanternfish species may struggle to cross boundaries between water masses. However, the remaining species showed no significant north-south distribution differences, possibly due to their adaptive capabilities, vertical migration patterns, or the duration of their planktonic larval stages. These findings suggest that seamounts and water mass distribution have varying implications for lanternfish species, potentially influencing gene flow and horizontal distribution patterns, which could contribute to speciation. Global climate change-induced alterations in ocean currents may profoundly impact the genetic diversity of fish species. This study provides new insights into the diversity of lanternfish in the Ninety East Ridge region and offers valuable data for understanding the biogeography of seamounts.","Validity of the findings
Manuscript Review: ""Lanternfish Larvae Distribution and Population Dynamics in the Ninety East Ridge, Indian Ocean""
Summary: The manuscript is an interesting and comprehensive study on the genetic diversity and distribution patterns of lanternfish larvae from the Ninety East Ridge, identified using COI barcoding to 38 species. The study compared the effects of different net types on sampling, explored the genetic differentiation of species across water masses, and looked into population expansion patterns for key species, C. warmingii and N. valdiviae. The findings of the study provide valuable information on the role of water masses in shaping genetic diversity and evolutionary lineages in lanternfish species.
Strengths:
The integration of molecular techniques, such as COI barcoding, with ecological sampling provides a robust and thorough approach to studying larval distribution patterns and population structure. Both genetic and ecological data are combined to strengthen the conclusions of the study.
In-depth analysis on genetic patterns may be made on the construction of a phylogenetic tree, haplotype networks, besides AMOVA analyses, giving an effective interpretation of the structure of lanternfish populations. To this extent, the finding about water masses presenting barriers to certain species' gene flow adds new dimensions to such ecological factors associated with genetic differences in the Indian Ocean.
Focus on Population Dynamics: Analysis of population expansion in C. warmingii and N. valdiviae using neutrality tests, mismatch distribution, and EBSP analyses provides informative information on the evolutionary history of these species. Variability in the timing of expansions across species adds nuance to our understanding of their population dynamics.
These studies may have clear implications with respect to the conservation of lanternfish populations, both in the context of climate change and marine protected areas. The study would also be useful in providing a foundation for future monitoring efforts and devising conservation strategies.
Improvements:
Hypothesis Testing for SEI-29: The finding that the aggregation of haplotypes at the SEI-29 station is due to its seamount location on an isolated seamount is intriguing. However, further sampling from surrounding areas-perhaps especially south-west of the station-would strengthen the argument. In fact, it would be helpful to discuss possible alternative explanations for this observation, such as oceanographic current patterns or local ecological conditions.
Discussion of Potential Ecological Implications: The manuscript might further develop the ecological significance of the observed genetic divergence and population structure of lanternfish larvae. For example, how will such divergence between C. warmingii and N. valdiviae affect the roles of each in the marine food web or vulnerability to changes such as those related to climate change?

Additional comments
Minor Comments:
More clearly stating the study's key research questions and objectives in the introduction may help present those study's main questions and objectives more coherently. Overall, the background is pretty well presented; however, it is hard to immediately see what the ultimate research questions are.",Authentic
Susceptibility of lymnaeid snails to Fasciola hepatica and Fasciola gigantica (Digenea: Fasciolidae): a systematic review and meta-analysis,"Background
Fasciolosis is a food-borne disease that causes major economic losses, globally. This zoonotic disease is caused by Fasciola hepatica and Fasciola gigantica species which employ freshwater snails from the family Lymnaeidae as their intermediate hosts. Thus, a key aspect of understanding the epidemiology of the disease lies in understanding the transmission ecology of the parasite. Therefore, this systematic review and meta-analysis were conducted to assess the experimental susceptibility and prevalence of natural infections of F. hepatica and F. gigantica in lymnaeid snails.

Methods
Relevant peer-reviewed articles published in the past 20 years (2004–2023) were searched and appraised. Prevalence and infection rate estimates were based on 41 studies that met the inclusion criteria.

Results
Five thousand five hundred and seventy-five (5,575) lymnaeid snails were subjected to experimental infections and 44,002 were screened for natural infections. The overall pooled infection rate was higher in experimental infections 50% (95% CI [42–58%]) compared to natural infections of field-collected snails 6% (95% CI [0–22%]). The highest pooled infection rate was recorded in South America at 64% (95% CI [48–78%]) for experimental infections while the lowest was recorded for natural infections at 2% (95% CI [0–6%]) in Europe and 2% (95% CI [0–17%]) in Asia. In experimental studies, F. gigantica recorded the highest pooled prevalence at 73% (95% CI [61–84%] compared to F. hepatica which recorded 47% (95% CI [38–56%]). For natural infections, however, F. hepatica had the highest prevalence (12% (95% CI [0–30%]) while the lowest was noted for naturally infected F. gigantica at 2% (95% CI [0–18%]). Based on the snail species, the highest pooled prevalence was recorded for Pseudosuccinea columella infected with F. hepatica and F. gigantica at 47% (95% CI [33–61%]) while the lowest was recorded for F. hepatica naturally infected Galba truncatula at 4% (95% CI [0–10%]). Natural Fasciola spp. infections in intermediate snail hosts decreased in prevalence while experimental infections have increased in prevalence over the past 20 years.

Conclusions
While there seems to be a strong intermediate host specificity between the two Fasciola spp., experimental infection results showed that G. truncatula and R. natalensis are susceptible to F. hepatica and F. gigantica, respectively.","Basic reporting
The manuscript is written in good, passable English, although I have proposed rephrasing of some sentences in a few instances (see attached pdf). Relevant literature is cited sufficiently to cover the study content. However, 26 extra references are listed that do not appear in text citations. I'm not sure if this can be related to errors in the reference manager software (as is sometimes the case), or the authors simply failed to thoroughly check the list. This is so glaring.

The review article is well structured and the results are relevant, bringing out an important picture on the global prevalence of fascioliasis from experimental studies and natural infections. The results adequately satisfy the research hypothesis.

Experimental design
The review study is well designed using standard, well-justified methods. and the study is within the scope and aims of the journal. The meta-analysis is technically sound and all data collection was standardised optimally, despite difficulties with the variability of the literature assessed.

Validity of the findings
The data was meaningfully presented and analysed. Tables 1 and 2 can benefit by adding columns showing the prevalence for each parasite species, as well as the overall prevalence for each method.

Additional comments
I have added a few comments in the attached annotated pdf.",Authentic
Investigating passive eDNA samplers and submergence times for marine surveillance,"Passive environmental DNA (eDNA) samplers offer a cost-effective and scalable approach to marine biodiversity monitoring, potentially aiding detections of non-indigenous species. This study explored the efficiency of passive eDNA samplers to detect a variety of globally problematic marine invasive species in field conditions: Sabella spallanzanii, Styela clava, Bugula neritina and Undaria pinnatifida. Four passive sampler substrates, nylon filters, positively charged nylon discs, nylon mesh, and artificial sponges, were tested across six submergence times, ranging from 10 to 720 min, against standard filtration-based approaches. Our results demonstrated that passive samplers could achieve comparable or even higher eDNA yields than traditional active filtration methods, indicating their potential for biosecurity surveillance. Species-specific droplet-digital PCR (ddPCR) assays provided sensitive and quantifiable eDNA signals, though assay validation remains crucial to avoid false negatives. Significant variation in eDNA signal detection highlighted the importance of considering both material selection and submersion time, depending on the targeted organisms. Furthermore, 18S rRNA metabarcoding was undertaken to assess how the overall detected biodiversity might interfere with species-specific detections. Certain sessile organisms, such as ascidians and polychaetes, dominated early representation on the passive filters but did not interfere with species-specific detection. By optimizing material selection, submersion time, and assay validation, passive eDNA sampling can enhance the sensitivity and reliability of eDNA-based monitoring, contributing to improved marine biosecurity and conservation efforts.","Basic reporting
Very clear and concise writing with good coverage of all relevant literature. Raw data is shard on SRA. Hypotheses could be better defined with rationalizations e.g was the reduction in ASV richness over time expected?

Experimental design
Very good experimental design to address the research questions.

Validity of the findings
The contrary results between the ddPCR and metabarcoding could be explored further that would make this article more interesting. It would also be interesting to know what aspects of the study the authors would encourage replication for, are both materials and submergence times equally important as independent variables in future studies? Should future studies use both ddPCR/qPCR and metabarcoding for their assessments ? What might be the risks of only using one metric as seen in this study.

Additional comments
I have reviewed the manuscript titled “Investigating passive eDNA samplers and submergence times for marine surveillance.” The manuscript builds on valid research questions raised by previous studies: How is the efficacy of passive samplers impacted by submergence times, and do the sampling materials influence these trends? These questions were investigated via empirical testing with ddPCR for the detection of four invasive species, as well as 18S rRNA metabarcoding. This study is clear and well-written. I have only suggested minor edits in the text.

However, the data from ddPCR and metabarcoding seem to show opposite trends. This is quite interesting and should be highlighted more clearly in the text and discussed in greater detail to reduce the potential for confirmation bias as readers evaluate this highly interesting and relevant study. I believe that with these concerns addressed, this study is fit for publication in PeerJ.

Specific Comments:
Line 85: The sentence cites two studies, but a third study’s results are elaborated on in the following sentence to back up the claim.
Line 127: This part needs to be explained better: “and additionally the water squeezed from the Whirl-PakÆ Speci-Sponges.”
Line 236: NIS needs to be defined.
Line 242: Was the sample data rarefied? It says that they were rarefied in line 247, but no threshold is mentioned.
Line 322: Did NIS detection increase over time with passive sampling? This seems confusing, as line 303 mentions that NIS decreased with time. Perhaps this needs more clarification.
Line 397: Does this refer to eDNA yields or copy number of targeted NIS?
Methodological Concerns:
ddPCR methods/results: Assay kinetics and LOD/LOQ must be reported based on standardized guidelines. See:
Klymus, Katy E., et al. ""Reporting the limits of detection and quantification for environmental DNA assays."" Environmental DNA 2.3 (2020): 271-282.
Figures:
Figure 3: The regressions are not linear. Is the relationship between eDNA accumulation and time expected to be non-linear/exponential? What is the mechanistic rationale for choosing the given regression method (GLM with exponential link function) for this application?
Figure 4B: Do these scatterplots also include the NIS detected by the ddPCR assays? Were they co-detected with both analytical methods but showed different trends with submergence time?
The ASV richness seems to decline with submergence time, while the ddPCR detection of invasive species eDNA seems to increase in most cases (Figures 3, 4). Authors should discuss this in greater detail. Could this all be attributed to degradation and biofouling? It would also be interesting to see the ASV richness of the active filtration samples alongside these plots.
Figure 5B: What are the white spaces in Figure 5B? Relative abundance plots should not have white spaces. Is it all B. leachii? Are there non-invasive taxa represented here?
Figure 6: Interesting! The beta community composition at longer submergence time points is more different from the initial time point. This might be due to reduced ASV richness over time, as shown in Figure 4. However, the increasing beta dissimilarity, as shown in Figure 6, could be easily misinterpreted as a function of increased diversity captured over submergence time due to intuitive hypotheses and signals from Figure 4. I would encourage the authors to use a different visualization to convey this information or add additional details to avoid misinterpretation.",Authentic
Future climate-driven habitat loss and range shift of the Critically Endangered whitefin swellshark (Cephaloscyllium albipinnum),"Climate change is driving many species to shift their geographical ranges poleward to maintain their environmental niche. However, for endemic species with restricted ranges, like the Critically Endangered whitefin swellshark (Cephaloscyllium albipinnum), endemic to southeastern Australia, such dispersal may be limited. Nevertheless, there is a poor understanding of how C. albipinnum might spatially adjust its distribution in response to climate change or whether suitable refugia exist for this species in the future. Therefore, to address this gap, this study utilised maximum entropy (MaxEnt) modelling to determine the potential distribution of suitable habitat for C. albipinnum under present-day (2010–2020) climate conditions and for future conditions, under six shared socioeconomic pathways (SSP1-1.9, SSP1-2.6, SSP2-4.5, SSP3-7.0, SSP4-6.0 and SSP5-8.5) for the middle (2040–2050) and end (2090–2100) of the century. Under present-day conditions (2010–2020), our model predicted a core distribution of potentially suitable habitat for C. albipinnum within the Great Australian Bight (GAB), with benthic primary productivity and surface ocean temperature identified as key distribution drivers. However, under all SSP scenarios, future projections indicated an expected range shift of at least 72 km, up to 1,087 km in an east-southeast direction towards Tasmania (TAS). In all future climate scenarios (except SSP1-1.9 by 2100), suitable habitat is expected to decline, especially in the high-emission scenario (SSP5-8.5), which anticipates a loss of over 70% of suitable habitat. Consequently, all future climate scenarios (except SSP1-1.9 by 2100) projected a decrease in suitable habitat within a currently designated marine protected area (MPA). These losses ranged from 0.6% under SSP1-1.9 by 2050 to a substantial 89.7% loss in coverage under SSP5-8.5 by 2100, leaving just 2.5% of suitable habitat remaining within MPAs. With C. albipinnum already facing a high risk of extinction, these findings underscore its vulnerability to future climate change. Our results highlight the urgency of implementing adaptive conservation measures and management strategies that consider the impacts of climate change on this species.","Basic reporting
This study titled ""Projected Habitat Loss and Range Shift of the Critically Endangered Whitefin Swellshark (Cephaloscyllium albipinnum) in response to climate change"" is well designed and the modeling is important to see whether swellshark will affect from climate change or not. The studies for such critical animals are valuable and it is obvious that they will fill an important gap in the literature.

Experimental design
I have a main question.
Why did not authors use Bio-ORACLE v3.0 (Assis et al., 2024)?This version was produced from the CMIP6 Earth system models, and for the future conditions, they have a lot of layers. The different layers can be shown different results for the model.

Assis, J., Fernández Bejarano, S.J., Salazar, V.W., Schepers, L., Gouvêa, L., Fragkopoulou, E., Leclercq, F., Vanhoorne, B., Tyberghein, L., Serrão, E.A., Verbruggen, H., De Clerck, O. (2024) Bio-ORACLE v3.0. Pushing marine data layers to the CMIP6 Earth system models of climate change research. Global Ecology and Biogeography. DOI: 10.1111/geb.13813.

Other comments:
Minor comments:
In Occurrence data section on line 113.
How many did you obtain occurrence data from GBIF? and after all filtration process, how many data have you continued to model? This status should be mentioned here.

On line 138
There is a new version for Bio-ORACLE.

On line 165
Do you have a citation for this threshold value?

On line 195
Why didn't you include other feature classes?

On line 221
Please rewrite ""Were areas with.......""

Validity of the findings
These findings are enough.",Authentic
Association of red blood cell distribution width-platelet ratio with mortality after coronary artery bypass grafting,"Background
This study aims to explore the association between red blood cell distribution width-platelet ratio (RPR) and mortality in patients after coronary artery bypass grafting (CABG).

Methods
Data on patients who underwent CABG from January 1, 2021, to July 31, 2022, were retrospectively collected. The locally weighted scatter plot smoothing (Lowess) method was utilized to display the crude association between RPR and in-hospital mortality. The areas under the receiver operating characteristic curves (AUC) were used to assess the discrimination. The cut-off value (0.107) of RPR was calculated using the Youden index method. The primary outcome was in-hospital mortality.

Results
In total, 1,258 patients were included. The Lowess curve showed an approximate positive linear relationship between RPR and in-hospital mortality. In the multivariable logistic regression model, RPR was an independent risk factor (OR 1.493, 95% CI [1.119–1.992] per standard deviation (SD) increase, p = 0.006) for in-hospital mortality after CABG. RPR (AUC 0.716, 95% CI [0.617–0.814]) demonstrated greater discrimination than RDW (AUC 0.578, 95% CI [0.477–0.680], p = 0.002). The cut-off value (0.107) of RPR was calculated for further analysis, and groups were further divided into the high RPR group (≥ 0.107) and the low RPR group (< 0.107). In the multivariable logistic regression model, high RPR (≥ 0.107) correlated with elevated risks of in-hospital mortality (OR 6.097, 95% CI [2.308–16.104], p < 0.001) and one-year mortality (OR 6.395, 95% CI [2.610–15.666], p < 0.001) after adjusting for all included covariates. Subgroup analyses revealed that high RPR consistently had increased risks of in-hospital mortality and one-year mortality. Besides, patients with low RPR show better one-year survival than those with high RPR.

Conclusion
Preoperative high RPR could serve as an independent risk predictor for in-hospital mortality and one-year mortality, which can be utilized to assess the prognosis of patients and further provide guidance for the treatment in patients following CABG.","Basic reporting
unambiguous professional English is used throughout.
Literature references, and sufficient field background/context are provided.
Professional article structure, figures, tables. Raw data shared.
The manuscrit is Self-contained with relevant results to hypotheses.

Experimental design
The researchh question i well-definedd, relevant & meaningful. It is stated how research fills an identified knowledge gap
Original primary research within the Aims and Scope of the journal.
Rigorous investigation performed to a high technical & ethical standard.
Methods described with sufficient detail & information to replicate.

Validity of the findings
Impact and novelty not assessed. Meaningful replication encouraged where rationale & benefit to literature is clearly stated
All underlying data have been provided; they are robust, statistically sound, & controlled
Conclusions are well stated, linked to original research question & limited to supporting results

Additional comments
I want to thank the authors for agreeing with the suggestions made by the reviewers and for including them in the manuscript, which I believe will greatly interest the entire scientific community concerned with this topic.",Generic
Mehran vs. Mehran2 pre-procedure: which score better predicts risk of contrast-induced acute kidney injury in patients with acute coronary syndrome?,"Background
Contrast-induced acute kidney injury (CI-AKI) is a significant concern during percutaneous coronary intervention (PCI) procedures. The novel Mehran 2 pre-procedural risk score, an updated version of the original Mehran score, shows promise as a predictive tool. However, its effectiveness specifically in acute coronary syndrome (ACS) patients requires further investigation. This study aims to evaluate the performance of Mehran 2 pre-procedure risk score compared to original score in predicting CI-AKI risk in acute coronary syndrome patients undergoing PCI.

Material and Methods
A prospective cohort study was conducted with patients with ACS undergoing PCI, who were followed up for 90 days (December 2019–February 2021). The Mehran 2 CI-AKI risk score with pre-procedure data was compared with the original Mehran score. Receiver operating characteristic (ROC) curve and area under the ROC curve (AUC-ROC) were used to evaluate the discriminative capacity.

Results
192 patients were analyzed and 33% (n = 64) developed CI-AKI. CI-AKI outcome was associated with advanced age, arterial hypertension, chronic kidney disease, troponin T, hemodynamic instability, serum hemoglobin, serum creatinine, and higher both Mehran scores. Both scores demonstrated good agreement. The original Mehran score demonstrated superior CI-AKI stratification with higher sensitivity (85.94%) and specificity (60.16%) compared to the Mehran 2 pre-procedural score (sensitivity 50%, specificity 75%). Significant differences were observed in the discriminative performance between both scores.

Conclusion
Sociodemographic, clinical, and laboratory variables were associated with CI-AKI. The original Mehran score demonstrated more consistent discriminative capacity for predicting CI-AKI risk in ACS patients undergoing PCI compared to the Mehran 2 pre-procedural score.","
Additional comments
In the index report, the authors explore the relationship and predictive ability of risk of contrast-induced acute kidney injury in patients with acute coronary syndrome using Mehran vs. Mehran2 scores. They demonstrated that The original Mehran score was more consistently discriminative for predicting CI-AKI risk in ACS patients undergoing PCI compared to the Mehran 2 Pre-procedural score. The analysis is timely, clinically relevant and of great use to physicians, and cardiologists. Tables and figures are used effectively to present complex data. The study employs robust statistical techniques, including ROC curve analysis and Poisson regression, to assess predictive capacity and validate findings. The comparison between scores is well-structured. Key metrics such as sensitivity, specificity, and AUC-ROC are systematically reported.

I have minor comments:-
1) The manuscript contains repetitive phrases and occasional complex sentence structures, which could hinder readability.
2) : Simplify sentences and eliminate redundancies. For example, replace ""the performance of the Mehran 2 Pre-procedural score was slightly inferior"" with ""The Mehran 2 Pre-procedural score underperformed slightly.""
3) The figures, such as the ROC curve, lack sufficient detail in their captions. Table labels are not consistently formatted, and legends can be improved for clarity.
4) Revise captions to be self-explanatory. Ensure consistent formatting and include details like ""confidence intervals"" or ""statistical tests used.""
5) The introduction provides an adequate background but could expand on the rationale for focusing on ACS patients exclusively.
6) Explain the clinical implications of excluding stable angina patients, particularly how this affects the broader applicability of findings.
7) The study population is relatively small and limited to a single center in Brazil, which may limit generalizability. Highlight this limitation more prominently and discuss its impact on external validity. Suggest multicenter studies for future research.
8) The study uses AUC-ROC as a key metric but does not explore other complementary measures of predictive power, such as reclassification indices. Include alternative metrics (e.g., net reclassification improvement) to provide a more comprehensive assessment of score performance.
9) The manuscript adequately explains most statistical tests but lacks details on model assumptions for Poisson regression. State how assumptions (e.g., no overdispersion) were tested and whether the Poisson model was compared with alternatives like negative binomial regression.
10) The study notes differences in findings compared to prior research but does not deeply analyze potential causes. Discuss these differences in the context of population characteristics, such as ethnicity or comorbidities, and their effect on score validity.
11) Highlight scenarios where the Mehran2 score's superior specificity could prevent unnecessary interventions.
12) Discuss the relevance of the original Mehran score's higher sensitivity for early detection in at-risk populations
13) Suggest a hybrid approach: using the original Mehran score for ACS patients at higher procedural risk and the Mehran2 score for broader, pre-procedural stratification.
14) Add a side-by-side comparison of score performance across patient subgroups to visually represent nuances in applicability",Authentic
Understanding uremic cardiomyopathy: from pathogenesis to diagnosis and the horizon of therapeutic innovations,"Uremic cardiomyopathy (UC) is a significant cardiovascular complication in individuals with end-stage renal disease. This review aims to explore the multifaceted landscape of UC, including the key pathophysiological mechanisms, diagnostic challenges, and current therapeutic approaches. The prevalence of cardiac hypertrophy, as a hallmark of UC, is highlighted and some new insights to its intricate pathogenesis, involving uremic toxins, oxidative stress, and inflammatory responses is elucidated. Diagnostic complexities, including the absence of specific biomarkers, are discussed, and the need for advanced imaging modalities and emerging diagnostic strategies are emphasized. Current therapeutic interventions, although lacking specificity, are addressed, paving its way to the potential future directions in targeted therapies. The review concludes new insights into the critical importance of ongoing research and technological advancements which will enhance early detection, precision treatment, and ultimately improve outcomes for individuals with UC.","Basic reporting
The introduction effectively sets the stage by defining UC as a serious complication of chronic kidney disease (CKD) and distinguishing it from other forms of cardiomyopathy. The emphasis on diastolic dysfunction and left ventricular hypertrophy (LVH) as primary features is well articulated, supported by relevant citations. The mention of UC's association with cardiac fibrosis and sudden cardiac death underscores its clinical significance, particularly in end-stage renal disease (ESRD) patients.

Experimental design
The study design is thorough, detailing a systematic literature search across multiple databases. The inclusion and exclusion criteria are clearly defined, ensuring the credibility of the selected studies. This section could benefit from a brief explanation of how the findings were synthesized, which would provide further clarity on the review process.

Validity of the findings
The article is well-researched and informative, addressing a critical area of concern in nephrology and cardiology. It successfully communicates the importance of understanding UC for improving patient outcomes in those with CKD.

Additional comments
Suggestions for Improvement
1. The article could benefit from a more detailed discussion on potential therapeutic strategies and interventions being explored for UC.
2. Including recent statistics or projections about the prevalence of UC could enhance the urgency of understanding and addressing this condition.
3. A clearer delineation of the implications for clinical practice, particularly in managing patients with CKD and UC, would add practical value for clinicians.",Authentic
Effect of atrial fibrosis on clot burden score and physicochemical properties of thrombus in patients with ischaemic stroke occurring in non-valvular atrial fibrillation,"Background
To investigate the effect of the degree of atrial fibrosis on the clot burden score (CBS) and physicochemical properties in patients with acute ischaemic stroke (AIS) due to non-valvular atrial fibrillation (NVAF).

Methods
A total of 117 patients with AIS in NVAF attending the Department of Cardiovascular Medicine and the Cerebrovascular Diagnostic and Treatment Centre between August 2021 and May 2024 were included in the study. Baseline clinical data, biochemical indexes, and imaging data of the patients were collected, and the patients were divided into 93 cases of the CBS (score of 0–6) group and 24 cases of the CBS (score of 7–10) group according to the CBS. CBS (score of 0–6) signifies higher clot burden. The enzyme-linked immunosorbent assay was used to measure the concentration of galactaglutinin-3 (gal-3) and transforming growth factor (TGF-β1) in the serum of the patients, and the PTFV1 were collected by 12-lead electrocardiogram, and the differences in the degree of atrial fibrosis between different groups and the risk factors of CBS (score of 0–6) were analysed. To analyse the effect of atrial fibrosis on the collateral circulation of stroke, the patients were divided into 31 cases with good collateral circulation (grade 3–4) and 86 cases with poor collateral circulation (grade 0–2) according to the digital subtraction angiography (DSA) images. The cerebral thrombus was collected from 60 AIS patients who underwent mechanical thrombectomy. The content of erythrocyte, fibrin/platelets and leukocytes in the thrombus was analysed by Mathew’s scarlet blue staining, and the density of thrombus was measured by computed tomography (CT).

Results
A total of 117 patients were included in this study, and the proportion of hypertensive patients, proportion of chronic atrial fibrillation (CAF), B-type natriuretic peptide (BNP), neutrophil/lymphocyte ratio (NLR), D-dimer, uric acid concentration, proportion of patients with PTFV1 < −0.03 mm s, gal-3, and TGF-β1 were higher in the CBS (score of 0–6) group as compared to the CBS (score of 7–10) group (P-value < 0.05). Hypertension, proportion of CAF, homocysteine, NLR, D-dimer, uric acid, PTFV1 < −0.03 mm s, gal-3, and TGF-β1, were risk factors for the development of high CBS in atrial fibrillation (AF), and hypertension and CAF were the most important factors for the occurrence of AF in the independent risk factors for stroke combined with high clot burden. gal-3 and TGF-β1 were risk factors for poor collateral circulation, atrial fibrosis indexes were not associated with thrombus pathological composition and thrombus density.

Conclusions
Atrial fibrosis increases clot burden in patients with AIS due to NVAF but does not significantly correlate with the physicochemical properties and density of the thrombus.","Additional comments
In the index report, the authors highlight the impact of the degree of atrial fibrosis on the clot burden score (CBS) and physicochemical properties in patients with acute ischaemic stroke (AIS) due to non-valvular atrial fibrillation (NVAF). The manuscript is timely, clinically relevant and of use to physicians, neurologists and cardiologist. I have minor concerns which need to be addressed: -
1) In the abstract, Please mention the study was retrospective in nature, remove the name of hospital from the abstract and the main text. Expand PTFV1 in abstract. Clearly indicate for authors that CBS 0-6 signifies higher thrombus burden, otherwise one may find it confusing and contradictory. Remove the statidtcal details at most places in the abstract and keep it simple, concise and relevant. Your abstract in the current state gives it a monotonous imporession
2) Expand AIS in introduction and add that AF is a risk factor for Coronary artery disease and ischemia and also dementia besides stroke and HF. For instance, one prospective observational study reported dementia in almost 37% of all NVAF patients. include these details and also provide the relevant recent references. Dementia in turn, is partly linked to atrial fibroses as seen in certain studies
3) The retrospective nature limits the generalizability of the study findings as selction bias cannot be completely accounted for. No formal sample size calculation available for determining the power of the study to draw these conclusions
4) Remove the details of (TOAST) classification from methods
5) How was PTFV1 calculated. Provide details and diagram of ECG along with the same for the readers understanding and to allow for repeatability of your findings in future by other studies.
6) Another major limitation is that one cannot assume that GAL-3 and TGF B will always be synonymous with atrial fibrosis. These markers are non-specific and in general would indicate excess inflammation and fibrosis in the body and not specifically localizing to the atrium. The only definitive evidence would come from atrial biopsy possible at the time of some other cardiac surgery (although it is not a realistic one). Perhaps using LA strain in combination with these biomarkers would make more sense and give a definitive alternative to biopsy for quantifying atrial fibroses
7) Mention mean age in results up to 2 decimals with standard deviation
8) The first line of discussion is a repeat. Please avoid
9) Please check this statement ‘Stroke caused by AF is usually fatal or results in severe disability.’ And give valid reference to it if it is appropriate
10) No need to include details of all the biomarkers like BNP, NLR etc. just stick to the most relevant ones like GAL-3 and TGF B will and make the discussion short, concise and more relevant to your research.
11) Another inflammatory marker besides the ones discussed and researched by the authors is initial or baseline Hs-CRP which in fact of late has been shown to independent predictor of clinical outcome in NVAF. Perhaps the authors can include 1-2 recent papers highlighting the same in the references for the readers
12) I would suggest the authors to further mention in discussion (although the same has not been studied in the index paper) that atrial fibroses has a strong link with underlying ischemia in the presence or absence of epicardial stenosis and this in turn is one of the strongest predictors of clinical outcomes including CAD, MI, stroke, HF, recurrence after ablation etc. accorindgly, I will suggest addition of some of the most relevant literature surrounding this published in the last 2 years.
13) The conclusions should be specific, focused and discussing the salient findings and key messages from your study rather than previously known facts. Make it concise and straightforward",Authentic
Berberine-induced browning and energy metabolism: mechanisms and implications,"Obesity has become a global pandemic. The approaches researched to prevent it include decreasing energy intake and/or enhancing energy expenditure. Therefore, research on brown adipose tissue is of great importance. Brown adipose tissue is characterized by its high mitochondrial content. Mitochondrial uncoupling protein 1 (UCP1) releases energy as heat instead of chemical energy. Thermogenesis increases energy expenditure. Berberine, a phytochemical widely used in Asian countries, has positive effects on body weight control. While the precise mechanisms behind this effect remain unclear, the adenosine monophosphate-activated protein kinase (AMPK) pathway is known to play a crucial role. Berberine activates AMPK through phosphorylation, significantly impacting brown adipose tissue by enhancing lipolytic activity and increasing the expression of UCP1, peroxisome proliferator-activated receptor γ-co-activator-1α (PGC1α), and PR domain containing 16 (PRDM16). While investigating the mechanism of action of berberine, both the AMPK pathway is being examined in more detail and alternative pathways are being explored. One such pathway is growth differentiation factor 15 (GDF15), known for its appetite-suppressing effect. Berberine’s low stability and bioavailability, which are the main obstacles to its clinical use, have been improved through the development of nanotechnological methods. This review examines the potential mechanisms of berberine on browning and summarizes the methods developed to enhance its effect.","Additional comments
1，This study first describes the metabolism of berberine, the basic classification and functional status of white and brown adipose tissue, followed by the mechanisms of browning and brown adipose tissue activation. Finally, it discusses the effects and pathways of berberine in brown adipose tissue activation, listing two key factors AMPK and GDF15, as well as a series of packaging methods and applicability of berberine. Finally, the conclusion is drawn. The review has a certain level of organization, but lacks innovation, Lack of eye-catching viewpoints and lacks the latest research progress. The research content is relatively scattered and the themes are not focused enough.

2, The abstract first mentioned the panic of obesity, increased energy consumption, reduced appetite, and the importance of brown adipose tissue. To stimulate the positive effects of berberine by consuming energy. However, in the Introduction (Line 34) of the main text, the source, composition, and efficacy of berberine are mentioned first. Then it was mentioned that there is a potential effect of weight loss, as it promotes glucose and lipid metabolism, inhibits and regulates microorganisms. Here, it does not obediently introduce the browning of adipose tissue, but stating that berberine can induce browning and thermogenesis of adipose tissue (Line 52-53), which seems a bit abrupt. The reason of linking brown fat to the discovery of thermogenic function of berberine is rather far-fetched.

3, The title is about the mechanism and implications of berberine induced browning of adipose tissue, but the overall of berberine metabolism and packaging methods is not specifically targeted at adipose tissue or brown adipose tissue, but applied to the whole body. Therefore, it is suggested that the author revise it slightly wider, such as energy metabolism of skeletal muscle, myocardium, fat, brain, etc., and develop some targeted packaging materials to delay metabolism, which can have a more meaningful effect after absorption. The author may consider refining the title or reorganizing the logical structure of the paper writing.

4, But in terms of mechanism, which is the key content. The author listed many methods and proteins that activate brown adipose tissue. Table 2 lists the doses and studied pathways of berberine intake in cells, rats, mice, and humans. The dose range is somewhat broad and the time period is long-term. Is there a summary of the lowest effective dose or other signaling pathways besides AMPK and GDF15? What dosage works through which pathways and what regulatory mechanisms are involved.

Actually, the mechanism is not very clear, and most of the literature is only observation of phenomena, not gene knockout followed by berberine supplementation to determine the target. As referenced in Wang et al. 2021; Yang et al. (2017) found that GDF15 binds to its receptor, increases UCP1 expression levels, promotes browning, and subsequently reduces body weight.

5, Some descriptions are not precise enough and are rather vague, L343-344, how much better is the bioavailability of berberine-SLN than free berberine? Please write it down, then it will look more convincing.

6, Is the nanotechnology methods of berberine specifically designed for brown adipose tissue? Or they are fit for the whole body.

7, There was also a lot of statement about the classification of adipose tissue, the sources and activation of brown fats, but in fact, many of them are unrelated to berberine directly.

8, When it comes to the regulation of brown adipose tissue by berberine through the AMPK and GDF15 pathways, there is a lot of talk about the regulation of AMPK, but it is actually not related to berberine. At last, I recommend the authors can carefully sort out the key points and logic of this study.",Authentic
"The association between fibroblast growth factor 21 with diabetes retinopathy among type 2 diabetes mellitus patients: a systematic review, meta-analysis, and meta-regression","Background
Diabetic retinopathy (DR), a leading cause of vision loss worldwide, is a common complication of type 2 diabetes mellitus (T2DM) driven by chronic hyperglycemia and microvascular damage. Fibroblast growth factor 21 (FGF21) is crucial in blood sugar regulation and has been linked to DR incidence and severity. While some studies suggest that FGF21 levels may contribute to the DR incidence, others propose a protective role. This discrepancy necessitates further analysis, prompting this study to evaluate the association between FGF21 levels and DR incidence and severity in T2DM patients.

Methods
A systematic search was conducted through MEDLINE, Web of Science, Scopus, and Embase up to May 2024 for studies evaluating the association between FGF21 and DR incidence and severity. A random-effect model meta-analysis was performed to calculate the pooled standardized mean difference (SMD) and 95% confidence intervals (CI). A univariate meta-regression was performed to analyze factors influencing pooled size estimates. All statistical analyses were performed using STATA 17 software.

Result
This systematic review and meta-analysis of 5,852 participants revealed that FGF21 was positively correlated with DR (SMD 3.11; 95% CI [0.92–5.30], p = 0.005) and sight-threatening DR (STDR) incidence (SMD 3.61; 95% CI [0.82–6.41], p = 0.01). There was no significant difference in FGF21 levels in DR vs STDR (p = 0.79). Subgroup analysis revealed a significant difference in DR incidence between LDL groups, with higher DR incidence in the group with low-density lipoprotein (LDL) levels >100 (P < 0.00001). Meta-regression revealed no variables significantly influenced the pooled size estimates.

Conclusion
A higher level of FGF21 was associated with higher DR and STDR incidence among T2DM patients, highlighting its potential utilization as a biomarker for DR detection and enabling the exploration of FGF21-based treatment strategies. However, variables independently predicting DR among patients with elevated FGF21 levels shall be explored further.","Basic reporting
The manuscript submitted 103488v1 explores the association between fibroblast growth factor 21 (FGF-21) with diabetes retinopathy (DR) among type 2 diabetes mellitus (T2DM) patients. Authors illustrate that DR is a leading cause of vision disorders worldwide. FGF-21plays a crucial role in blood sugar regulation and have reported to be correlated with DR incidence and severity and they have concluded that higher level of FGF-21 is associated with higher DR and STDR incidence among T2DM patients. This study supports its use for further eye exams and therapies associated particularly with DR. The association between FGF-21 levels and DR will provide a way to clinicians and researchers to get insight into a novel pathway for future DR research, emphasizing its relevance as a biomarker for monitoring and predicting diabetic complications in type 2 diabetes patient. Clear and unambiguous English is used throughout the manuscript but the manuscript should be carefully revised to correct numerous grammatical issues. Sufficient data is provided to support the findings.

Experimental design
The study is well designed and the findings are interesting and have scientific values. Experimental design is impressive.

Material and Methods
1. Line 154- klotho association with diabetes and retinopathy, needs to be define in introduction.
2. Line 174- authors have not included TG levels as independent variable also dependent variables are not mentioned.
3. Line 183- I2, 2 should be written as subscript as it is creating confusion.
4. Line 193 pooled ORS, What is the significance of doing sensitivity analysis in present review)

Validity of the findings
Results

1. Line 237, what does it mean by the present study demonstrate a significant inverse association between FGF-21 levels and retinopathy diabetes incidence. It is suggested to give more explanation.

Discussion

1. Line 308-313 in discussion is in repetition to line 84 to 88 in introduction, again 313-315 is in repetition.
2. Clearly explain the role of FGF-21 in glucose metabolism and insulin resistance and how increase FGF-21 can affect glucose metabolism. The relation is not clear here, how increase hyperglycemia is related to increase FGF-21 levels and how increase FGF-21 can impair glucose metabolism. The role of FGF-21 with hyperglycemia and DR need more clear consideration.
3. Line 317-325 requires further explanation.
4. FGF-21 leads towards increase gluconeogenesis, so how can FGF-21 aim to repair microvascular damage in retinopathy.
5. More clarification regarding the cutoff values of FGF-21 is required to use it as a biomarker. Are the authors agreed with Jin et al., cutoff value 554.69 pg/ml.

Conclusion
In conclusions, authors have suggested that FGF-21 levels can be used to explore new pathway for future for DR treatment. How kindly explain.

Additional comments
The manuscript should be carefully revised to correct numerous grammatical issues. Additionally, the authors should address the specific issues listed below:
Abstract:
1. Line 25, Diabetic retinopathy (DR) is a leading cause of vision worldwide. It is a mistake, correct this sentence, word problem/ disorder is missing here.
2. Various abbreviation are used without defining them first such as T2DM, STDR and LDL. It is suggested to define them first and then use abbreviations throughout the manuscript.

Introduction:
1. Use of abbreviation is inconsistent, at some places the abbreviation of type 2 diabetes mellitus is used as T2DM and at some places it is written as Type 2 DM (Line 56). Again line 68 and 71, full form of diabetic retinopathy is used instead of abbreviation though it is already defined. Line no. 87 VEGF is not defined here. More consideration is required in this regard.
2. Line 59-61, it is suggested to either break the sentence or rephrase it for better understanding.
3. It would be worthwhile to give a brief detail regarding how oxidative stress can lead to diabetic retinopathy.
4. How proliferative and non-proliferative diabetic retinopathy are different from each other.
5. Line no. 88-90, how this is connected with T2DM, what is the purpose of adding this sentence.
6. Text is not justified throughout the manuscript.
7. It would be beneficial to explore the role of different other pathways associating T2DM and DR.",Authentic
Reliability and utility of blood glucose levels in the periodontal pockets of patients with type 2 diabetes mellitus: a cross-sectional study,"Background
Several studies have measured gingival blood glucose (GBG) levels, but few have confirmed systematic bias using Bland–Altman analysis. This study compared the effectiveness of GBG levels with that of fingertip blood glucose (FTBG) levels using Bland–Altman and receiver operating characteristic (ROC) analyses.

Methods
A total of 15 healthy volunteers and 15 patients with type 2 diabetes were selected according to inclusion and exclusion criteria. Each group comprised eight male and seven female participants. The GBG and FTBG levels were measured using a self-monitoring blood glucose device after periodontal examination. Pearson’s product‒moment correlation and simple linear regression analyses were performed. In addition, Bland‒Altman analysis was also performed to assess the degree of agreement between the two methods. ROC analysis was conducted to determine the sensitivity, specificity, and cutoff values for patients with diabetes. The area under the ROC curve (AUC) was used to identify significant differences.

Results
The mean GBG and FTBG levels were 120 ± 44.8 mg/dL and 137 ± 45.1, respectively, for the whole sample. The mean GBG and FTBG levels were 145 ± 47.2 mg/dL and 163 ± 49.1, respectively, in the diabetes group. The mean GBG and FTBG levels in the nondiabetes group were 95.3 ± 25.2 and 111 ± 18.8, respectively. Patients with diabetes were more likely to have a probing pocket depth (PPD) of ≥4 mm at the sampled site. Pearson’s product‒moment correlation and simple linear regression analyses revealed a significant correlation between the GBG and FTBG measurements. Bland–Altman analysis revealed that GBG and FTBG measurements differed significantly among all participants; however, no significant differences were observed among the patients with diabetes (mean difference (MD) ± standard deviation (SD) = −18.1 ± 34.2, 95% confidence interval (CI) [−37.0 to 0.88]) or among the participants with a PPD of ≥4 mm (MD ± SD = −15.2 ± 30.4, 95% CI [−30.8 to 0.43]). The sensitivity, specificity, and cutoff values of the GBG measurements for detecting diabetes were 80%, 93%, and 123.5 mg/dL, respectively. The sensitivity, specificity, and cutoff values of the FTBG measurements for detecting diabetes were 73%, 87%, and 134.0 mg/dL, respectively. No significant differences were observed between the AUCs (0.078, 95% CI [−0.006 to 0.161]).

Conclusions
The GBG measurements aligned with the FTBG measurements in the patients with diabetes and among the participants with a PPD of ≥4 mm. Patients with diabetes were more likely to have a PPD of ≥4 mm at the sampled site, GBG levels can be used to screen for type 2 diabetes in dental clinics.","Basic reporting
The authors have successfully addressed all the concerns raised in the initial review of their manuscript titled ""Reliability and Utility of Blood Glucose Levels in the Periodontal Pockets of Patients with Type 2 Diabetes Mellitus: A Cross-Sectional Study."" The revisions have improved the clarity, organization, and scientific stringency of the study. The methods and results are now well-presented, with appropriate approach, and the discussion effectively contextualizes the findings within the existing literature. Ethical considerations have been clearly outlined, and the language has been polished for better readability. Overall, the manuscript is now suitable for publication.

Experimental design
There are no specifical comments for this section.

Validity of the findings
I have no further questions.",Authentic
Reliability and utility of blood glucose levels in the periodontal pockets of patients with type 2 diabetes mellitus: a cross-sectional study,"Background
Several studies have measured gingival blood glucose (GBG) levels, but few have confirmed systematic bias using Bland–Altman analysis. This study compared the effectiveness of GBG levels with that of fingertip blood glucose (FTBG) levels using Bland–Altman and receiver operating characteristic (ROC) analyses.

Methods
A total of 15 healthy volunteers and 15 patients with type 2 diabetes were selected according to inclusion and exclusion criteria. Each group comprised eight male and seven female participants. The GBG and FTBG levels were measured using a self-monitoring blood glucose device after periodontal examination. Pearson’s product‒moment correlation and simple linear regression analyses were performed. In addition, Bland‒Altman analysis was also performed to assess the degree of agreement between the two methods. ROC analysis was conducted to determine the sensitivity, specificity, and cutoff values for patients with diabetes. The area under the ROC curve (AUC) was used to identify significant differences.

Results
The mean GBG and FTBG levels were 120 ± 44.8 mg/dL and 137 ± 45.1, respectively, for the whole sample. The mean GBG and FTBG levels were 145 ± 47.2 mg/dL and 163 ± 49.1, respectively, in the diabetes group. The mean GBG and FTBG levels in the nondiabetes group were 95.3 ± 25.2 and 111 ± 18.8, respectively. Patients with diabetes were more likely to have a probing pocket depth (PPD) of ≥4 mm at the sampled site. Pearson’s product‒moment correlation and simple linear regression analyses revealed a significant correlation between the GBG and FTBG measurements. Bland–Altman analysis revealed that GBG and FTBG measurements differed significantly among all participants; however, no significant differences were observed among the patients with diabetes (mean difference (MD) ± standard deviation (SD) = −18.1 ± 34.2, 95% confidence interval (CI) [−37.0 to 0.88]) or among the participants with a PPD of ≥4 mm (MD ± SD = −15.2 ± 30.4, 95% CI [−30.8 to 0.43]). The sensitivity, specificity, and cutoff values of the GBG measurements for detecting diabetes were 80%, 93%, and 123.5 mg/dL, respectively. The sensitivity, specificity, and cutoff values of the FTBG measurements for detecting diabetes were 73%, 87%, and 134.0 mg/dL, respectively. No significant differences were observed between the AUCs (0.078, 95% CI [−0.006 to 0.161]).

Conclusions
The GBG measurements aligned with the FTBG measurements in the patients with diabetes and among the participants with a PPD of ≥4 mm. Patients with diabetes were more likely to have a PPD of ≥4 mm at the sampled site, GBG levels can be used to screen for type 2 diabetes in dental clinics.","Basic reporting
Overall, the article uses clear and professional English, provides sufficient background information and literature references, has a well-organized structure, and shares raw data. However, there are a few areas that need improvement:

Language and Grammar:
Some sentences could be simplified for clarity. For example, the abstract contains some complex sentence structures that can be made more concise.
For instance, “Several studies have reported the measurement of gingival blood glucose (GBG) levels; however, few studies have confirmed the presence of systematic bias using the Bland-Altman analysis.” could be revised to: “Several studies have measured gingival blood glucose (GBG) levels, but few have confirmed systematic bias using Bland-Altman analysis.”
Literature References:
It is suggested to include more recent studies in the background section to ensure the timeliness and relevance of the literature.
For example, when discussing the measurement of GBG, adding references to recent publications would enhance the completeness of the background information.

Experimental design
The experimental design is robust, and the methods are described in detail, but the following areas could be improved:

Sample Size:
The sample size is relatively small (15 participants per group). It is recommended to discuss the impact of the sample size on the study’s findings and to mention in the discussion section whether further research with larger samples is needed to validate the results.
For example: “The sample size of 15 participants per group might limit the generalizability of the findings. Future studies with larger sample sizes are necessary to validate these results.”
Methodological Details:
Some methodological details could be further clarified. For instance, the criteria for selecting participants or the specific procedures for blood glucose measurement should be described in more detail to enhance replicability.
For example: “The inclusion and exclusion criteria for participant selection should be specified more clearly. Additionally, a step-by-step description of the blood glucose measurement procedure would help ensure the study can be accurately replicated.”

Validity of the findings
The study’s findings are robust and statistically sound, but the following points should be considered:

Statistical Analysis:
While the statistical methods used are appropriate, it would be helpful to provide more detailed explanations of the statistical tests and their results in the text.
For example: “A more detailed explanation of the Bland-Altman analysis results, including the interpretation of the bias and limits of agreement, would provide a clearer understanding of the findings.”
Conclusions and Implications:
The conclusions are well-stated and supported by the results, but discussing the broader implications of the findings and potential limitations in more detail would strengthen the article.
For example: “Discussing the potential clinical implications of using gingival blood glucose measurements in dental settings and any limitations related to the study’s scope or methodology would enhance the discussion section.”

Additional comments
The study is commendable for its thorough investigation and clear presentation. The findings are significant for clinical applications in screening type 2 diabetes in dental settings. However, minor revisions are needed to address the above points.",Authentic
"Association of thyroid hormones with the severity of chronic kidney disease: a cross-sectional observational study at Tabuk, Saudi Arabia","Background
The interplay between chronic kidney disease (CKD) and thyroid dysfunction is becoming more evident in the biomedical community. However, the intricacies of their relationship warrant deeper investigation to understand the clinical implications fully.

Objective
This study aims to systematically evaluate the correlation between thyroid hormone levels, including thyroid-stimulating hormone (TSH), triiodothyronine (T3), and thyroxine (T4), and markers of renal disease severity. These markers include serum creatinine, urea, and parathyroid hormone (PTH) levels in individuals diagnosed with CK).

Methods
We conducted a cross-sectional observational study involving a cohort of 86 participants with CKD recruited from the renal clinic at King Fahad Hospital in Tabuk. Biochemical parameters, encompassing plasma electrolytes and thyroid hormone concentrations, were quantitatively assessed. These measurements were performed with the aid of a Roche Cobas E411 analyzer. The Pearson correlation coefficient was employed to delineate the strength and direction of the associations between the thyroid function markers and renal disease indicators.

Results
The statistical analysis highlighted a generally weak correlation between the concentrations of thyroid hormones and the indicators of renal disease severity, with Pearson correlation coefficients between −0.319 and 0.815. Critically, no significant correlation was found between creatinine and thyroid hormones (TSH, T3, T4), nor was any substantial correlation between urea and thyroid hormones. Conversely, a robust positive correlation was noted between the levels of parathyroid hormone and serum creatinine (r = 0.718, p < 0.001).

Conclusion
The data suggests that thyroid hormone levels have a minimal correlation with the severity of renal disease markers. In contrast, the pronounced correlation between PTH and creatinine underscores the importance of considering PTH as a significant factor in managing and therapeutic intervention of CKD complications. These initial findings catalyze further research to thoroughly investigate the pathophysiological relationships and potential therapeutic targets concerning thyroid dysfunction in patients with renal impairment.","Basic reporting
Correlation Between Thyroid Hormone Levels and Renal Disease Severity in Chronic Kidney Disease Patients"" aims to investigate the relationship between thyroid hormone levels (TSH, T3, T4) and markers of renal disease severity (creatinine, urea, PTH) in patients with chronic kidney disease (CKD). The study employs a cross-sectional observational design involving 86 participants from King Fahad Hospital in Tabuk, Saudi Arabia. The research is structured well, with clear objectives outlined in the abstract and introduction sections. Key concepts are introduced effectively, providing a comprehensive background to contextualize the study's significance.

Experimental design
The study utilizes a prospective cross-sectional observational framework, which is appropriate for exploring associations between variables but limits the ability to establish causality. The cohort selection from a renal clinic and adherence to ethical guidelines are strengths of the study design. The methodology details the use of biochemical assays conducted on Roche Cobas E411 analyzers for thyroid hormones and renal markers, ensuring standardized measurements. However, the study lacks details on potential confounders or variables controlled for in the analysis, which could affect the interpretation of results.

Validity of the findings
The findings suggest a weak correlation between thyroid hormone levels and renal disease severity markers. Specifically, no significant correlations were found between creatinine/urea and thyroid hormones, contrasting with a strong positive correlation between PTH and creatinine. The use of Pearson correlation coefficients to quantify associations is appropriate, although the significance and clinical relevance of some correlations, such as T3 with creatinine, are questionable due to weak coefficients and lack of statistical significance. The strong correlation between PTH and creatinine highlights the study's contribution to understanding secondary hyperparathyroidism in CKD.

Additional comments
A few of the suggestions are listed.
Specify briefly why understanding the relationship between CKD and thyroid dysfunction is important from a clinical perspective. For instance, mention the prevalence or impact of thyroid dysfunction in CKD patients.
Consider rephrasing to explicitly state the primary aim of the study. For example, ""This study aims to assess the correlation between thyroid hormone levels (TSH, T3, T4) and markers of renal disease severity in CKD patients.""
Include a brief sentence explaining why these specific biochemical parameters and this cohort size were chosen. This adds context to the study design.
Instead of listing Pearson correlation coefficients, summarize the key findings in relation to the study objective. For instance, ""Thyroid hormone levels showed weak correlations with markers of renal disease severity, while a significant positive correlation was found between PTH and serum creatinine.
Emphasize the implications of the findings for clinical practice and future research. For example, ""While thyroid hormone levels appear to have limited impact on renal disease markers in CKD, the strong correlation between PTH and creatinine suggests potential implications for therapeutic strategies.""
The study provides valuable insights into the complex interplay between thyroid function and renal disease markers in CKD patients. While the findings contribute to the existing literature, several methodological improvements and considerations for future research could strengthen the validity and applicability of the results",Authentic
Diabetes self-care and its associated factors among type 2 diabetes mellitus with chronic kidney disease patients in the East Coast of Peninsular Malaysia,"Introduction
Diabetes self-care among diabetic patients is crucial as it determines how patients care for their illness in their daily routine for better diabetes control. This study aims to calculate the average score for diabetes self-care among patients with type 2 diabetes mellitus and chronic kidney disease and to identify factors that are associated with this score.

Materials and Methods
This cross-sectional study enrols patients over 18 years old with type 2 diabetes mellitus and chronic renal disease with an eGFR of less than 60 mL/min/1.73 m2 in a tertiary hospital in Malaysia. The Malay version of the Summary of Diabetic Self-Care Activities (SDSCA) was used to assess diabetes self-care, the Malay version of the diabetes-related distress questionnaire (DDS-17) was used to assess diabetes distress, and the Malay version of the Patient Health Questionnaire-9 (PHQ-9) was used to assess depression. Data analysis was performed using both simple and multiple linear regression models to determine the associations between variables.

Result
One hundred and seventy-six eligible patients were recruited for this study. The mean score for diabetes self-care is 3.62. The eGFR (p = 0.002) and diabetes distress (p = 0.004) are the significant associated factors for diabetes self-care among type 2 diabetes mellitus patients with chronic kidney disease.

Conclusion
The mean score for diabetes self-care indicated a moderate level of self-care. The eGFR level and diabetes distress were important factors influencing diabetes self-care practices.","Basic reporting
This manuscript has been written well, and there are no significant issues with English or grammar.
The conclusion (in the abstract) can be reframed and contextualized.
All abbreviations, such as DKD and ESKD, must be spelled out when used for the first time on a page.
The study objectives can be made explicit with clear hypotheses presented at the beginning. Subsequently, a reference to statistically and non-statistically significant relationships can be made later.
The manuscript needs to be carefully evaluated for missing citations. For example – “Numerous research studies and recommendations in the field of diabetes support these methods. Numerous diabetes self-care initiatives have been documented in the literature but with varying degrees of success.” Reference to those studies must be provided.
I have included some other suggestions which can improve the manuscript.

Experimental design
The study could be informative and valuable if it includes the literature gap, indicating its rationale. However, the study has included sufficient literature to demonstrate the relationships between different variables to support the logical implications for the study’s overall goal. The overall goal should be rewritten. It says, “This study aims to determine the mean diabetes self-care score and its associated factors among type 2 diabetes mellitus patients with chronic kidney disease.” What does “the mean diabetes self-care score” indicate? It can be reframed. Also, what are those associated factors?

The result section should include more than just information about the tables. It should be a narrative description of the results with reference to hypotheses and appropriate tables for further information. Authors should make reference to their research questions/hypotheses.
Information in all tables must be interpreted, and key information should be presented in a descriptive manner for readers, such as what are the key characteristics of the research subjects and what the mean score of self-care actually represents. It should be more than just “moderate self-care.”
Authors claimed in line 310 “ There was no previous study done to determine a direct association between diabetes self-care and depression.” Please check the following study found through Google Scholar and they have used the same self-care scale. Authors need to do a literature review to contextualize the result. There should also be a rationale for difference or convergence with previous findings.

Tohid H, Papo M, Ahmad S, Sumeh AS, Jamil TR, Hamzah Z. Self-care activities among patients with Type 2 Diabetes Mellitus in Penampang, Sabah and its association with depression, anxiety and stress. Malaysian Journal of Public Health Medicine. 2019 Jan 1;19(1):117-25.
Additionally, in the same paragraph, there could be some citation issues.

Validity of the findings
The validity of the result must be reevaluated in light of a new literature review. How the findings contribute back to the literature must be presented succinctly.",Authentic
"Knowledge, attitude and purchasing behavior of Saudi mothers towards food additives and dietary pattern of preschool children","Background
There are over 506 children’s products containing one or more types of additives. Maternal awareness of these additives is essential for the health of preschool-aged children, as this period is vital for children’s growth and development. This study aims to assess the knowledge, attitudes, and purchasing behaviors related to food additives among mothers living in the western region of Saudi Arabia, as well as the dietary patterns of preschool children.

Method
A cross-sectional study was conducted using an online survey with a convenience sample of 521 mothers of preschool-aged children (3–5 years old). The survey gathered data on the child’s age, number of children, the youngest child’s weight and height, food intolerance, tooth decay, as well as the dietary patterns of preschool children. It also assessed the mother’s knowledge, attitude, and purchasing behaviors related to food additives.

Results
The study found that 46.6% of mothers demonstrated good knowledge of food additives, while 56.0% demonstrated fair attitudes and 78.5% good purchasing behavior regarding additives. Additionally, the majority of mothers reported favorable dietary patterns for their preschool-aged children. “Biscuits and crackers” had the highest consumption frequency (4.98 ± 1.50), with 36.7% of children consuming them once daily, while “Soft beverages” had the lowest consumption frequency (2.73 ± 2.04), with 46.6% of children never consuming them. Statistically significant differences were identified between mothers’ knowledge and their age, education level, occupation status, and economic status (p < 0.05). ANOVA results also indicated a statistically significant difference between mothers’ attitudes and occupation status (p < 0.05). Furthermore, there were significant positive correlations between mothers’ knowledge of food additives and their attitudes (r = 0.293) and purchasing behaviors (r = 0.284) related to additives.

Conclusion
The findings suggest that mothers possess a relatively good level of knowledge of food additives and hold fair attitudes toward them, tending to result in healthier purchasing behaviors and dietary practices for their preschool-aged children. To increase awareness, nutrition intervention programs are required across various socio-economic groups of mothers in the western region of Saudi Arabia. These programs can significantly contribute to promoting healthier dietary practices for preschool-aged children and improving overall family health and well-being.","Basic reporting
I have reviewed the manuscript titled "" Knowledge, attitude and purchasing behavior of
Saudi mothers towards food additives and dietary pattern of preschool children."" The study was interesting; however, certain areas require improvement, my detailed comments and suggestions are provided below.
For authors I should recommend that’s, please enhance the introduction by providing more recent references to underline the significance of the study.
Clearly state the research gap that this study aims to fill.
For review litrature purpose, expand on the adverse effects of specific food additives with more detailed examples from recent studies.
You mentioned that the study aims to evaluate the knowledge, attitude, and purchasing behavior toward food additives among mothers in the western region of Saudi Arabia. Why was this specific region chosen for the study? Are there particular characteristics or trends in this region that justify its selection?
The introduction lists several food additives and mentions over 10,000 substances classified as food additives. Does your study focus on specific food additives or categories of additives? If so, which ones and why were they chosen?
The introduction suggests that educating mothers about healthy food choices can positively influence their children's diets. Can you elaborate on the specific educational interventions or strategies that have been shown to be effective in previous studies? How does your study aim to contribute to this body of knowledge?
Is there evidence to suggest that regulatory awareness impacts purchasing decisions?
Ensure all references are formatted according to PeerJ guidelines.
Add more recent references to support the introduction and discussion sections.

Experimental design
The study uses a cross-sectional design with an online survey, which is appropriate for the research question. However, I have some concerns as detailed below,
You utilized a nonprobability convenience sample of 385 mothers. Could you provide more detailed justification for the sample size and explain why a convenience sample was chosen over other sampling methods? How might this sampling method impact the generalizability of your findings?
Mention any potential biases introduced by this sampling method and how they were addressed?
The Food Frequency Questionnaire (FFQ) was adopted from a previous study. Could you provide more details on the process of adapting the FFQ to your study? Were there any modifications made to tailor it to the specific context of your research?

Validity of the findings
You excluded 51 responses from participants living outside the western region of Saudi Arabia. Can you provide more details on how the exclusion criteria were determined? Were there any notable differences in the excluded responses that could impact the study's findings?
While 52.1% of participants acquired information about food additives from social media/the internet, can you assess the accuracy and reliability of these sources? How do you think the source of information might affect the participants' knowledge and perceptions of food additives?
The results indicate that a significant percentage of children were underweight or obese, and many suffered from food allergies or dental caries. Can you discuss how these health indicators were measured and their potential implications for the study's findings on dietary patterns?
You found statistically significant differences in mothers' knowledge of food additives based on age, education level, occupation status, and economic status. Can you provide more insight into why these particular demographic factors influence knowledge levels? Are there any interventions or educational programs that could target these specific groups to improve knowledge?
There were 15 missing data points for weight and 94 for height. How did you handle these missing data in your analysis? Did you employ imputation methods, or were these cases excluded from specific analyses?
The results show that employed mothers had more positive attitudes towards food additives. Can you explore possible reasons for this finding? How might employment status influence mothers' attitudes and purchasing behaviour towards food products containing additives?

Comparative analysis with previous studies in the discussion section
The discussion mentions that higher socioeconomic status correlates with better knowledge and attitudes towards food additives. Could you provide more details on how socioeconomic factors specifically contribute to these differences? For example, does income level affect access to healthier food options or educational materials?
The discussion mentions that preschool children consume certain unhealthy food products daily. What specific interventions or policy changes would you recommend to improve dietary patterns among this age group? Could educational programs targeting both mothers and children be effective?
You acknowledge several limitations, including the observational nature of the study and response bias. Can you propose specific methodologies for future research that might address these limitations? For example, would longitudinal studies or randomized controlled trials provide more robust data?

Additional comments
In your ethical considerations section, you mention obtaining ethical approval and informed consent. Can you include the specific protocol reference number for the ethical approval? Additionally, were there any specific ethical challenges you faced during the study, and how were they addressed?",Authentic
DMSA-Net: a deformable multiscale adaptive classroom behavior recognition network,"In the intelligent transformation of education, accurate recognition of students’ classroom behavior has become one of the key technologies for enhancing the quality of instruction and the efficacy of learning. However, in the recognition of target behavior in real classroom scenarios, due to the use of wide-angle or panoramic images for image acquisition, students in the back row are far away from monitoring devices, and their subtle body movements such as the small opening and closing of the mouth (to determine whether they are speaking), fine finger operations (to distinguish between reading books or operating mobile phones) are difficult to recognize. Moreover, there are occlusions and scale differences in the front and back rankings, which can easily cause confusion and interference with target features in the detection process, greatly limiting the accurate recognition ability of existing visual algorithms for classroom behavior. This article proposes a deformable multiscale adaptive classroom behavior recognition network. To improve the network’s capacity to model minute behavioral phenomena, the backbone section introduces a deformable self-attention dattention module, dynamically modifying the receptive field’s geometry to enhance the model’s concentration on the region of interest. To improve the network’s capacity for feature extraction and integration of behavior occlusion and classroom behavior at different scales, a proposal has been put forward the Multiscale Attention Feature Pyramid Structure (MSAFPS), to achieve multi-level feature aggregation after multiscale feature fusion, reducing the impact of mutual occlusion and scale differences in classroom behavior between front and back rows. In the detect section, we adopt the Wise Intersection Over Union (Wise-IoU) loss as our loss criterion, augmenting the evaluation framework with richer contextual cues to broaden its scope and elevate the network’s detection prowess. Extensive experimentation reveals that our proposed method outperforms rival algorithms on two widely adopted benchmark datasets: SCB-Dataset3-S (the Student Classroom Behavior Dataset–https://github.com/Whiffe/SCB-dataset) and we created object detection dataset DataMountainSCB (https://github.com/Chunyu-Dong/DataFountainSCB1) containing six types of behaviors.","Basic reporting
The paper is well-structured and easy to read. The narrative progresses logically: the introduction outlines the problem, the second section reviews related work on the topic, and subsequent sections describe the methodology and present the results.

Minor comments:
1. When numbering equations in LaTeX, it is recommended to use the ""{equation}"" environment to ensure consistent formatting. In the current document, equations are manually numbered with a space following the formula.
2. Figures 1, 2, 3, 4, 5, 6, 9, and 10 are presented in relatively low resolution. Pixelation is visible, and in some cases (e.g., Figures 9 and 10), it is difficult to discern the objects detected by the model.
3. Figure 8 appears to be a screenshot, as indicated by the presence of a gray element in the lower-left corner.
4. It is unclear why the abbreviation for ""Multiscale Attention Feature Pyramid Structure"" is ""NSAFPS"" rather than ""MSAFPS."" The reasoning behind the use of ""N"" instead of ""M"" should be clarified.
5. Line 32 contains a possible typographical error: ""prowess"" should likely be ""process.""
6. Many instances of incorrect capitalization appear throughout the text. For example:
- Articles are capitalized unnecessarily, such as on line 23, several times between lines 262 and 263, and in other parts of the document.
- ""We"" is incorrectly capitalized on line 30.
7. Conversely, some terms and sentences should be capitalized but are not:
- Line 58: ""RoI"" should be ""ROI.""
- Figure captions (e.g., for Figures 6, 9, 10, and 11) begin with lowercase letters but should start with uppercase.
- Line 268: The sentence starts with a lowercase letter, or there is a period is used in place of a comma.
Similar issues are present throughout the document and require thorough proofreading.
8. Most references to figures in the text appear after the corresponding figure. It would be more logical to reference the figure before presenting it.
9. Figures 3 and 6 are included in the article, but there are no references to them in the text.
10. On line 62, multiple citations are listed separately. It would be better to combine them using a single command, e.g., ""\cite{ref-book8, ref-book9, ref-book10, ref-book11, ref-book12, ref-book13, ref-book14}"" instead of writing each citation individually.
11. There are several instances where no space follows a citation or a parenthetical explanation, such as in ""NSAFPS(Multiscale Attention Feature Pyramid Structure)"" on line 28 or ""conduct\cite{ref-book1}"" on line 39. In contrast, there are cases where unnecessary spaces precede punctuation marks, such as on line 39 and between lines 262 and 263. To address these inconsistencies, it requires thorough proofreading.
12. The term ""Multiscale"" is inconsistently written as ""Multi-scale"" in some places. Consistent spelling should be used throughout the document.
13. For numbered lists, it is recommended to use the ""enumerate"" environment in LaTeX for clarity and formatting consistency (e.g., lines 108, 113, and 118).
14. Line 170 contains a typographical error: ""BoumdingBox"" should be ""BoundingBox."" or even with space between bounding and box.
15. Line 248: ""BackBone"" should be ""Backbone.""
16. Abbreviations should be defined upon their first occurrence. For example, ""EMA"" is first expanded on line 306, but it appears earlier on line 293 without explanation. Similarly, the origin of the ""E"" in ""Multiscale Attention"" (abbreviated as ""MA"") is unclear. This issue is repeated for other abbreviations and should be addressed.
17. In Figure 6, uppercase letters are used for labels, but the same labels appear in lowercase in the description. Furthermore, the labels on the figure and in the image description differ significantly and should be aligned.
18. Formulas 6–8 appear to require reordering for clarity. For example, Formula 6 introduces $L_{WIOUv1}$, which has not been previously defined, and combines it with a formula calculating $\gamma$. Probably these two formulas should be separated in two lines. Formula 7 uses $R_{WIOU}$, which is described in Formula 8. This arrangement forces the reader to search backward to understand the notation.
19. Lines 512–513: Are you certain that the reference is to Figure 8? It is better to use the ""ref{}"" command in the LaTeX document for referencing figures and tables instead of manual references.
20. No links to the table 7 in the text.

Experimental design
After a review of the article, the following questions remain:
1. What is the execution time of the models? Is the proposed method intended for real-time analysis of student behavior during lectures?
2. Student behavior can change throughout a lecture. However, the experiments were conducted on static images. How is the proposed algorithm intended to work when applied to lecture video recordings? Specifically, how should the model's output be interpreted if, at one moment, a student is attentively listening, then becomes distracted by their phone, and later resumes paying attention?
3. How will the proposed approach perform in scenarios involving varying lighting conditions in the classroom and differing input photo/video quality?

Validity of the findings
1. Comparing the results of the proposed algorithm, authors conducted experiments with other well-known model. However, results obtained by other researchers on the SCB-Dataset3-S dataset were not included. It would be valuable to see a comparison with the results of other researchers. These results can be added to the table 7.
2. Figure 11 presents the model's output. Why were many faces not detected?
3. Will the code be available on GitHub or another platform for researchers to reproduce the results presented in the article?

Additional comments
A significant contribution of this work is the publication of a new dataset containing images of students in a classroom exhibiting various behavior patterns: hand-raising, reading, writing, using a phone, bowing their head, etc.",Authentic
UMEDNet: a multimodal approach for emotion detection in the Urdu language,"Emotion detection is a critical component of interaction between human and computer systems, more especially affective computing, and health screening. Integrating video, speech, and text information provides better coverage of the basic and derived affective states with improved estimation of verbal and non-verbal behavior. However, there is a lack of systematic preferences and models for the detection of emotions in low-resource languages such as Urdu. To this effect, we propose Urdu Multimodal Emotion Detection Network (UMEDNet), a new emotion detection model for Urdu that works with video, speech, and text inputs for a better understanding of emotion. To support our proposed UMEDNet, we created the Urdu Multimodal Emotion Detection (UMED) corpus, which is a seventeen-hour annotated corpus of five basic emotions. To the best of our knowledge, the current study provides the first corpus for detecting emotion in the context of multimodal emotion detection for the Urdu language and is extensible for extended research. UMEDNet leverages state-of-the-art techniques for feature extraction across modalities; for extracting facial features from video, both Multi-task Cascaded Convolutional Networks (MTCNN) and FaceNet were used with fine-tuned Wav2Vec2 for speech features and XLM-Roberta for text. These features are then projected into common latent spaces to enable the effective fusion of multimodal data and to enhance the accuracy of emotion prediction. The model demonstrates strong performance, achieving an overall accuracy of 85.27%, while precision, recall, and F1 scores, are all approximately equivalent. In the end, we analyzed the impact of UMEDNet and found that our model integrates data on different modalities and leads to better performance.","Basic reporting
This paper presents UMEDNet, a multi-modal emotion prediction model for Urdu, and the UMED Corpus, a key resource for emotion classification in low-resource languages. The model effectively classifies emotions using video, speech, and text data.

1. In the ""Related Works"" section, it would be beneficial to provide a more comprehensive discussion of the challenges faced by low-resource languages in both emotion prediction and multi-modal models. Specifically, the paper could elaborate on the limitations posed by the scarcity of publicly available datasets and pre-trained models, which are major obstacles in these fields. It would also be valuable to explore existing solutions, such as cross-lingual transfer learning, and offer a comparison between these approaches and multi-modal studies. Additionally, including relevant literature on similar studies conducted in languages such as Tamil or Haitian Creole would further strengthen the context and breadth of the research.
2. In the ""Corpus Collection"" section, further elaboration on the data quality and diversity would enhance the comprehensiveness of the discussion. Specifically, it would be valuable to address the balance between formal and informal language (or written vs. spoken language) in the collected data. Additionally, the inclusion of demographic factors, such as age, gender, and other relevant variables, would provide a more nuanced understanding of the corpus' representativeness. Furthermore, a discussion on the overall size of the dataset would be beneficial, as this impacts the generalizability and robustness of the findings.
3. In the ""Data Annotation"" section, it would be useful to outline the measures taken to ensure the quality and reliability of the annotated data. Specifically, clarifying the procedures followed to guarantee that the annotated video meets high standards, often referred to as ""gold"" annotations, would strengthen the validity of the dataset. This could include details on inter-annotator agreements, quality control processes, and any validation methods employed.
",Authentic
TASCI: transformers for aspect-based sentiment analysis with contextual intent integration,"In this article, we present a novel Transformer-Based Aspect-Level Sentiment Classification with Intent (TASCI) model, designed to enhance sentiment analysis by integrating aspect-level sentiment classification with intent analysis. Traditional sentiment analysis methods often overlook the nuanced relationship between the intent behind a statement and the sentiment expressed toward specific aspects of an entity. TASCI addresses this gap by first extracting aspects using a self-attention mechanism and then employing a Transformer-based model to infer the speaker’s intent from preceding sentences. This dual approach allows TASCI to contextualize sentiment analysis, providing a more accurate reflection of user opinions. We validate TASCI’s performance on three benchmark datasets: Restaurant, Laptop, and Twitter, achieving state-of-the-art results with an accuracy of 89.10% and a macro-F1 score of 83.38% on the Restaurant dataset, 84.81% accuracy and 78.63% macro-F1 score on the Laptop dataset, and 79.08% accuracy and 77.27% macro-F1 score on the Twitter dataset. These results demonstrate that incorporating intent analysis significantly enhances the model’s ability to capture complex sentiment expressions across different domains, thereby setting a new standard for aspect-level sentiment classification.","Basic reporting
The manuscript is written in clear, professional English, with precise terminology aligning with the fields of sentiment analysis and machine learning. The introduction effectively provides context for sentiment analysis, aspect-based sentiment analysis (ABSA), and intent-based sentiment analysis (IBSA), though it could benefit from explicitly linking the identified research gap to the objectives of the proposed TASCI model. The figures and tables are relevant, well-labeled, and contribute significantly to the understanding of the model's performance, though figure resolution should be checked for publication standards. The manuscript adheres to a logical structure, flowing seamlessly from the introduction to the methodology, results, and conclusions. The raw data and supplementary materials, including datasets and source code, are made available, meeting data transparency requirements, though ensuring all supplementary details, such as hyperparameters and configurations, are included will enhance reproducibility. The results are self-contained and tied to the hypotheses, with metrics like Accuracy, Precision, Recall, and F1-Score clearly presented; however, definitions of these metrics should be added for broader accessibility. The ablation study is thorough, validating the contributions of various model components. Additionally, the manuscript includes sufficient explanations of technical concepts, such as self-attention and GRU mechanisms, making it approachable for readers across different expertise levels. Expanding the conclusion to outline future research directions, such as the broader applicability of TASCI in other NLP tasks, would further strengthen the manuscript. Overall, the study adheres to PeerJ standards and effectively communicates its findings.

Experimental design
The paper makes a significant contribution to the field of context-aware sentiment analysis. The research question is clearly defined and aims to fill the knowledge gap in the literature. The TASCI model provides an innovative approach that includes contextual intent integration, which is lacking in traditional sentiment analysis methods. The results obtained on the restaurant, laptop and Twitter datasets demonstrate the accuracy and effectiveness of the proposed model. The originality of the research and the proposed solution are of significant value in the fields of NLP and machine learning.

In the method section, the components of the model are explained in detail and clearly, which increases the reproducibility. The integration of modern techniques such as the self-attention mechanism, GRU-based intent analysis and transformer-based sentiment classification supports the robustness and validity of the method. The benchmark datasets and performance metrics used increase the reliability of the analysis. However, a clearer specification of ethical standards regarding the use of data could strengthen the integrity of the study.

The results clearly demonstrate the superiority of the proposed model. The accuracy and Macro-F1 scores of TASCI show that it outperforms other existing models. However, more analytical and visual details of the results can deepen the reader's understanding. The strengths of the model include filling a gap in the literature, a detailed explanation of the method, and its success on different datasets. However, discussion of the limitations of the model and more detailed consideration of some of the shortcomings can broaden the scope of the study.
Overall, the article provides an innovative and effective solution to an important problem in the field of NLP and, with the proposed TASCI model, goes beyond existing methods in the literature. However, ethical standards, visual support and more detailed discussion of limitations would further strengthen the study.

Validity of the findings
The impact and novelty of the research have not been explicitly assessed within the manuscript. While the proposed TASCI model presents a clear contribution to the field by integrating intent analysis with aspect-based sentiment analysis, further elaboration on how this approach significantly advances the state-of-the-art or addresses practical challenges in real-world applications could enhance the impact. Meaningful replication of the study is encouraged, as the rationale for the model and its integration with existing methodologies is well-stated. The benefit of TASCI to the literature is evident, particularly in its ability to outperform prior models on benchmark datasets; however, its broader implications could be articulated more thoroughly.

All underlying data have been provided and are robust, statistically sound, and well-controlled. The use of benchmark datasets (Restaurant, Laptop, and Twitter) ensures that the results are comparable and relevant to the field of natural language processing. Additionally, the inclusion of detailed performance metrics (Accuracy and Macro-F1) for each dataset enhances the reliability of the findings.

The conclusions are well-stated and appropriately linked to the original research question. They are limited to supporting results, effectively summarizing the contributions and the superiority of TASCI in aspect-based sentiment classification. The conclusions align with the evidence presented and do not overreach, ensuring a balanced and evidence-based interpretation of the findings.",Authentic
Eternal-MAML: a meta-learning framework for cross-domain defect recognition,"Defect recognition tasks for industrial product suffer from a serious lack of samples, greatly limiting the generalizability of deep learning models. Addressing the imbalance of defective samples often involves leveraging pre-trained models for transfer learning. However, when these models, pre-trained on natural image datasets, are transferred to pixel-level defect recognition tasks, they frequently suffer from overfitting due to data scarcity. Furthermore, significant variations in the morphology, texture, and underlying causes of defects across different industrial products often lead to a degradation in performance, or even complete failure, when directly transferring a defect classification model trained on one type of product to another. The Model-Agnostic Meta-Learning (MAML) framework can learn a general representation of defects from multiple industrial defect recognition tasks and build a foundational model. Despite lacking sufficient training data, the MAML framework can still achieve effective knowledge transfer among cross-domain tasks. We noticed there exists serious label arrangement issues in MAML because of the random selection of recognition tasks, which seriously affects the performance of MAML model during both training and testing phase. This article proposes a novel MAML framework, termed as Eternal-MAML, which guides the update of the classifier module by learning a meta-vector that shares commonality across batch tasks in the inner loop, and addresses the overfitting phenomenon caused by label arrangement issues in testing phase for vanilla MAML. Additionally, the feature extractor in this framework combines the advantages of the Squeeze-and-Excitation module and Residual block to enhance training stability and improve the generalization accuracy of model transfer with the learned initialization parameters. In the simulation experiments, several datasets are applied to verified the cross-domain meta-learning performance of the proposed Eternal-MAML framework. The experimental results show that the proposed framework outperforms the state-of-the-art baselines in terms of average normalized accuracy. Finally, the ablation studies are conducted to examine how the primary components of the framework affect its overall performance. Code is available at https://github.com/zhg-SZPT/Eternal-MAML.","Basic reporting
This paper tackles the challenge of limited training samples in industrial defect detection by proposing Eternal-MAML, an enhanced MAML framework. It addresses label arrangement issues, integrates Squeeze-and-Excitation and Residual blocks for stability, and shows an 8% accuracy improvement over state-of-the-art methods on the MVTec dataset. Ablation studies further validate the framework's components. This work offers a promising solution for improving meta-learning models in industrial applications.

Experimental design
The experimental design is rational, covering the main needs of current industrial inspection while also demonstrating the superiority of the method through recognized evaluation criteria. The problem setup in the experiments is generally reasonable, reflecting the innovation and advantage of the method in this aspect. However, the description of the experimental methods and parameter settings should be more detailed, thoroughly describing the specific setup and corresponding results during the experimental process. Moreover, the lack of data description makes it difficult to ensure the reproducibility of the results. It is suggested to make the souce code public available on github.

Validity of the findings
The conclusions drawn in this paper effectively address the proposed issues and present viable solutions. Based on the experiments and their results, the method proposed in this paper exhibits good performance. However, the stability and generalizability of the method cannot be fully ascertained from the presentation in the paper. It is recommended to provide a more detailed description of the experimental data and an explanation of the corresponding experimental settings to enhance the persuasiveness and validity of the article.

Additional comments
The folloowing concerns should be addressed.
1.Clarification on the mechanism of vector W.
The manuscript mentions that the vector W improves the performance, but does not provide a clear explanation of how W guides the updates. I suggest adding a detailed description of the mechanism of W and its role in the model’s performance improvement. Providing a theoretical foundation or referencing similar studies would strengthen this section.
2.Unclear explanation in Lines 213-216?
The statement in lines 213-216 seems confusing and potentially misleading. It is unclear whether the issue arises from labeling of dataset, the MAML method, or a combination of both. I recommend revising this part to clarify the source of the problem. It would be helpful to provide more details and distinguish between these potential causes, supported by evidence from the text or relevant references.
3.Figure 1 does not explicitly show all modules.
The current version of Figure 1 does not clearly illustrate the SE module and residual connections. I suggest modifying the figure to explicitly depict these components or adding annotations that help the viewer intuitively understand the whole structure. This would enhance the clarity of the presentation.
4.Confusing definitions in Lines 268-274.
The definitions of “transfer learning” and “learning from scratch” presented in lines 268-274 are not entirely clear. Specifically, it is unusual to define transfer learning (TF) as training with a larger train set, and learning from scratch (LFS) as training with a smaller test set. Additionally, the explanation that LFS is initialized using pre-trained weights from ImageNet does not align with the traditional method of training from scratch. I suggest revising these definitions to align with standard usage in the literature and providing clearer justification for the choices made in the study.",Authentic
RMIS-Net: a fast medical image segmentation network based on multilayer perceptron,"Medical image segmentation, a pivotal component in diagnostic workflows and therapeutic decision-making, plays a critical role in clinical applications ranging from pathological diagnosis to surgical navigation and treatment evaluation. To address the persistent challenges of computational complexity and efficiency limitations in existing methods, we propose RMIS-Net—an innovative lightweight segmentation network with three core components: a convolutional layer for preliminary feature extraction, a shift-based fully connected layer for parameter-efficient spatial modeling, and a tokenized multilayer perceptron for global context capture. This architecture achieves significant parameter reduction while enhancing local feature representation through optimized shift operations. The network incorporates layer normalization and dropout regularization to ensure training stability, complemented by Gaussian error linear unit (GELU) activation functions for improved non-linear modeling. To further refine segmentation precision, we integrate residual connections for gradient flow optimization, a Dice loss function for class imbalance mitigation, and bilinear interpolation for accurate mask reconstruction. Comprehensive evaluations on two benchmark datasets (2018 Data Science Bowl for cellular structure segmentation and ISIC-2018 for lesion boundary delineation) demonstrate RMIS-Net’s superior performance, achieving state-of-the-art metrics including an average F1-score of 0.91 and mean intersection-over-union of 0.82. Remarkably, the proposed architecture requires only 0.03 s per image inference while achieving 27× parameter compression, 10× acceleration in inference speed, and 53× reduction in computational complexity compared to conventional approaches, establishing new benchmarks for efficient yet accurate medical image analysis.","Basic reporting
- The article's English is quite weak.
- The purpose of the article is unclear, additions should be made to the introduction section accordingly.
- The contributions of the article are listed. However, what purpose do these contributions provide? The contributions section should be detailed instead of giving one sentence each.
- The organization of the paper should be given at the end of the introduction section.
- ""Dice"" or ""DICE"", a single usage form should be preferred.
- What does RMIS stand for, its expansion should be given where it is first used.
-Explanations should be written in Figure 6. Text should be added to understand what each module is expressed in the figure.

Experimental design
- How were the C1, C2,..., C5 channel number values ​​determined in the proposed method.
- Before moving on to the subheading in section 4, information about the experimental conditions should be given. Under which conditions and with which parameters was the experiment performed?
- Why were horizontal flipping, rotation, and cutting applied as data augmentation methods? Why were different methods not preferred?

Validity of the findings
- What is the data partitioning ratio for training, testing, and validation according to the values ​​in Table 1?
- More detailed information should be provided about the metrics used.
- Can the channel number values ​​(Table 3) be varied?
- The conclusion section should be expanded.

Additional comments
The article titled ""RMIS-Net: A Fast Medical Image Segmentation Network Based on Multilayer Perceptron"" should be revised according to the following points.",Authentic
SODU2-NET: a novel deep learning-based approach for salient object detection utilizing U-NET,"Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. To address this challenge posed by complex backgrounds in salient object detection is crucial for advancing the field. This article proposes a novel deep learning-based architecture called SODU2-NET (Salient object detection U2-Net) for salient object detection that utilizes the U-NET base structure. This model addresses a gap in previous work that focused primarily on complex backgrounds by employing a densely supervised encoder-decoder network. The proposed SODU2-NET employs sophisticated background subtraction techniques and utilizes advanced deep learning architectures that can discern relevant foreground information when dealing with complex backgrounds. Firstly, an enriched encoder block with full feature fusion (FFF) with atrous spatial pyramid pooling (ASPP) varying dilation rates to efficiently capture multi-scale contextual information, improving salient object detection in complex backgrounds and reducing the loss of information during down-sampling. Secondly the block includes an attention module that refines the decoder, is constructed to enhances the detection of salient objects in complex backgrounds by selectively focusing attention on relevant features. This allows the model to reconstruct detailed and contextually relevant information, which is essential to determining salient objects accurately. Finally, the architecture has been improved by adding a residual block at the encoder end, which is responsible for both saliency prediction and map refinement. The proposed network is designed to learn the transformation between input images and ground truth, enabling accurate segmentation of salient object regions with clear borders and accurate prediction of fine structures. SODU2-NET is demonstrated to have superior performance in five public datasets, including DUTS, SOD, DUT OMRON, HKU-IS, PASCAL-S, and a new real world dataset, the Changsha dataset. Based on a comparative assessment of the model FCN, Squeeze-net, Deep Lab, Mask R-CNN the proposed SODU2-NET is found and achieve an improvement of precision (6%), recall (5%) and accuracy (3%). Overall, approach shows promise for improving the accuracy and efficiency of salient object detection in a variety of settings.","Basic reporting
This work proposes an DL-based model SODU2-NET for automated saliency detection using 92 residual blocks, and Atrous Spatial Pyramid Pooling (ASPP). It seems that the main contribution of this work lies in modified U-NET that adds the residual blocks inside the encoder architecture. Here, authors use multiple metrics to identify and compare the issue of SOD in old 107 models. However, the concept of the U-NET used for salient object detection in this paper have been used in many existing works, and the incorporation of these well-developed methods into the proposed work cannot be contribution to salient object detection. In addition, some suggestions for authors are as follows:
1. The introduction and related works must provide a critical evaluation of the models used for SOD in previous studies. The main contributions of this article and the differences with existing methods should be made clear in the Introduction and Related Work sections.
2. Authors are advised to present a critical discussion, not just a descriptive summary of the topic in literature review and other subsections.
3. Some of the figures are used from other literatures, which violating copy right rules. It would be better to re-create the figures and cite them properly.
4. Furthermore, the references should be arranged in the order of their appearance.

The contribution of the paper is not adequate. The whole paper needs significant improvement. The content of the paper is ambiguous. Introduction is not well written, and authors are unable to justify their contributions. Moreover, the results of the proposed work are not clear.

Experimental design
Experimental work is not enough to show advantages of the proposed work over existing techniques. It would better to discuss technical aspects of the proposed work. For example, What are the reasons that proposed work is giving better mean average precision, f1-score etc.?

Validity of the findings
the contribution of the paper is not adequate and it is not properly verified.",Authentic
"Fast2Vec, a modified model of FastText that enhances semantic analysis in topic evolution","Background
Topic modeling approaches, such as latent Dirichlet allocation (LDA) and its successor, the dynamic topic model (DTM), are widely used to identify specific topics by extracting words with similar frequencies from documents. However, these topics often require manual interpretation, which poses challenges in constructing semantics topic evolution, mainly when topics contain negations, synonyms, or rare terms. Neural network-based word embeddings, such as Word2vec and FastText, have advanced semantic understanding but have their limitations. Word2Vec struggles with out-of-vocabulary (OOV) words, and FastText generates suboptimal embeddings for infrequent terms.

Methods
This study introduces Fast2Vec, a novel model that integrates the semantic capabilities of Word2Vec with the subword analysis strength of FastText to enhance semantic analysis in topic modeling. The model was evaluated using research abstracts from the Science and Technology Index (SINTA) journal database and validated using twelve public word similarity benchmarks, covering diverse semantic and syntactic dimensions. Evaluation metrics include Spearman and Pearson correlation coefficients to assess the alignment with human judgments.

Results
Experimental findings demonstrated that Fast2Vec outperforms or closely matches Word2Vec and FastText across most benchmark datasets, particularly in task requiring fine-grained semantic similarity. In OOV scenarios, Fast2Vec improved semantic similarity by 39.64% compared to Word2Vec, and 6.18% compared to FastText. Even in scenarios without OOV terms, Fast2Vec achieved a 7.82% improvement over FastText and a marginal 0.087% improvement over Word2Vec. Additionally, the model effectively categorized topics into four distinct evolution patterns (diffusion, shifting, moderate fluctuations, and stability), enabling a deeper understanding of evolution topic interests and their dynamic characteristics.

Conclusion
Fast2Vec presents a robust and generalizable word embedding framework for semantic-based topic modeling. By combining the contextual sensitivity of Word2Vec with the subword flexibility of FastText, Fast2Vec effectively addresses prior limitations in handling OOV terms and semantic variation and demonstrates strong potential for boarder applications in natural language processing tasks.","Basic reporting
The document is written in professional English and provides a clear introduction and background. However, there are instances where sentence structure is overly complex, which may affect readability. Simplify some of the sentences to enhance clarity for a broader audience. For example, the sentence in the introduction, ""Without proper analysis, scientific data will remain meaningless raw information,"" could be rephrased as, ""Proper analysis is essential to transform raw scientific data into meaningful insights.""

Experimental design
The experimental design is described in detail, and the methods are replicable. However, there is limited explanation of how the dataset was cleaned and pre-processed before training models.
Improvement: Include a more detailed explanation of the data pre-processing steps, such as handling missing values, removing duplicates, or normalizing text, to provide better transparency and replicability.",Generic
Enhancing phishing detection with dynamic optimization and character-level deep learning in cloud environments,"As cloud computing becomes increasingly prevalent, the detection and prevention of phishing URL attacks are essential, particularly in the Internet of Vehicles (IoV) environment, to maintain service reliability. In such a scenario, an attacker could send misleading phishing links, potentially compromising the system’s functionality or, at worst, leading to a complete shutdown. To address these emerging threats, this study introduces a novel Dynamic Arithmetic Optimization Algorithm with Deep Learning-Driven Phishing URL Classification (DAOA-DLPC) model for cloud-enabled IoV infrastructure. The candidate’s research utilizes character-level embeddings instead of word embeddings, as the former can capture intricate URL patterns more effectively. These embeddings are integrated with a deep learning model, the Multi-Head Attention and Bidirectional Gated Recurrent Units (MHA-BiGRU). To improve precision, hyperparameter tuning has been done using DAOA. The proposed method offers a feasible solution for identifying the phishing URLs, and the method achieves computational efficiency through the attention mechanism and dynamic hyperparameter optimization. The need for this work comes from the observation that the traditional machine learning approaches are not effective in dynamic environments like phishing threat landscapes in a dynamic environment such as the one of phishing threats. The presented DLPC approach is capable of learning new forms of phishing attacks in real time and reduce false positives. The experimental results show that the proposed DAOA-DLPC model outperforms the other models with an accuracy of 98.85%, recall of 98.49%, and F1-score of 98.38% and can effectively detect safe and phishing URLs in dynamic environments. These results imply that the proposed model is useful in distinguishing between safe and unsafe URLs than the conventional models.","Basic reporting
1.The abstract should be improved. Your point is your own work that should be further highlighted.
2.Introduction seems to be incomplete. Please carefully check and supplement it.
3. More statistical methods are recommended to analyze the experimental results.
4. The article can be further enhanced by connecting the undergoing work with some existing literatures.
5. The numerical simulation verification is not convincing, and the actual engineering application example verification should be given.
6. There are a few typos and grammar errors in the manuscript.

Experimental design
As above

Validity of the findings
As above

Additional comments
As above",Generic
Enhancing healthcare data privacy and interoperability with federated learning,"This article explores the application of federated learning (FL) with the Fast Healthcare Interoperability Resources (FHIR) protocol to address the underutilization of the huge volumes of healthcare data generated by the digital health revolution, especially those from wearable sensors, due to privacy concerns and interoperability challenges. Despite advances in electronic medical records, mobile health applications, and wearable sensors, current digital health cannot fully exploit these data due to the lack of data analysis and exchange between heterogeneous systems. To address this gap, we present a novel converged platform combining FL and FHIR, which enables collaborative model training that preserves the privacy of wearable sensor data while promoting data standardization and interoperability. Unlike traditional centralized learning (CL) solutions that require data centralization, our platform uses local model learning, which naturally improves data privacy. Our empirical evaluation demonstrates that federated learning models perform as well as, or even numerically better than, centralized learning models in terms of classification accuracy, while also performing equally well in regression, as indicated by metrics such as accuracy, area under the curve (AUC), recall, and precision, among others, for classification, and mean absolute error (MAE), mean squared error (MSE), and root mean square error (RMSE) for regression. In addition, we developed an intuitive AutoML-powered web application that is FL and CL compatible to illustrate the feasibility of our platform for predictive modeling of physical activity and energy expenditure, while complying with FHIR data reporting standards. These results highlight the immense potential of our FHIR-integrated federated learning platform as a practical framework for future interoperable and privacy-preserving digital health ecosystems to optimize the use of connected health data.","Basic reporting
The source document is a paper undergoing peer review that discusses enhancing healthcare data privacy and interoperability with federated learning. The document includes an AI detection score of 29%, which suggests areas needing revision to ensure the content is original and adheres to academic standards.

Experimental design
The paper acknowledges that real data might be non-Independent and Identically Distributed (non-IID), which can significantly affect the results of CL and FL. To address this, the authors used the Synthetic Minority Over-sampling Technique (SMOTE) to synthesize data for classification tasks and Adaptive Synthetic Sampling (ADASYN) for regression problems. While using SMOTE and ADASYN is a good start, the authors should provide a more in-depth discussion on how these techniques mitigate the challenges posed by non-IID data in their specific context. They could also explore and compare other data synthesis techniques or partitioning strategies that are more robust to non-IID data.
The paper uses standard evaluation metrics such as Accuracy, F1-score, Kappa, and MCC for classification, and RMSE, MAE, R-squared, and MAPE for regression. The choice of evaluation metrics is appropriate, but the authors should justify their selection more explicitly. Additionally, they could consider including other metrics that are relevant to the specific healthcare applications they are addressing.
While comparing FL and CL is valuable, the authors could enhance their analysis by including additional baseline models or comparing against other federated learning algorithms. This would provide a more comprehensive understanding of the strengths and weaknesses of their proposed approach.
The experiments were simulated locally using only one dataset, and the data was divided into pairs of 60%/40%, 70%/30%, 80%/20%, and 90%/10% for training and testing.The authors should address the limitations of simulating the experiments locally. Running experiments on a distributed system with multiple devices would provide a more realistic evaluation of the performance of FL. They should also justify their choice of partition proportions and the number of clients used in the FL experiments.",Authentic
Employing SAE-GRU deep learning for scalable botnet detection in smart city infrastructure,"The proliferation of Internet of Things (IoT) devices in smart cities has revolutionized urban infrastructure while escalating the risk of botnet attacks that threaten essential services and public safety. This research addresses the critical need for intrusion detection and mitigation systems by introducing a novel hybrid deep learning model, Stacked Autoencoder–Gated Recurrent Unit (SAE-GRU), specifically designed for IoT networks in smart cities. The study targets the dual challenges of processing high-dimensional data and recognizing temporal patterns to identify and mitigate botnet activities in real time. The methodology integrates Stacked Autoencoders for reducing dimensionality and gated recurrent units for analyzing sequential data to ensure both accuracy and efficiency. An emulated smart city environment with diverse IoT devices and communication protocols provided a realistic testbed for evaluating the model. Results demonstrate significant improvements in detection performance with an average accuracy of 98.65 percent and consistently high precision and recall values. These findings enhance the understanding of IoT security by offering a scalable and resource-efficient solution for botnet detection. The functional investigation establishes a foundation for future research into adaptive security mechanisms that address emerging threats and highlights the practical potential of advanced deep learning techniques in safeguarding next-generation smart city ecosystems.","Basic reporting
The authors have improved clarity by shortening complex sentences and reducing overly technical language​
A ""Significant Contributions"" section was added to highlight key contributions​
The structure conforms to PeerJ standards, and references are well-cited.
Figures are appropriately labeled, but Figure 4 could still benefit from a more descriptive caption.

Remaining Suggestions:
Ensure all figures have fully descriptive captions to improve readability.

Experimental design
The research question is well-defined, and the study now explicitly states how it addresses IoT security challenges​
The authors have clarified dataset choices (IoT-23 and MedBIoT) but should further elaborate on how these datasets represent real-world IoT botnet activity.
Methods are now described in more detail, including model pruning, weight quantization, and k-fold cross-validation

Validity of the findings
The authors have added details on cross-validation and performance metrics, strengthening the study’s statistical rigor​
.
The conclusions are now more closely linked to the research questions.
Feature importance techniques like SHAP values have been added to validate detection accuracy

Additional comments
The revised manuscript is significantly improved, with a clearer focus and stronger justifications for methodology choices",Authentic
Comparative evaluation of approaches & tools for effective security testing of Web applications,"It is generally accepted that adopting both static application security testing (SAST) and dynamic application security testing (DAST) approaches is vital for thorough and effective security testing. However, this suggestion has not been comprehensively evaluated, especially with regard to the individual risk categories mentioned in Open Web Application Security Project (OWASP) Top 10:2021 and common weakness enumeration (CWE) Top 25:2023 lists. Also, it is rare to find any evidence-based recommendations for effective tools for detecting vulnerabilities from a specific risk category or severity level. These shortcomings increase both the time and cost of systematic security testing when its need is heightened by increasingly frequent and preventable incidents. This study aims to fill these gaps by empirically testing seventy-five real-world Web applications using four SAST and five DAST tools. Only popular, free, and open-source tools were selected and each Web application was scanned using these nine tools. From the report generated by these tools, we considered two parameters to measure effectiveness: count and severity of the vulnerability found. We also mapped the vulnerabilities to OWASP Top 10:2021 and CWE Top 25:2023 lists. Our results show that using only DAST tools is the preferred option for four OWASP Top 10:2021 risk categories while using only SAST tools is preferred for only three risk categories. Either approach is effective for two of the OWASP Top 10:2021 risk categories. For CWE Top 25:2023 list, all three approaches were equally effective and found vulnerabilities belonging to three risk categories each. We also found that none of the tools were able to detect any vulnerability in one OWASP Top 10:2021 risk category and in eight CWE Top 25:2023 categories. This highlights a critical limitation of popular tools. The most effective DAST tool was OWASP Zed Attack Proxy (ZAP), especially for detecting vulnerabilities in broken access control, insecure design, and security misconfiguration risk categories. Yasca was the best-performing SAST tool, and outperformed all other tools at finding high-severity vulnerabilities. For medium-severity and low-severity levels, the DAST tools Iron Web application Advanced Security testing Platform (WASP) and Vega performed better than all the other tools. These findings reveal key insights, such as, the superiority of DAST tools for detecting certain types of vulnerabilities and the indispensability of SAST tools for detecting high-severity issues (due to detailed static code analysis). This study also addresses significant limitations in previous research by testing multiple real-world Web applications across diverse domains (technology, health, and education), enhancing generalization of the findings. Unlike studies that rely primarily on proprietary tools, our use of open-source SAST and DAST tools ensures better reproducibility and accessibility for organizations with limited budget.","Basic reporting
The study mainly determines the effectiveness of SAST and DAST tools in finding OWASP Top
10:2021 and CWE Top 25:2023 vulnerabilities in web apps. For this purpose, 4 SAST and 5
DAST tools were selected to test 75 real-world web applications from technology, health and
education domains. The limitations of previous studies are highlighted in “Related Work” but it
is not mentioned that whether these limitations (some or all) are resolved in current study or not.

Experimental design
This work is specifically testing web applications but the RQs are just about the effectiveness of
DAST and SAST tools in finding OWASP Top 10 and CWE Top 25 vulnerabilities. Authors did
not mention about the web applications in any of the 4 RQs.
Need to add some details about “Report” column in Table 4. Which report it is representing?
Except 2, all SAST and DAST tools were deployed on Windows 10. Why these 2 tools were
deployed on Kali Linux. Any specific reason for using Linux for these tools?

Validity of the findings
In Results section, while answering “Research Question 1” authors have written “six out of nine
risk categories” while in “Research Question 2” it is “six out of ten”. Which one is correct?
In Table 8, the limitations of this work are not added.",Authentic
Enhancing cybersecurity through autonomous knowledge graph construction by integrating heterogeneous data sources,"Cybersecurity plays a critical role in today’s modern human society, and leveraging knowledge graphs can enhance cybersecurity and privacy in the cyberspace. By harnessing the heterogeneous and vast amount of information on potential attacks, organizations can improve their ability to proactively detect and mitigate any threat or damage to their online valuable resources. Integrating critical cyberattack information into a knowledge graph offers a significant boost to cybersecurity, safeguarding cyberspace from malicious activities. This information can be obtained from structured and unstructured data, with a particular focus on extracting valuable insights from unstructured text through natural language processing (NLP). By storing a wide range of cyber threat information in a semantic triples form which machines can interpret autonomously, cybersecurity experts gain improved visibility and are better equipped to identify and address cyber threats. However, constructing an efficient knowledge graph poses challenges. In our research, we construct a cybersecurity knowledge graph (CKG) autonomously using heterogeneous data sources. We further enhance the CKG by applying logical rules and employing graph analytic algorithms. To evaluate the effectiveness of our proposed CKG, we formulate a set of queries as questions to validate the logical rules. Ultimately, the CKG empowers experts to efficiently analyze data and gain comprehensive understanding of cyberattacks, thereby help minimize potential attack vectors.","Basic reporting
- The English language should be improved so that an international audience can clearly understand your text. The current phrasing makes comprehension difficult. Some examples are
- l35: it's -> it has
- l38: Where small
- l51: there are different sources -> different source
- l90: ""lack of language understanding"" -> vague information
- l99: different types of formats -> different formats
- l104: Cite LPG
- l131: KG ""are"" highly useful
- l146: which types .... be tackled
- l160: did not have consistency or accuracy -> vague (clarify and cite)
- l171-174: many previous works mentioned but not cited
- l186: cite previous works
- l188: cite previous works
- l203: unclear whether it is 95% or 80% -> rephrase
- l221: high: vague (quantify)
- l234: Reasoning Function: vague (clarify)
- l469: named??
- l535: John managed to open -> John managed to open the door
- l682,687: Use the same representation of spaCy or Spacy
- l689: Proper names or proper nouns
- l713: CWE documents mentioned twice
- l763: somewhat long sentences -> vague (rephrase or give statistics)
- l817: manually examined -> not clear what this means
- l838: is it two or three, the table shows three measures
- l892 these lists cannot be directed answered -> clarify which lists
- l894: run the questions -> answer instead of run?
- l910: Json-based model -> model or dataset
- l912: edges not existed -> do not exist

- The Introduction and background are well written and show enough relevant articles which build up the context for the research work
- The structure conforms to the standards
- Figures are relevant and helped me understand the texts better. However, in some figures, it is hard to see the contents of the nodes of the graph (for eg, in Fig 25, relations in Fig 26,27, and Nodes in Fig 28). A table containing all node names should be provided and then referenced to the image

Experimental design
- The design of experiments and research questions are relevant and well-defined.
- Multiple experiments and investigations performed which validate the claims
- Most of the methods are clear. I have some clarifying questions which should be addressed in the revision to make the approach clear.
- A table for the different types of data should be provided for readers to have a clear understanding of the heterogeneous data sources. I suggest having a table with fields like source (dataset name), nature (structured or unstructured or both), volume of data points, fields (authors used or present in the data if structured))
- l545 -> What patterns created? Please clarify in detail
- For sections f (l550), g(l560), h(l574), i(l585), and j(l598) use examples from your data to illustrate instead or using general natural language examples so that the readers have clear understanding on the data used.
- It is not clear to me how the disambiguation is done since the entities usually extracted by pre-trained models are different that specific cyber-security entities.
- I suggest giving a table for evaluation (Line 871) on how many questions asked and how many answered to qualify the results. Use accuracy or other metrics to measure

Validity of the findings
- The overall approach is novel and has a potential impact, especially the KG extension part
- The created CKG will be helpful for future cyber-security
- The conclusion is well stated supporting the results and linked to the original research questions",Authentic
The pivotal role of software defined networks to safeguard against cyber attacks: a comprehensive review,"Software defined networks (SDNs) offer novel approaches to managing networks by separating the control plane from the data plane to enable programmable control over network resources effectively and dynamically. This framework supports monitoring of traffic flow and detection of threats while also enabling easy adaptation of network configurations, which is critical in safeguarding against cyber threats. However, this separation also brings forth security risks that cyber attackers may exploit. In this examination, the basic concepts of SDN are explained, pointing out their benefits compared to conventional networks and exploring the security issues that are part of SDN architectures. Different types of threats that focus on SDN layers are categorized and how they impact network security while suggesting different ways to address them. Furthermore, the review highlights issues and suggests potential research paths to enhance SDN security measures and ensure their effectiveness against ever-changing cyber dangers.","Basic reporting
The manuscript is a well-structured and informative which address one of the most critical issues in modern cybersecurity. Clearly, the authors made good use of conciseness, making the material accessible to a wider audience. The introduction provides a solid foundation, clearly describing the problem and the proposed solution. The literature review is comprehensive, focuses on relevant research and demonstrates an in-depth understanding of the topic. The structure of the manuscript is consistent with academic standards, allowing the ideas to flow logically. The topic of network security, especially in software-defined networks, is of great interest to researchers, practitioners, and policymakers. This manuscript contributes to the ongoing discourse by providing new perspectives and valuable insights for further research.

Experimental design
Regarding the study design, the content of the manuscript is constant with the aim and scope of the journal. The authors achieved rigorous investigation of studies while adhering to excessive technical and ethical requirements. The literature review is comprehensive, properly-cited, and logically prepared into coherent paragraphs and subsections. The manuscript contributes to the sphere by means of inspecting a well-timed and applicable trouble, presenting treasured insight into the capability of software program-defined networks to enhance cybersecurity.

Validity of the findings
Once the findings are well explained, related to the motivations of the research, and based on supporting results, the paper would benefit from a thorough analysis of the impact and relevance of the findings If the contribution of the paper to the existing literature will be explicitly mentioned, which will enhance its relevance. In addition, identifying unresolved issues or future research directions will provide valuable directions for future studies in this area.

Additional comments
Here are some typos to improve manuscript quality:
• The introduction section is written in one long paragraph, it is better to divide it into two paragraphs and add a brief paragraph at the beginning of the introduction that gives the readers an idea about the importance of SDN in the field of networking. Also, I think there is no need to make a citation for the purpose of the paper in the fourth line of the introduction.
• It is preferable to change the title of Table3 to ""Existing work in SDN Cybersecurity field ""instead of ""Existing work in this field"".
• Although the language is clear, the grammar should be reviewed and verified.",Generic
Graph neural networks embedded with domain knowledge for cyber threat intelligence entity and relationship mining,"The escalating frequency and severity of cyber-attacks have presented formidable challenges to the safeguarding of cyberspace. Named Entity Recognition (NER) technology is utilized for the rapid identification of threat entities and their relationships within cyber threat intelligence, enabling security researchers to be promptly informed of the occurrence of cyber threats, thereby enhancing the efficiency of security defense and analysis. However, current models for identifying network threat entities and extracting relationships suffer from limitations such as the inadequate representation of textual semantic information, insufficient granularity in threat entity recognition, and errors in relationship extraction propagation. To address these issues, this article proposes a novel model for Network Threat Entity Recognition and Relationship Extraction (CtiErRe). Additionally, it redefines seven network threat entities and two types of relationships between threat entities. Specifically, first, domain knowledge is collected to build a domain knowledge graph, which is then embedded using graph convolutional networks (GCN) to enhance the feature representation of threat intelligence text. Next, the features from domain knowledge graph embedding and those generated by the bidirectional encoder representations from transformers (BERT) model are fused using the Layernorm algorithm. Finally, the fused features are processed using the GlobalPointer algorithm to generate both the threat entity type matrix and the threat entity relation type matrix, thereby enabling the identification of threat entities and their relationships. To validate our proposed model, we conducted extensive experiments, and the results demonstrate its superiority over existing models. Our model performs remarkably in threat entity recognition tasks, with accuracy and F1 scores reaching 92.13% and 93.11%, respectively. In the relationship extraction task, our model achieves accuracy and F1 scores of 91.45% and 92.45%, respectively.","Basic reporting
The English used in this paper is both professional and fluent, and extensive research has been conducted on related work. The structure of the article is well-organized, and the experimental results are detailed and consistent with the hypotheses. However, the first challenge it faces is questionable, as there are some flaws in the chart drawing and some formulas in the formal expression are not standardized enough.

Experimental design
The methods detailed in this paper provide sufficient information to replicate the experiments. However, the research questions are not appropriate. Regarding challenge (1), the author argues that current Named Entity Recognition (NER) methods overlook contextual information, whereas BERT, a well-established approach, takes such information into account and is considered a classic method for NER. For challenge (3), the Globalpointer algorithm is a one-stage method that can address these issues.

Validity of the findings
The experimental method applied in this paper is appropriate, the data presented is detailed, and it effectively addresses the problems raised by the author.

Additional comments
The issues that need to be addressed include:
（1）The proposed approach in the paper appears to be a combination of existing methods. For challenges 1 and 3, there are already established methods to address them, thus making the novelty ambiguous.
（2）In the abstract, the authors mention “Finally, the Globalpointer algorithm is employed to recognize relationships between threat entities and entities.” It’s unclear whether the relationship is identified only between threat and benign entities, or between all entities.
（3）In Table 4, the bold formatting should indicate the highest value for the same metric across different models for a specific category of entities. However, the bold formatting only highlights the highest value in this column, which includes multiple metrics and may make the different metrics non-comparable.
（4）There are some typos and unclear points. For instance, in Equation (8), should “f_MIP” be ”f_MLP”? In line 334 of Algorithm 1, is it correct that X_{new_b}<=（X_b-μ）/sqr(σ)? In line 359 of Algorithm2, what’s the meaning of axis = -2? In the 357 of Algorithm2, “ operatio”->”operation”? Additionally, there misses an equation operation in Equation (13).
（5）Some redundant abbreviations, such as Cyber Threat Intelligence (CTI) in the Introduction section.
（6）Some figures are too large and should be appropriately adjusted, such as Fig. 2, Fig. 10-12.",Authentic
Detecting malicious code variants using convolutional neural network (CNN) with transfer learning,"Malware presents a significant threat to computer networks and devices that lack robust defense mechanisms, despite the widespread use of anti-malware solutions. The rapid growth of the Internet has led to an increase in malicious code attacks, making them one of the most critical challenges in network security. Accurate identification and classification of malware variants are crucial for preventing data theft, security breaches, and other cyber risks. However, existing malware detection methods are often inefficient or inaccurate. Prior research has explored converting malicious code into grayscale images, but these approaches are often computationally intensive, especially in binary form. To address these challenges, we propose the Malware Variants Detection System (MVDS), a novel technique that transforms malicious code into color images, enhancing malware detection capabilities compared to traditional methods. Our approach leverages the richer information in color images to achieve higher classification accuracy than grayscale-based methods. We further improve the detection process by employing transfer learning to automatically identify and classify malware images based on their distinctive features. Empirical results demonstrate that MVDS achieves 97.98% accuracy with high detection speed, highlighting its potential for practical implementation in strengthening network security.","Basic reporting
It is well-written, with a clear and professional tone. However, some minor grammatical inconsistencies should be reviewed for refinement.

The authors provide an adequate background on malicious code detection and the role of CNN with transfer learning. However, additional references to recent works (e.g., Li et al., 2023; Khan et al., 2024) on adversarial robustness in malware detection could strengthen the discussion.

It mentions datasets used.

Experimental design
-It contributes to CNN-based malware detection using transfer learning, aligning with the journal’s scope.
- Clearly stated. However, the hypothesis testing would be strengthened by comparing against non-deep-learning approaches (e.g., traditional feature-based classifiers).

Validity of the findings
The presented accuracy and loss values are reasonable, but statistical validation (e.g., confidence intervals, significance tests) should be included.

Additional comments
The paper provides a meaningful contribution to the field of malware detection using deep learning. Addressing the recommended improvements will strengthen its impact and reproducibility.",Generic
Federated learning with LSTM for intrusion detection in IoT-based wireless sensor networks: a multi-dataset analysis,"Intrusion detection in Internet of Things (IoT)-based wireless sensor networks (WSNs) is essential due to their widespread use and inherent vulnerability to security breaches. Traditional centralized intrusion detection systems (IDS) face significant challenges in data privacy, computational efficiency, and scalability, particularly in resource-constrained IoT environments. This study aims to create and assess a federated learning (FL) framework that integrates with long short-term memory (LSTM) networks for efficient intrusion detection in IoT-based WSNs. We design the framework to enhance detection accuracy, minimize false positive rates (FPR), and ensure data privacy, while maintaining system scalability. Using an FL approach, multiple IoT nodes collaboratively train a global LSTM model without exchanging raw data, thereby addressing privacy concerns and improving detection capabilities. The proposed model was tested on three widely used datasets: WSN-DS, CIC-IDS-2017, and UNSW-NB15. The evaluation metrics for its performance included accuracy, F1 score, FPR, and root mean square error (RMSE). We evaluated the performance of the FL-based LSTM model against traditional centralized models, finding significant improvements in intrusion detection. The FL-based LSTM model achieved higher accuracy and a lower FPR across all datasets than centralized models. It effectively managed sequential data in WSNs, ensuring data privacy while maintaining competitive performance, particularly in complex attack scenarios. FL and LSTM networks work well together to make a strong way to find intrusions in IoT-based WSNs, which improves both privacy and detection. This study underscores the potential of FL-based systems to address key challenges in IoT security, including data privacy, scalability, and performance, making the proposed framework suitable for real-world IoT applications.","Basic reporting
The writing is clear and professional, effectively addressing all previous concerns. The background and context in IoT wireless sensor networks and intrusion detection systems are well-established. The article is well-structured, with sufficient literature references, and the datasets and preprocessing steps ensure reproducibility.

Experimental design
This paper presents original research that fits within the journal's Aims and Scope. The research question is clearly defined and relevant, addressing a significant knowledge gap in IoT security. The methods are described in sufficient detail for replication.

Validity of the findings
The impact and novelty of the findings have been adequately assessed in this revision. The authors have included baseline models for comparison and showed the effectiveness of the proposed model. The conclusions drawn are well stated and directly linked to the original research question, remaining within the bounds of the supporting results. Overall, the findings are robust and statistically sound.

Additional comments
Overall, the paper is well-written and presents a novel approach to intrusion detection. The authors have successfully addressed all previous comments, and I have no further concerns.",Generic
Efficient unified architecture for post-quantum cryptography: combining Dilithium and Kyber,"As the ongoing standardization process of post-quantum schemes yields initial outcomes, it becomes increasingly important to not only optimize standalone implementations but also explore the potential of combining multiple schemes into a single, unified architecture. In this article, we investigate the combination of two National Institute of Standards and Technology (NIST)-selected schemes: the Dilithium digital signature scheme and the Kyber key encapsulation mechanism. We propose a novel set of optimization techniques for a unified hardware implementation of these leading post-quantum schemes, achieving a balanced approach between area efficiency and high performance. Our design demonstrates superior resource efficiency and performance compared to previously reported unified architecture (DOI 10.1109/TCSI.2022.3219555), also achieving results that are better than, or comparable, to those of standalone implementations. The efficient and combined implementation of lattice-based digital signatures and key establishment methods can be deployed for establishing secure sessions in high-speed communication networks at servers and gateways. Moreover, the unique and compact design that requires small hardware resources can be directly used in small and cost-effective field programmable gate array (FPGA) platforms that can be used as security co-processors for embedded devices and in the Internet of Things.","Basic reporting
The author primarily researched the integrated hardware architecture of the post-quantum cryptographic digital signature scheme Dilithium and the KEM scheme Kyber. In the paper, the author proposed optimizations in modular multipliers, data flow, and scheduling. Finally, the design's implementation results and comparisons on FPGA were presented.

The article is written in fluent and clear language, making it easy to understand. The research terminology is used accurately, and the study includes a thorough literature review of the research background. The detailed description of the specific components of both algorithms is commendable.

Experimental design
The author claims to have proposed various optimization techniques and specific implementation methods to support the hardware design. However, I believe there are still several issues that need to be clarified:

1. Dual-port RAM is a common design in Kyber and Dilithium hardware implementations. The memory bit-width and data computation width should align accordingly. The author should specify the parallelism of the polynomial computation units corresponding to the bandwidth and how it meets the design requirements.

2. The butterfly unit is the fundamental computation structure in NTT. The structure claimed by the author employs extensive pipelining but shows no significant difference from conventional butterfly units in its specific implementation. The author should clarify how this structure ensures compatibility with both Kyber and Dilithium simultaneously.

3. The polynomial sampling unit is equally important in the overall hardware implementation. However, the author only briefly introduces the basic sampling process without presenting the related hardware implementation results, such as the hardware designs of Keccak and SHA-3, as well as the different sampling structures for Kyber and Dilithium.

4. The overlapping of multi-sampling and NTT computation is a commonly used technique in Kyber and Dilithium hardware designs. The author should elaborate on the advantages of the proposed design compared to prior designs in the literature.

Validity of the findings
In the implementation results and comparisons, the author used two different FPGAs for diverse data comparisons and drew some conclusions. However, I still have questions that need clarification:

1. In Figure 5, the author indicates that areas in different colors represent different components, and the compress component occupies a significant portion of the area. However, the compression process in Kyber only constitutes a small part, with low hardware resource requirements. The author should provide a reasonable explanation for this discrepancy.

2. The author's design demonstrates certain advantages in frequency, showing improved frequency performance compared to other designs. The author should specify which key optimizations and techniques contributed to this improvement in frequency performance.",Authentic
Modeling and implementation of a real-time digital twin for the Stewart platform with real-time trajectory computation,"The concept of a digital twin is increasingly acknowledged as an innovative and promising tool with significant potential in various end-use applications. At the heart of digital twin technology is the acquisition of real-time data from physical entities. However, the occurrence of disturbances necessitates the incorporation of resilience features within the digital twin architecture. The primary objective of this article is to develop resilient digital twins specifically for the Stewart platform. This work focuses on constructing the virtual component of the digital twin using MATLAB/Simulink and subsequently integrating this virtual model with its physical counterpart to establish a comprehensive digital twin system. Unlike other models, this system includes a motion trajectory computation module. This module is designed to receive signals from physical entities and convert them into motion trajectory data for input into the model, thereby aiming to accurately reflect the state of the physical entities under disruptive conditions. This functionality significantly enhances the reliability of the system beyond that of traditional digital twin systems. Furthermore, the article explores novel strategies and a framework for enhancing the resilience of the Stewart platform to disturbances.","Basic reporting
Professional English Usage:
The manuscript maintains a professional tone and is generally well-written. However, a few minor grammatical errors and awkward phrasings remain. For example, in the introduction, the phrase ""the trustworthiness of a digital twin is a critical factor influencing its effectiveness"" could be restructured for better clarity.

Literature References and Background:
The authors have done an excellent job providing relevant literature and contextual background for their research. The inclusion of new citations and restructuring of explanations for key terms such as ""Jacobian matrix"" and ""inverse kinematics"" improves accessibility for a broader audience.

Figures and Data Presentation:
The authors have addressed my comments regarding figure descriptions. The clarification of color coding in Figure 5 and the addition of axis units in Figures 16-18 enhance clarity. The additional explanation of Figure 22 outlining the three-layer resilience architecture strengthens the manuscript's technical depth.

Self-Containment and Hypothesis Alignment:
The manuscript is self-contained and aligns well with the stated research objectives. However, the conclusions could still benefit from a more explicit discussion on how the study’s findings align with the hypothesis.

Experimental design
Originality and Scope:
The study aligns well with the journal's aims and scope, offering an innovative approach to integrating resilience into digital twins for Stewart platforms.

Research Question and Knowledge Gap:
The research question is well-defined, and the manuscript effectively highlights how it contributes to filling a knowledge gap. The discussion on resilience in digital twins and the integration of adaptive feedback mechanisms strengthens the manuscript’s relevance.

Methodological Rigor and Replicability:
The revised manuscript provides additional details on how data flows between the physical and virtual components. The clarification regarding network communication protocols (TCP/IP) and the use of structured databases for storing and refining model parameters improves transparency and replicability. The explanation of MQTT implementation for real-time communication is a valuable addition.

Validity of the findings
Data and Statistical Soundness:
The authors have responded to concerns regarding validation but should further strengthen their presentation of results demonstrating resilience. The paper primarily focuses on building a digital twin but does not provide extensive quantitative results showing how resilience is improved.

Conclusions and Justification:
While the discussion on resilience mechanisms has been expanded, additional experimental results showcasing the effectiveness of these mechanisms would further solidify the claims. The proposed future integration of neural networks for enhanced resilience is promising, but more evidence from simulations or tests would strengthen the conclusions.

Additional comments
The explanation of how disruptions were introduced (e.g., deactivating Stewart platform actuators) is much clearer now. However, numerical results demonstrating system recovery times or performance improvements would be beneficial.

The discussion section includes future work on integrating AI techniques, such as graph convolutional neural networks, to enhance resilience. While valuable, a brief feasibility assessment or preliminary results would provide additional support for this proposed direction.",Authentic
Intersection collision prediction and prevention based on vehicle-to-vehicle (V2V) and cloud computing communication,"In modern transportation systems, the management of traffic safety has become increasingly critical as both the number and complexity of vehicles continue to rise. These systems frequently encounter multiple challenges. Consequently, the effective assessment and management of collision risks in various scenarios within transportation systems are paramount to ensuring traffic safety and enhancing road utilization efficiency. In this paper, we tackle the issue of intelligent traffic collision prediction and propose a vehicle collision risk prediction model based on vehicle-to-vehicle (V2V) communication and the graph attention network (GAT). Initially, the framework gathers vehicle trajectory, speed, acceleration, and relative position information via V2V communication technology to construct a graph representation of the traffic environment. Subsequently, the GAT model extracts interaction features between vehicles and optimizes the vehicle driving strategy through deep reinforcement learning (DRL), thereby augmenting the model’s decision-making capabilities. Experimental results demonstrate that the framework achieves over 80% collision recognition accuracy concerning true warning rate on both public and real-world datasets. The metrics for false detection are thoroughly analyzed, revealing the efficacy and robustness of the proposed framework. This method introduces a novel technological approach to collision prediction in intelligent transportation systems and holds significant implications for enhancing traffic safety and decision-making efficiency.","Basic reporting
The manuscript proposes a deep learning-based model for a specific prediction task. It discusses the model architecture, data preprocessing steps, and experimental results. The author claims significant improvements over existing methods and thoroughly analyses the model's performance. However, some aspects could be strengthened to enhance the work's clarity and robustness, particularly in areas related to model architecture, hyperparameter tuning, and evaluation metrics.
1. You mention using a ""deep learning network"" for prediction in Chapter 2, but it is unclear which specific network architecture was used. To enhance clarity, I recommend expanding on the architecture in the ""Model Architecture"" section by specifying the type of network used, detailing each layer's configuration activation functions, and providing a structure diagram. This will help readers understand the model design.

Experimental design
2. In Section 3, you mention standardization but do not provide enough detail on the methodology or parameter selection. I suggest expanding this section by specifying how standardization was performed using the mean and standard deviation of the training set, whether missing values were imputed, and how outliers were handled.
3. The model training section lacks any mention of hyperparameter tuning. I recommend discussing hyperparameter optimization in the ""Model Training"" section, describing the methods used (such as grid search or random search), and specifying the ranges of hyperparameters explored.

Validity of the findings
4. Currently, you only use accuracy as an evaluation metric, which may not fully capture the model's performance, especially in imbalanced datasets. I recommend adding other evaluation metrics such as F1-score, AUC, and Precision-Recall curves, especially when dealing with classification tasks, to provide a more comprehensive assessment of the model's performance.
5. The manuscript does not provide sufficient information about the experimental setup, hardware configurations, or random seeds, which makes reproducing the experiments difficult. I suggest adding this information in the ""Experimental Setup"" section, including details of the hardware platform used, the framework version, and the random seed to ensure reproducibility.
6. The manuscript does not compare with other models or traditional methods. To validate the superiority of the proposed model, I recommend adding comparison experiments with at least two traditional machine learning methods (such as SVM XGBoost) and presenting their performance in terms of accuracy, F1-score, etc.",Authentic
Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios,"In contemporary wireless communication systems, channel estimation and optimization have become increasingly pivotal with the growing number and complexity of devices. Communication systems frequently encounter multiple challenges, such as multipath propagation, signal fading, and interference, which may result in the degradation of communication quality, a reduction in data transmission rates, and even communication interruptions. Therefore, effective estimation and optimization of channels in complex communication environments are of paramount importance to ensure communication quality and enhance system performance. In this article, we address the intelligent, reflective surface (IRS)-assisted channel estimation problem and propose an intelligent channel estimation model based on the fusion of convolutional neural network (CNN) and gated recurrent unit (GRU) row features, utilizing the reinforcement learning Deep Deterministic Policy Gradient (DDPG) strategy for Channel Reconstruction Prediction and Generation Network (CRPG-Net). The framework initially acquires the received signal by converting the guide-frequency symbols at the transmitter into time-domain sequences to be transmitted, and after propagating through the direct channel and the IRS reflection channel, processes the data at the receiver. Subsequently, the spatial and temporal features in the received signal are extracted using the CRPG-Net model, with the adaptive optimization capability of the model enhanced by deep reinforcement learning. The introduction of reinforcement learning enables the model to continuously optimize decisions in dynamic channel environments, improve the robustness of channel estimation, and quickly adjust the IRS reflection parameters when the channel state changes to adapt to complex communication conditions. Experimental results demonstrate that the framework achieves significant channel estimation accuracy and robustness across several public datasets and real test scenarios, with the channel estimation error markedly smaller than that of traditional least squares (LS) and linear minimum mean square error (LMMSE) methods. This method introduces innovative techniques for channel estimation in intelligent communication systems, playing a crucial role in enhancing communication quality and overall system performance.","Basic reporting
All comments have been added in detail to the last section.

Experimental design
All comments have been added in detail to the last section.

Validity of the findings
All comments have been added in detail to the last section.

Additional comments
Review Report for PeerJ Computer Science
(Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios)

1. Within the scope of the study, a reinforcement learning based deep learning model was proposed to increase the quality of communication in complex communication environments and various channel estimation studies were carried out.

2. In the introduction, intelligent reflective surface technology and the importance of the subject with communication technologies were mentioned at a basic and sufficient level. In addition, the main contributions of the study were stated clearly and in bullet points.

3. In the Related works section, the literature related to the study was discussed in terms of both channel estimation based on deep learning methods and intelligent reflective surface and joint sensing channel estimation. Although the literature on the subject was mentioned in this section, a more in-depth analysis should be made especially in terms of deep learning methods. In this section, it is suggested to add a detailed literature table so that the proposed model can come to the forefront more.

4. In the Methodology section, both gated recurrent unit networks and convolutional neural networks based on deep learning were mentioned at a basic but sufficient level.

5. When the framework and details of the proposed CRPG-Net model are examined and compared with the literature, it is observed that it has a certain level of originality in this study.

6. Examining the types of metrics used in the study and the results obtained accordingly, it is understood that they are both at an acceptable level. In addition, comparing the obtained results with some models in the literature and demonstrating their superiority further increases the quality of the study.

As a result, the study has the potential to present an important deep learning-based model to the literature in terms of channel estimation model. However, attention should be paid to the above sections.",Authentic
Isolation and identification of endophytic fungi from Conyza blinii that exhibit antioxidant and antibacterial activities,"Background
As a medicinal plant, Conyza blinii is known to contain a wealth of bioactive constituents, including flavonoids, terpenes, and triterpenoid saponins, which contribute to its anti-inflammatory and anticancer properties. Endophytic fungi, which symbiotically inhabit plant tissues, are recognized for their ability to synthesize bioactive metabolites analogous to those of their hosts. However, the potential of C. blinii-associated endophytes remains underexplored. This study aims to isolate and characterize phenols-producing endophytic fungi from C. blinii, evaluate their biological activities, and analyze their chemical components to provide new insights for drug development.

Methods
During the study, 20 endophytic fungi were isolated from C. blinii. The Folin-Ciocalteu method was used to screen for strains capable of producing phenolic compounds. To assess their bioactivity, ethyl acetate extracts of different concentrations were tested for antibacterial and antioxidant activities. Antibacterial activity was evaluated using minimum inhibitory concentration (MIC) determinations, while antioxidant activity was assessed through 2,2-Diphenyl-1-picrylhydrazyl (DPPH) radical, 2,2′-Azinobis-(3-ethylbenzthiazoline-6-sulfonic acid) (ABTS) radical, hydroxyl radical, and superoxide anion radical scavenging assays. Additionally, liquid chromatography-mass spectrometry analysis was conducted to quantify the active components in the extracts.

Results
Among the isolated 20 endophytic fungi, four strains successfully produced phenolic compounds, with the highest total phenolic content of 77.17 ± 1.93 mg milligrams of gallic acid equivalents per gram of extract (GAE/g). All ethyl acetate extracts from the endophytic fungi exhibited good antibacterial and antioxidant properties. Notably, Fusarium circinatum demonstrated exceptional antioxidant activity, with scavenging rates for DPPH and ABTS radicals reaching 94.28 ± 0.042% and 96.60 ± 0.017%, respectively. The ethyl acetate extract of F. foetens showed remarkable antibacterial effects against Escherichia coli and Staphylococcus aureus, with MIC values as low as 0.5 mg/mL. Furthermore, liquid chromatography-mass spectrometry (LC-MS) analysis revealed that F. foetens could produce various high-value phenolic compounds, including tyrosol (626.1884 ng/mL) and homovanillic acid (369.15486 ng/mL), which hold potential pharmaceutical value.

Discussion
This study isolated 20 endophytic fungi from C. blinii, discovering that four strains, produced phenolic compounds with strong antioxidant and antimicrobial properties. Among them, F. circinatum exhibited the highest antioxidant activity. Additionally, the fungi produced bioactive metabolites with potential applications in health care, medicine, and agriculture. These findings highlight the potential of C. blinii endophytes for sustainable bioactive compound production.","Basic reporting
The English language should be improved to ensure that an international audience can clearly understand your text. I suggest you have a colleague who is proficient in English and familiar with the subject matter review your manuscript, or contact a professional
editing service.
In the Introduction the authors stated ""This study might potentially facilitate the future use of bioactive compounds generated by C. blinii endophytic fungi in the food or medicinal sectors"". Detailed explanations for these perspectives should be provided.
The Discussion section needs more details. Relevant and recent studies on the isolated and selceted fungi should be cited and discussed. Particular attention must be given to the metabolites produced by these fungi.
The quality of the figures must be increased.
Different decimals are reported in Table 5 the authors should report the same number of decimals.

Experimental design
Why the auhtors decided to work on endophytic fungi of C. blinii? They stated ""At present, the researches on endophytic fungi of C. blinii are few and single."" What does it means? Is there a single research or few? The relative references are missing.

Validity of the findings
From table six in the supporting information it can be seen that the masses were also recorded in positive mode but only the results of negative mode were reported. What about all the other compounds produced by the fungi? Fusarium spp. are able to produce micotoxins and it is important to understand if the selected fungi are also producers of micotoxins. This will help to propose them as producers of compounds with potential medicinal value.
The retention times reported in Table 5 are not coincident with those reported in Supplementary Figure 3.
Were internal standards used for the quantification of the metabolites present in the different extracts?
Are the authors certain that all the metabolites were produced by fungi or are some of them the result of contamination? For example 4-dodecylbenzenesulfonic acid is a synthetic strong anionic surfactant and it's a key ingredient in many household and industrial detergents. Are the authors sure that in not a contaminat?
The conclusions need to be implemented by adding future perspectives on the practical application of these fungi, of thier extracts and of their pure metabolites.

Additional comments
Line 2: ""higher""? I suggest to delete it.
Line 22-23: Why it necesseray to report this sentence ""As a medicinal plant, Conyza blinii may contain a wealth of bioactive constituents""?
Line 49: ""Conyza blini"" should be ""Conyza blinii"".
Line 136: ethyl n-butanol???",Generic
Effects of gerbil disturbance on the ecological stoichiometric characteristics and nutrient uptake and utilization of H. ammodendron,"Rodent activity is an important factor that affects the growth and development of Haloxylon ammodendron. Studying the effect of rodent disturbance on plant ecological stoichiometric ratios helps evaluate the mechanism by which rodent disturbance affects plant growth and development. In this study, H. ammodendron, a dominant plant, and the gerbil, a typical rodent in the Gurbantunggut Desert, were selected as research objects. By measuring the biomass, root soil , and C: N: P ecostoichiometric ratios of the assimilated branches of H. ammodendron at different growth phases, the impact of great gerbil disturbance on the biomass, ecostoichiometric ratios, and nutrient uptake and use of H. ammodendron were investigated at different growth stages. The results showed that the gerbil disturbance increased the biomass of the aboveground part of the adult H. ammodendron. Gerbil disturbance also increased the soil N/P around the roots during the growth stage and the assimilation branch when the plants were middle-aged. In addition, this disturbance decreased the C/N value. The photosynthetic nitrogen use efficiency (PNUE) and photosynthetic phosphorus use efficiency (PPUE) of H. ammodendron during various growth periods decreased, and the absorption of total nitrogen (TN) in the soil decreased. However, soil total potassium (TK) absorption increased. The soil TN absorption capacity was weakened by gerbil disturbance. Meanwhile, the TK absorption capacity was enhanced, and the biomass of adult H. ammodendron increased. PNUE and PPUE of H. ammodendron were decreased by gerbil interference. In this study, the influence of gerbil disturbance on nutrient absorption by H. ammodendron and use of H. ammodendron was determined. This has provided a baseline for further studies on the coexistence mechanisms of gerbils and H. ammodendron.","Basic reporting
1. Basic Reporting
The report is well written, but many grammatical errors need linguistic review.

Experimental design
2. Experimental design
Materials & Methods
- Study area
It is preferable to put coordinates in place to define the study area better.
- Experimental design and analysis process
Clear

Validity of the findings
4. Validity of the findings
clear and correct
5. Tables and figures
clear and effective

Additional comments
6. General comments
The whole paper needs an English review.
7. Confidential notes to the editor ammodendron.
None",Generic
"Biomass allocation, carbon content change and carbon stock distribution of Scots pine (Pinus sylvestris var. mongholica) plantation forests at different stand ages and densities in the sandy area of western Liaoning Province, China","Scots pine (Pinus sylvestris var. mongholica) is one of the main afforestation species in the southeastern edge of the Horqin Sandy Land, which not only effectively prevents the expansion of the sandland, but also serves as an important carbon reservoir. Uncovering the biomass allocation, carbon content changes and carbon stock distribution among organs of Scots pine at different ages and densities can provide a theoretical basis for rational afforestation and management in the western Liaoning sandy area. In this study, the biomass and carbon content of four organs, namely, trunk, branch, leaf and root, were measured at different age classes (young stage, half-mature stage, near-mature stage, mature stage and over-mature stage forests) and densities, and the carbon stock of Scots pine plantations in the western Liaoning sandy area was estimated. The results showed that the biomass of all organs except leaves increased with the increase of stand age, but the rate of increase of each organ was not consistent. To resist wind and sand, the biomass was preferentially allocated to the trunk and roots, which was in line with the theory of allometry and optimal allocation. The carbon content of each organ of Scots pine increases and then decreases with the rise of forest age classes, and the root carbon content is the lowest in five forest ages, and the plant carbon is mainly stored in the aboveground part. The biomass of each organ in both near mature and mature forests increased with the decrease in density. Still, the root carbon content decreased with the decrease of density, and the PCA analysis showed that near mature and mature forests had better carbon sequestration capacity in low density. The carbon stock of Scots pine plantation forests in the sandy area of western Liaoning was mainly concentrated in Fuxin and Chaoyang cities, and the lowest carbon stock was found in Jinzhou. The age and density of the forest stand are important factors affecting the biomass and carbon content of Scots pine, therefore, when operating Scots pine plantation forests in the sandy areas of western Liaoning, different stand densities should be retained at different age stages, so that their biomass and carbon content can be sufficiently accumulated and distributed to improve the local environment.","Basic reporting
The article is very well prepared and contains the results studied in detail. Afforestation efforts and their success, especially in stressful areas, are of vital importance. Examining both the success and carbon sequestration potential of such afforestation will shed light on the applications. The article contains important results in these aspects and can be recommended for acceptance.

The tree specifically studied in the article is a variety and there are some errors in its Latin spelling. It will be sufficient to write the Latin name once and correctly. Its English name is stated as ""Pine"". Instead of this common name, the common name of the specific variety must be written. Throughout the text, it would be appropriate to continue with the specific common name only after specifying the common name (Latin name) in the Abstract section.

Experimental design
Throughout the article, there are references such as ""Chen Zheng et al. showed that the highest carbon stocks 79 are expected in northeastern China by 2060 (Chen et al., 2024)."" Instead of writing the references in this way, it would be more appropriate to write them as ""Chen Zheng et al. (2024) showed that the highest carbon stocks are expected in northeastern China.""

Research question is well defined, relevant & meaningful. However, the aim of the study may clearly been writen just after the hypotheses.

Validity of the findings
The results and analysis looks meaningful and include important comparisons from different age classes and land features.

Additional comments
This paper may be accepted after making some minor corrections",Authentic
Mass harvested per trunkload as a constraint to forage consumption by the African savanna elephant (Loxodonta africana),"African elephants can convert woodland to shrubland or grassland. Moderate conversion observed at low elephant densities may improve conditions for other animals, while extensive transformation at high densities may reduce plant and animal diversity. The threshold density separating facilitation from habitat destruction varies spatially and is partly determined by food choice, which differs between adult bulls and members of breeding herds. When elephants consume herbaceous forage, woodland damage is low but this increases when woody plants are the primary food source. Consequently, an understanding of diet selection by elephants is important for forecasting the degree of vegetation conversion. One hypothesis is that elephants select forage that provides the highest rate of intake. The mass harvested per trunkload is a constraint to intake and therefore this study sought to determine if trunkload mass changes seasonally; varies across common forage types utilised by elephants; and differs between adult bulls and members of breeding herds.

Methods
Mechanistic models were used to estimate the mass harvested per trunkload of green grass, mixed green and dry grass, forbs, and leaves and bark from woody plants across a heterogenous, semi-arid savanna at a daily time step for one annual cycle. Separate models were constructed for adult bulls and members of breeding herds.

Results
Harvestable mass changed seasonally for herbaceous forage and for leaves from woody plants but was constant for canopy bark. The maximum average trunkload mass of green grass was >75 times heavier than the bite mass reported for other grazers while trunkloads of leaves from woody plants were only eight times heavier than the bite mass reported for other browsers. This is attributed to the advantage provided by the trunk, which increases harvestable mass beyond the constraint of mouth volume, particularly when feeding on grass. Herbaceous forage yielded heavier trunkloads than leaves and bark from woody plants during the wet season, but this was reversed in the dry season. Adult bulls harvested heavier trunkloads than members of breeding herds for all forage types except forbs; and adult bulls harvested disproportionately large trunkloads of grass and bark.

Conclusion
The strong correlation between the model outputs and well-established trends in the seasonal changes in elephants’ diet suggests that elephants are preferential foragers of the largest trunkload on offer. Consequently, they are grazers when suitable herbaceous forage is available, and browsers when it is scarce. Green grass provides adult bulls with disproportionately large trunkloads and, therefore, adult bulls are predicted to have a strong preference for green grass. Availability of suitable green grass during the dry season may therefore buffer woodlands from heavy impact by adult bulls. Consequently, where possible, protected areas with elephants should aim to include key grass resources.","Basic reporting
The writing is clear and unambiguous. The literature utilised is correct and sufficient. Structure is correct. Results link to hypotheses.

Experimental design
Experimental design is excellent. research questions are clear. Very rigorous investigation. Methods described well.

Validity of the findings
Findings are solid, robust and sound. Conclusions well stated.

Additional comments
In this paper, the authors used mechanistic models to estimate how mass harvested per trunkload of different vegetation types (e.g. grass, forbs, woody vegetation) varied between seasons, and males and breeding herds. They found that harvestable mass varied seasonally, and bulls harvested larger trunkloads compared to breeding herds across all vegetation types. In addition, elephants obtained trunkload masses of green grass that were >75 times larger than other grazers, and trunkload masses of woody vegetation that were 8 times larger than other browsers. This is not too surprising (but still very cool) as elephants are unique in this comparison in being able to use their trunks to obtain food and not having to rely on mouth width, teeth, tongues, or lips to take bites. In addition, I really liked the fact that the authors could quantify when the transitions would and should take place by determining when the harvestable mass of a preferred food source (e.g. green grass) declines to the point where it overlaps with the mass available next preferred food source (e.g. green browse). This provides a really nice example of how herbivores expand their diet breadth. Moreover, the similar patterns of forage selection between the authors’ models and typical elephants foraging patterns highlights elephants’ preference for green grass. Understanding this, the authors suggest that in areas with a high availability of green grass into the dry season would reduce elephant foraging and thus impacts on woody vegetation. Overall, I really enjoyed the paper. It was a real pleasure to read. It is well written, easy to follow, and timely. Moreover, the results will be helpful to anyone working on elephant foraging. Due to the high quality of the manuscript I cannot think of any adjustments that would improve the manuscript. This is the first time that this has happened, so congratulations to the authors!

Prof Adrian M Shrader
University of Pretoria",Authentic
Use of integrated population models for assessing density-dependence and juvenile survival in Northern Bobwhites (Colinus virginianus),"Management of wildlife populations is most effective with a thorough understanding of the interplay among vital rates, population growth, and density-dependent feedback; however, measuring all relevant vital rates and assessing density-dependence can prove challenging. Integrated population models have been proposed as a method to address these issues, as they allow for direct modeling of density-dependent pathways and inference on parameters without direct data. We developed integrated population models from a 25-year demography dataset of Northern Bobwhites (Colinus virginianus) from southern Georgia, USA, to assess the demographic drivers of population growth rates and to estimate the strength of multiple density-dependent processes simultaneously. Furthermore, we utilize a novel approach combining breeding productivity and post-breeding abundance and age-and-sex ratio data to infer juvenile survival. Population abundance was relatively stable for the first 14 years of the study but began growing after 2012, showing that bobwhite populations may be stable or exhibit positive population growth in areas of intensive management. Variation in breeding and non-breeding survival drove changes in population growth in a few years; however, population growth rates were most affected by productivity across the entire study duration. A similar pattern was observed for density-dependence, with relatively stronger negative effects of density on productivity than on survival. Our novel modeling approach required an informative prior but was successful at updating the prior distribution for juvenile survival. Our results show that integrated population models provide an attractive and flexible method for directly modeling all relevant density-dependent processes and for combining breeding and post-breeding data to estimate juvenile survival in the absence of direct data.","Basic reporting
This paper is well written. It includes sufficient and appropriate refences. Although there were ample supplemental figures and explanations provided, I did not see links to data or code, but I presume/hope the authors will provide those at a later date.

Experimental design
I have no concerns about experimental design. The study is especially impressive for having maintained consistent and rigorous data collection for so many years in a row.

Validity of the findings
There was nothing to make me question the validity of the findings.",Generic
Buscando Luciérnagas: findings on Mexican fireflies from an 8-year virtual citizen science project,"Fireflies are charismatic and conspicuous animals that often evoke childhood memories, which make firefly watching an emotional and even transformative experience. Citizen science projects have the potential to enhance transformative interactions with nature. Like many insects, firefly populations are declining due to land-use change, urbanization and watershed pollution, but ecological data for this group is scarce, particularly in Mexico. Virtual Citizen Science (VCS) initiatives can serve as a scientific instrument, yield reliable and relevant scientific data, and may also offer a platform to promote broader educational outcomes. We established a VCS project to document fireflies through a Facebook page named Buscando Luciernagas with the following hashtag in every post #veobrillar in 2015. After seven years we complied the gathered data and analyzed the results. We had 647 reports in total, with strong fluctuations from year to year that were correlated with the number of posts and publicity we made each year. The largest number of sightings (319) occurred in 2021, coinciding with a change in our reporting format. Most of the reports came from central Mexico (91.5%), but we had reports from eight states and also received some international reports from nine different countries. Fireflies were most frequently seen in habitats characterized as grasslands (35%) or forests (27%), followed by gardens (17%), vacant lots (9%) and parks (5%) but also paved areas and agricultural lands were reported (3% each). Most citizen scientists reported few fireflies, 1–5 individuals (31%) while only 11% reported more than 50 fireflies per sighting. Our study can serve as a preliminary approach to explore more focused research areas in the future. For example, in areas with no sightings, we could reach out to specific local people to corroborate that there are no fireflies in the region, or in areas with high sightings we could promote conservation measures. Notably, we found it intriguing to discover numerous sightings of fireflies in urban areas, which could offer a potential avenue for further research in urban ecology.","Basic reporting
I have revised both the manuscript and the rebuttal letter. My intention was to see whether authors followed those reasonable suggestions so that the new version looks improved. My general impression is that authors did a great job in crafting their work both by accepting those criticisms that made sense as well as adding or removing information they realized was or was not needed.

Given the above, my opinion is that this paper looks great now and ready to be accepted.

Experimental design
This is report of data collected by citizens who watched fireflies (mainly in Mexico) and sent such information via Facebook. This approach is valid.

Validity of the findings
Validity is 100% as authors filtered and analyzed the sightings good enough to sustain their conclusions. I particularly like the data where fireflies are linked to land use as this is very novel. This should serve as an initiative to promote firefly conservation.",Authentic
Tree regeneration and ontogenetic strategies of northern European hemiboreal forests: transitioning towards closer-to-nature forest management,"Background
Tree ontogeny is the genetic trajectories of regenerative processes in trees, repeating in time and space, including both development and reproduction. Understanding the principles of tree ontogeny is a key priority in emulating natural ecological patterns and processes that fall within the calls for closer-to-nature forest management. By recognizing and respecting the growth and development of individual trees and forest stands, forest managers can implement strategies that align with the inherent dynamics of forest ecosystem. Therefore, this study aims to determine the ontogenetic characteristics of tree regeneration and growth in northern European hemiboreal forests.

Methodology
We applied a three-step process to review i) the ontogenetic characteristics of forest trees, ii) ontogenetic strategies of trees for stand-forming species, and iii) summarise the review findings of points i and ii to propose a conceptual framework for transitioning towards closer-to-nature management of hemiboreal forest trees. To achieve this, we applied the super-organism approach to forest development as a holistic progression towards the establishment of natural stand forming ecosystems.

Results
The review showed multiple aspects; first, there are unique growth and development characteristics of individual trees at the pre-generative and generative stages of ontogenesis under full and minimal light conditions. Second, there are four main modes of tree establishment, growth and development related to the light requirements of trees; they were described as ontogenetic strategies of stand-forming tree species: gap colonisers, gap successors, gap fillers and gap competitors. Third, the summary of our analysis of the ontogenetic characteristics of tree regeneration and growth in northern European hemiboreal forests shows that stand-forming species occupy multiple niche positions relative to forest dynamics modes.

Conclusions
This study demonstrates the importance of understanding tree ontogeny under the pretext of closer-to-nature forest management, and its potential towards formulating sustainable forest management that emulates the natural dynamics of forest structure. We suggest that scientists and foresters can adapt closer-to-nature management strategies, such as assisted natural regeneration of trees, to improve the vitality of tree communities and overall forest health. The presented approach prioritizes ecological integrity and forest resilience, promoting assisted natural regeneration, and fostering adaptability and connectivity among plant populations in hemiboreal tree communities.","Basic reporting
The article by Raimundas Petrokas, Michael Manton and Darius Kavaliauskas is devoted to the review of ideas on discrete description of tree ontogeny and tree regeneration strategies for the development of closer-to-nature forest management strategies. This review is of broad interest and within the scope of the journal. The introduction presents in detail and adequately the topic of the review and its relevance for a wide range of specialists related to both forest theory and practice.

Experimental design
The presentation of results and discussion are greatly improved by the authors. Links between different parts of the review have been added. Information on existing concepts has been added and new sources discussed. The quality of the text has been improved so that it can be understood by specialists from different disciplines.

Validity of the findings
The conclusion and abstract are generally consistent with the content of the review, identifying opportunities for future directions in forest management in relation to more fully utilizing knowledge of tree species biology. The authors have improved the linkage of the conclusion to the discussion.

Additional comments
The article may be recommended for publication.",Generic
Association of metallic elements with telomere length in children with autism spectrum disorder,"Background
Imbalances in metal elements have been identified as a potential risk factor for autism spectrum disorder (ASD), and shortened telomere length (TL) is commonly observed in children with ASD. Metal elements may influence telomere homeostasis through oxidative stress, which could contribute to the pathogenesis of autism. However, studies examining the combined effects of metal elements on TL in children with ASD are limited. To fill the gaps in the current literature, this study aimed to investigate the relationship between six metallic elements: manganese (Mn), copper (Cu), zinc (Zn), calcium (Ca), magnesium (Mg), and iron (Fe), and TL in the whole blood of children with ASD.

Methods
A total of 83 children with ASD and 95 typically developing children were recruited. TL was measured using digital PCR, while metal concentrations were assessed using inductively coupled plasma mass spectrometry (ICP-MS). Linear regression analysis was first conducted to explore the correlations between metal elements and TL in both groups. Additionally, Bayesian Kernel Machine Regression (BKMR) was used to further examine the combined effects and potential interactions of these metals on TL in the ASD group.

Results
In the ASD group, Ca was found to have a protective effect on TL (β = 0.07, 95% CI [0.01–0.13], P = 0.027). In contrast, Mg showed a protective effect on TL in the control group (β = 0.10, 95% CI [0.01–0.18], P = 0.027). The BKMR model revealed a significant positive combined effect of the metal mixtures on TL in the ASD group, with Ca having the largest individual effect (PIP = 0.45). Further analysis indicated that increases in Zn and Mn concentrations from the 25th to the 75th percentile were negatively correlated with TL, while higher concentrations of Cu, Ca, Mg, and Fe were positively associated with TL. No significant interactions among the metals were observed.

Conclusions
This study suggests a potential link between metallic elements and TL in children with ASD, with Ca having the greatest effect. Our findings highlight the potential benefits of appropriate calcium supplementation as a protective strategy for lengthening telomeres in children with ASD, emphasizing the importance of early nutritional interventions to improve their overall health.","Basic reporting
I am satisfied with the answers provided by authors.

Experimental design
Authors explained restrictions concerning experimental design.
I accept their explanation.

Validity of the findings
Revised text is good and I accept authors explanation.

Additional comments
No additional comments",Generic
"The Italian version of the extended Barcelona Music Reward Questionnaire (eBMRQ): a validation study and association with age, gender, and musicianship","Background
Music is a primary source of pleasure for humans. Nevertheless, there is large interindividual variability in how individuals experience and derive pleasure from music and music-related activities. With this study we propose and validate the Italian version of the extended Barcelona Music Reward Questionnaire (eBMRQ), the most in-depth and comprehensive tool for investigating the diverse characterization of individual sensitivity to pleasure in music. In addition, we aim to investigate eBMRQ scores as a function of age, gender, and musicianship across Italian population.

Methods
For the validation process of the Italian eBMRQ, we first conducted forward and backward translation from the original English eBMRQ version. The new Italian version was then administered to 1,012 participants who were fluent in Italian from the north and the south of Italy through online surveys (age range 18–86 years old; M = 34.9, SD = 16.9, females 74%). Unrestricted confirmatory analysis was computed for both six-factor and single-factor models. The effect of gender, age, and musicianship on eBMRQ scores was analyzed through analysis of variance (ANOVA).

Results
The quality assessment of the factor solution indicated that the Italian eBMRQ demonstrated acceptable quality and reliability, making it a valid tool for assessing sensitivity to music reward. All factors were significantly correlated with each other, in line with previous adaptations of the BMRQ. Our findings indicate that females reported higher music reward sensitivity compared to males, except for Social Reward subscale. Moreover, individual reward sensitivity was significantly higher among musicians and amateurs compared to non-musicians, although this trend did not emerge for Sensory-motor and Mood Regulation subscales. Also, overall musical reward sensitivity was negatively associated with age.

Conclusions
The results obtained suggest the feasibility of applying the Italian version of eBMRQ as a reliable tool in the field of affective and clinical music-related research. Furthermore, the significant associations we have highlighted between eBMRQ scores, gender, age, and musicianship contribute to emphasizing the significant impact of individual factors on music reward sensitivity.","Basic reporting
The authors have a done thorough job in providing the missing details and have patiently explained and expanded the issues raised.

Experimental design
Nothing extra to report here, the authors have addressed the issues raised and fixed the minor omissions.

Validity of the findings
All the issues have been addressed and the choices about the factor interpretations and age groupings have been explained in a satisfactory way.

Additional comments
I thank the authors for their diligent and valuable work on this contribution music scholarship and it is great to see careful and systematic validation and expansion of the tools to other languages and cultural contexts.",Generic
Identification of bacteria on Thai banknotes and coins using MALDI-TOF mass spectrometry and their phenotypic antimicrobial susceptibility profiles,"Background
The existence and transmission of pathogenic and antibiotic-resistant bacteria through currency banknotes and coins poses a global public health risk. Banknotes and coins are handled by people in everyday life and have been identified as a universal medium for potentially microbial contamination.

Methods
To ascertain existence of medically important bacteria, a total of 300 samples including 150 banknotes and 150 coins were randomly collected at onsite retail fresh meat stores, i.e., pork and chicken, fish, and seafood stores, from nineteen fresh markets distributed across Bangkok, Thailand. An individual banknote or coin was entirely swabbed, and bacterial culture was carried out using tryptic soy agar (TSA), sheep blood agar (SBA) and MacConkey agar (Mac). A colony count was performed and bacterial species identification was conducted using matrix-assisted laser desorption/ionization (MALDI)-time of flight (TOF) mass spectrometry. Phenotypic antimicrobial susceptibility testing was carried out using the Kirby–Bauer disc diffusion methods.

Results
The results demonstrated that the bacterial contamination rate was higher on banknotes than on coins (93.33% vs. 30.00%) in all three store types. A substantial number of colonies of >3,000 colony forming units (CFU) was predominantly found in banknotes (70.00%), especially from fish store (83.3%); meanwhile, <1,000 CFU was observed in coin sample (76.67%). MALDI-TOF mass spectrometry could identify 107 bacterial species, most of them were Staphylococcus kloosii (14.02%, 15/107), Staphylococcus saprophyticus (12.15%, 13/107), and Macrococcus caseolyticus (8.41%, 9/107). The prevalence based on genera were Staphylococcus (36.45%, 39/107), Acinetobacter (20.56%, 22/107), and Macrococcus (10.28%, 11/107). Almost all Staphylococcus isolates had low susceptibility to penicillin (21%). Notably, Staphylococcus arlettae, Staphylococcus haemolyticus and M. caseolyticus were multidrug-resistant (MDR). It is notable that none of the staphylococci and macrococci isolates exhibited inducible clindamycin resistance (D-test negative). Escherichia coli and Pseudomonas putida isolates were carbapenem-resistant, and Acinetobacter baumannii isolates were MDR with showing carbapenem resistance.

Conclusion
Our data demonstrated a high prevalence of medically important bacteria presented on Thai currency, which may pose a potential risk to human health and food safety. Food vendors and consumers should be educated about the possible cross-contamination of bacteria between the environment, food item, and currency.","Basic reporting
In this study, the authors investigated bacterial contaminants on banknotes and coins using MALDI-TOF Mass Spectrometry and determined antimicrobial susceptibility testing (AST) in selected isolates. While the manuscript presents interesting data, several areas require clarification and improvement before further consideration.
1. The authors should specify the criteria for selecting the 150 banknotes and coins used in the study.
2. Additional samples from other sources, such as vegetable and grocery stores, should be included to provide more comprehensive insights into the prevalence of medically important bacterial pathogens.
3. I recommend using phosphate-buffered saline (PBS) instead of TSB to resuspend the swab.
4. Bacterial identification should be performed directly from the plates used for colony counting to enhance the accuracy of results.
5. The authors reported a higher contamination rate on banknotes than on coins, possibly due to differences in size or surface area. However, the comparison between banknotes and coins may not be justified. Instead, a comparison of contamination levels across different collection sites for both banknotes and coins would be more informative.
6. The study focuses solely on bacterial contamination. The authors should also examine fungal pathogens to provide a more comprehensive analysis.
7. Based on the results, the variation among bacterial contaminants appears minimal. Further clarification is needed to support this conclusion.
8. References should be included throughout the methodology section, and detailed information about the database used for bacterial identification should be provided.

Experimental design
The experimental design requires improvement.

Validity of the findings
Validity of the findings looks good to me

Additional comments
NA",Authentic
Selective enrichment of active bacterial taxa in the Microcystis associated microbiome during colony growth,"The toxic cyanobacterium Microcystis causes worldwide health concerns, being frequently found in freshwater and estuarine ecosystems. Under natural conditions, Microcystis spp. show a colonial lifestyle involving a phycosphere populated by a highly diverse associated microbiome. In a previous study, we have proposed that colony formation and growth may be achieved through mechanisms of multispecies bacterial biofilm formation. Starting with single-cells, specific bacteria would be recruited from the environment to attach and create a buoyant biofilm or colony. This progression from a few single cells to large colonies would encompass the growth of the Microcystis community and bloom formation. In order to test this, we applied 16S rDNA metabarcoding to evaluate the changes in bacterial community structure (gDNA) and its active portion (cDNA) between different sample sizes obtained from a Microcystis bloom. Bloom sample was sieved by size, from one or a few cells (U fraction) to large colonies (maximum linear dimension ≥ 150 µm; L fraction), including small (20–60 µm, S fraction) and medium size (60–150 µm, M fraction) colonies. We found that gDNA- and cDNA-based bacterial assemblages significantly differed mostly due to the presence of different taxa that became active among the different sizes. The compositional variations in the communities between the assessed sample sizes were mainly attributed to turnover. From U to M fractions the turnover was a result of selection processes, while between M and L fractions stochastic processes were likely responsible for the changes. The results suggest that colony formation and growth are a consequence of mechanisms accounting for recruitment and selection of specific bacterial groups, which activate or stop growing through the different phases of the biofilm formation. When the final phase (L fraction colonies) is reached the colonies start to disaggregate (bloom decay), few cells or single cells are released and they can start new biofilms when conditions are suitable (bloom development).","Basic reporting
1) I am not so convinced of the often-used terms “Microcystis and its microbiome” or “microbiome of Microcystis”. This is because Microcystis is a microbe, too, and it doesn’t really have a microbiome like the human gut or Daphnia for example… Personally I find “Microcystis associated microbiome” or something similar more fitting.
2) Taxonomy: L 171and onward – Why did you reclassify Burkholderiales as Beta? I think Silva is now based on the revised taxonomy where Beta don’t exist anymore. From silva: “With SILVA release 138 the Genome Taxonomy Database (GTDB) has been adopted. As a consequence of our efforts the following groups were prone to significant adaptations: Archaea, Enterobacterales, Deltaproteobacteria, Firmicutes, Clostridia. Betaproteobacteriales (formerly known as Betaproteobacteria) is now Burkholderiales, an order of Gammaproteobacteria. Epsilonproteobacteria vanishes within a new phylum Campilobacterota. Tenericutes are gone, they are now all part of Bacilli, inside Firmicutes. “ (for more info see https://www.arb-silva.de/documentation/faqs/)
I think just reclassifying a single group is really wrong. If the authors don’t agree with this new taxonomy I guess they should use an old taxonomy but then completely commit to that one since there are many groups with new taxonomic names in the text. Just changing one group is a bit arbitrary.
3) I could not evaluate data sharing since the sequences are not public

small suggestions for the text:
L 116 – filtered onto ?
L 121 – for RNA later I don’t think the ® is need, maybe it would be better to mention the producer.
L 136 – Toxic cell abundance
L 153 - onward, also here I would remove the ® and TM it seems a bit arbitrarily put just for some products.
L 159 – BP not pb
L 174 – the sequencing data is not public so I couldn’t check.
L 180 & 197 - there is some problem with ( -diversity)
Fig 5 – where are the unclassified genera? Are they in the <5% or removed from the graph?

Experimental design
L 138 – Please give more detailes on qPCR, such as machine, quantification system and standard curve consruction. Also how was specificity and inhibition evaluated.
L 141 – copy number per what?

Validity of the findings
no comment

Additional comments
The authors have analysed their data in an appropriate and very interesting way attributing changes in the community structure to different ecological processes. This study was clearly well planned to analyse exactly what they did. In my opinion the study is basically flawless and the results are very interesting. Graphs are comprehensive and aesthetically pleasing. The article is well written, and of appropriate length. It was a pleasure to review this work.",Authentic
Performance comparison of QuantiFERON-TB Gold In-Tube and QuantiFERON-TB Gold Plus in detecting Mycobacterium tuberculosis infection among HIV patients in China,"Introduction
No direct comparative study assessing QuantiFERON-TB Gold In-Tube (QFT-GIT) and QuantiFERON-TB Gold Plus (QFT-Plus) for Mycobacterium tuberculosis infection among persons living with HIV (PLHIV) in China has been conducted.

Methods
Simultaneous QFT-GIT and QFT-Plus tests were conducted on PLHIV in a prison hospital. Positivity and negativity results from both assays were compared, and their diagnostic agreement was assessed.

Results
A total of 232 PLHIV individuals were included in this study. Among them, 57 patients (24.6%) and 56 patients (24.1%) were diagnosed with Mycobacterium tuberculosis infection based on QFT-GIT results and QFT-Plus, respectively. The overall agreement between the two assays was 98.3%, with a Cohen’s kappa value of 0.954. Consistency rates were observed between QFT-GIT plus, QFT-Plus TB1 and TB2 with QFT-GIT were 98.3%, 97.4% and 97.8%. The IFN-γ levels measured in QFT-GIT were found to surpass those in QFT-Plus TB1 (P = 0.04), while the difference compared to QFT-Plus TB2 exhibited a marginal trend (P = 0.134). Among the subgroup of 52 individuals who underwent dual QFT-GIT tests, a significant proportion of 23.1% (12 individuals) experienced a change in their QFT-GIT results, transitioning from a positive to a negative outcome.

Conclusions
The diagnostic performance of QFT-GIT and QFT-Plus for Mycobacterium tuberculosis infection among PLHIV with relatively higher CD4 counts was found to be comparable. Additionally, our investigation revealed that irrespective of the treatment regimen, whether it involved chemotherapy or immunotherapy, preventive Mycobacterium tuberculosis infection interventions among PLHIV consistently led to a reduction in IFN-γ levels.","Basic reporting
Line 115: Mycobacterium Vaccae >> Mycobacterium vaccae
Line 124: Mycobacterium tuberculosis >> Mycobacterium tuberculosis italicize species names. Do these throughout the entire manuscript

Line 218: A review and meta-evaluation revealed a 1.3% increase in sensitivity for QFT-Plus compared to QFT-GIT. Authors need to specifically mention whether the review and meta-evaluation were done in China? Because they indicated that this is the first of such studies in China.

Line 278: In conclusion, the diagnosis performance of QFT-GIT and QFT-Plus across PLHIV with relatively higher CD4 count for Mycobacterium tuberculosis infection was comparable >> In conclusion, the diagnostic performance of QFT-GIT and QFT-Plus for detecting Mycobacterium tuberculosis infection among PLHIV with relatively higher CD4 counts was comparable.

Experimental design
The original primary research's aims follow within the journal's scope.
The research question is well-defined, relevant & meaningful. The authors stated how their research fills an identified knowledge gap. The authors demonstrated that they have performed rigorous investigations to a high technical and ethical standard.

The authors described methods with sufficient detail and information to replicate.

Validity of the findings
The authors demonstrate impact and novelty in this study.

Conclusions are well-stated and linked to the original research question.",Authentic
A proof-of-concept point-of-care test for the serodiagnosis of human amebic liver abscess,"Background
Amebic liver abscess (ALA), caused by an extraintestinal invasion of the virulent protozoan Entamoeba histolytica, is important among parasitic causes of morbidity and mortality, especially in the tropics. Clinical symptoms, medical-imaging abnormalities of the liver and serological tests are normally made for supportive diagnosis. Serum-based enzyme-linked immunosorbent assay (ELISA) has been conventionally used for diagnosing ALA but is time-consuming and sophisticated equipment is required. Therefore, we sought to develop a new and rapid innovative point-of-care immunochromatographic test (ICT) that can use whole blood as an alternative to serum-based ELISA. An ICT tool using simulated whole-blood samples was developed for immunoglobulin G antibody detection, and its diagnostic efficiency was evaluated in comparison with serum-based ELISA.

Methods
Both methods were tested to assess their diagnostic performance using a total of 253 serum samples. These came from ALA patients (n = 13), healthy individuals (n = 40), and patients with other diseases (n = 200).

Results
Amebiasis-ICT exhibited 100% (95% confidential interval (CI) [75.3–100.0]) sensitivity and 97.1% (95% CI [94.1–98.8]) specificity, whereas ELISA gave the same sensitivity (100% 95% CI [75.3 –100.0]) and slightly lower specificity (95.8% 95% CI [92.5–98.0]). There were no significant differences in sensitivity and specificity between the two tests (Exact McNemar’s test; p > 0.05), with Cohen’s kappa agreement 96.44% (κ-value = 0.771, p < 0.001) indicating substantial agreement.

Conclusion
This ICT tool using simulated whole-blood samples has a high possibility of being used with real whole blood. Therefore, since there is no need to separate serum, this can be considered an innovative diagnostic tool to replace serum-based ELISA in clinics and field surveys in remote areas where medical facilities are limited.","Basic reporting
I am not native speaker; therefore I am not considered myself an authority to correct the language.
The introduction and the backgrounds are short, but they contain the necessary information to introduce the need and description of the study.
It accomplished with the Peer J. standards
It only contains one figure. It is well described, and I consider is enough to describe the design of the lateral flow system.

The authors dont mention clearly the source of the EhHK9 strain

Experimental design
This work covers topics related to health science and I consider it covers the scope of the journal.
The authors highlight the relevance of the research as they describe the need of having the development of the CT platform necessary for the diagnostic of ALA, which is a neglected disease in undeveloped countries.
The author presents the complete results without hiding any of them that can compromise the final development and commercialization of the platform.
The methods describe in detail the components of the CT platform, with the specification to obtain replicates.

Validity of the findings
The study present original results, it is not replication or any other study.
The authors present the data that support the study. They report that the total number of samples analyzed are 253, out of them, 13 are positive to ALA. I consider this number is mall and from this mall number they don’t report false positives, and 40 negative samples that also don’t present false negatives.

When they present the false positives using clinical samples from other diseases, in most of the cases they are positive for ELISA and ICT platform. These results imply that the results are inherent to the ICT, as the cross reactivity is the most common problem in this type of samples.
Based on the above observations, I consider that the conclusions where they claim that CT platform is the sero diagnostics of ALA are risky. Even if it is correct for a sero diagnostics of amebiasis, it is necessary to contrast with the clinic information of the patient to discriminate amebiasis vs. ALA.

Additional comments
Based on the sample size and the results obtained, I suggest changing the title, even when it is innovative, I consider is still a proof of concept of the sero diagnostics platform.
By definition the Amebic liver abscess (ALA) is a severe extraintestinal manifestation caused when pathogenic trophozoites of E. histolytica disseminate to the liver, at this point the CT platform does not discriminate if the antibodies produced in the patients correspond to this type of infection or they correspond to another clinical manifestations of the disease. It is is necessary to include the clinical studies and the image analyses.",Authentic
Skin microbiota variation in Indian families,"Background
In India, joint families often encompass members spanning multiple generations cohabiting in the same household, thereby sharing the same ethnicity, genetics, dietary habits, lifestyles, and other living conditions. Such an extended family provides a unique opportunity to evaluate the effect of genetics and other confounding factors like geographical location, diet and age on the skin microbiota within and between families across three generations.

Methods
The present study involved seventy-two individuals from fifteen families from two geographical regions of Maharashtra, India. The 16S rRNA sequencing of V3–V4 regions was performed and the generated taxonomic profiles were used for downstream analysis.

Results
Our study highlights a significant difference in community composition (beta diversity) between families (PERMANOVA; p = 0.001) and geographical locations (p = 0.001). We observed geographical location-wise differences in the relative abundances Staphylococcus in the families from Pune (Wilcoxon test, p = 0.007), and Bacillus in the Ahmednagar families (Wilcoxon test, p = 0.004). When within and between-family comparisons of skin microbiota composition were carried out between different generations (G1–G2, G2–G3, and G1–G3); we observed skin microbiota tended to be more similar within than between families but this difference was not significant.

Conclusion
This study underscores the diversity and commonalities in skin microbiota composition within and between families. Our result suggests that geographical location is significantly associated with the genus composition of skin microbiota, which is quantitatively unique for a family and likely explained by co-habitation.

","Basic reporting
The introduction provides a solid background, but the rationale for choosing multi-generational families specifically could be further emphasized, especially regarding why this model is particularly valuable for studying skin microbiota.

Sharing of metadata related to the sequencing results could be beneficial for transparency

provide figures of better quality

Experimental design
A more explicit discussion of the limitations due to the sample size (i.e., families with only one grandparent or sibling) could be addressed earlier in the paper. This would manage expectations about the study's power and generalizability

Further detail on how potential biases during sample collection (e.g., temperature changes, collection timing) were minimized could add rigor to the methodology

Validity of the findings
How do these results contribute to global understanding, particularly given the Indian population's unique lifestyle factors?

While the authors mention the limitation of sample size, more robust discussion around the implications of not having more genetically diverse or unrelated family members within the cohort would be useful.

Additional comments
While the manuscript is well-structured and contributes to the understanding of skin microbiota in Indian families, addressing the above points regarding methodology clarity and better highlighting the findings would strengthen the paper further.",Authentic
"A novel microbial agent reduces soil paclobutrazol residue, enhances enzyme activities and increases Ophiopogon japonicus production","
Background
Ophiopogon japonicus (O. japonicus) is a versatile plant valued for its medicinal, food, and ornamental properties. Its cultivation often involves the excessive use of paclobutrazol, leading to a series of environmental and agricultural problems such as soil contamination, nutrient depletion, and safety risks. However, there is currently no effective solution.

Methods
Based on a novel microbial agent, Micrococcus yunnanensis strain HY001 (MYSH), field experiments were conducted in the main production area of O. japonicus. Soil paclobutrazol residue, soil enzyme activities, and the yield and dry matter ratio of O. japonicus were measured. Hierarchical partitioning (HP) was used to identify the relative importance of different variables, and partial least squares path modeling (PLS-PM) was applied to elucidate the mechanisms underlying MYSH’s effects on soil health and crop production.

Results
MYSH significantly reduced soil paclobutrazol residue by 75.18% over five months, compared to a natural degradation rate of 50.72% over a year. Compared to the control group, the MYSH-treated group enhanced soil sucrase activity, soil urease activity, and soil alkaline phosphatase activity, with rates of 47.81%, 46.70%, and 216.66%, respectively. Additionally, MYSH improved O. japonicus productivity, with a 94.75% increase in yield and a 17.64% increase in dry matter ratio. HP revealed that MYSH was the primary factor affecting the yield and dry matter ratio of O. japonicus, with relative importance of 47.75% and 42.28%, respectively. The key mechanism was that MYSH degraded soil paclobutrazol residue, which in turn influenced soil sucrase activity, ultimately impacting the yield of O. japonicus (p < 0.05).

Conclusions
This study demonstrates the dual role of MYSH as both an environmental remediation agent and a crop productivity enhancer for the first time. By reducing paclobutrazol residue and enhancing soil health and crop production, MYSH shows great potential for broader application in sustainable agricultural practices. This research highlights the efficacy of microbial agents in addressing agrochemical contamination and promoting sustainable farming, providing a valuable contribution to the development of eco-friendly agricultural solutions.","Basic reporting
Minor revisions have been requested for the manuscript. It is appropriate to publish after revision. Suggestions and corrections are indicated on the text.

Experimental design
good

Validity of the findings
good

Additional comments
no comment",Generic
Benchmarking of a time-saving and scalable protocol for the extraction of DNA from diverse viromes,"The virome, composed of viruses inhabiting diverse ecosystems, significantly influences microbial community dynamics and host health. The phenol-chloroform DNA extraction protocol for viromes, though effective, is time-intensive and requires the use of multiple toxic chemicals. This study introduces a streamlined, scalable protocol for DNA extraction using a commercially-available kit as an alternative, assessing its performance against the phenol-chloroform method across human fecal, mouse fecal, and soil samples. No significant differences in virome diversity or community composition were seen between methods. Most viral operational taxonomic units (vOTUs) were common to both methods, with only a small percentage unique to either approach. Alpha- and beta-diversity analyses showed no significant impact of the extraction method on virome composition, confirming the kit’s efficacy and versatility on sample types beyond those officially supported by the manufacturer. While the kit approach offers benefits like reduced toxicity and increased throughput, it has limitations such as higher costs and potential issues reliably capturing low-abundance taxa. This protocol provides a viable option for large-scale virome studies, although the phenol-chloroform approach may still be preferable for specific sample types.","Basic reporting
I have thoroughly reviewed the current work and found no notable differences between it and a previously published study that employs a very similar experimental method and kit to investigate the human virome (https://pubmed.ncbi.nlm.nih.gov/35199035/). The research objectives and methodologies in both studies closely align, raising essential questions about the novelty and contribution of the current findings.

I recommend that the authors introduce and actively discuss their work compared to the previously published study. This discussion should include a detailed comparison of the methodologies used, highlighting similarities and identifying areas where they have built upon or diverged from the earlier research. Such a comparative analysis will help to contextualize their findings within the broader landscape of virome research, enhancing the significance of the current study and providing a clearer understanding of its contributions to the field. Addressing these points will strengthen the manuscript and offer readers a more comprehensive view of the subject matter.

Experimental design
no comment

Validity of the findings
no comment

Additional comments
no comment",Authentic
Effects of tropical fruit blends on fermentative and pigmentation aspects of probiotic native cultured goat milk,"Background
Fruits are sources of bioactive compounds such as phenolics that bring health benefits to consumers. The addition of fruit products and microorganisms with probiotic potential in fermented goat milk can facilitate the acquisition of these benefits through diet. In this sense, the objective of this study was to evaluate the effect of incorporating a mixture of ingredients from jaboticaba (Myrciaria cauliflora), jambolana (Syzygium cumini), and mandacaru (Cereus jamacaru) fruits on fermentation parameters (pH, titratable acidity, viability of the native culture Lactiplantibacillus plantarum CNPC003 and the starter culture), associated with pigmentation (phenolic compound content and color) through experimental mixture design.

Methods
A simplex-centroid experimental design was conducted, comprised of seven trials totaling the addition of 30% of the fruit preparations in the final formulation of fermented milk and one control trial (without addition of preparations), with the response being the total phenolic content and the instrumental color parameter a*. Fermentations were carried out with the addition of the native culture Lactiplantibacillus plantarum CNPC003 and the starter culture Streptococcus thermophilus. Subsequently, analyses of pH, titratable acidity, viability of the native and starter cultures, total phenolic compound content, and the instrumental color parameter a* were performed.

Results
The final pH among trials ranged from 4.55 to 4.69, titratable acidity ranged from 0.59 to 0.64, the population of L. plantarum CNPC003 reached levels exceeding 8 log CFU/g, as did the population of Streptococcus thermophilus. The content of phenolic compounds was higher in trials T1, T5, and T7, as well as the color parameter (a*). The use of experimental mixture design contributed to the development of products with high viability of L. plantarum, high content of phenolic compounds, and a characteristic color of the added fruits, bringing benefits to consumer health.","Basic reporting
In the article titled ""Influence of tropical fruit blends on fermentative parameters and pigmentation in fermented goat milk with potentially probiotic native culture"", it is understood that the effects of 3 different regionally valuable fruits on some quality parameters in goat milk fermented with probiotic microorganisms in combination were studied. For this purpose, Total Phenolic Substance, pH and titratable acidity, color values ​​and the viability of L. plantarum and Streptococcus thermophilus were determined in the product. At the same time, fruit mixtures were determined using the experimental design method.
Although the aim of the study was good, the characteristics of the product could not be fully reflected due to the few quality analyzes performed. Examining the study in terms of total monomeric anthocyanin and in vitro antioxidant activity will add more meaning and enrich the study.

Keywords:
The number of keywords is not sufficient to represent the entire text. Therefore, keywords should be kept broad enough to reflect the entire study (5-6 words may be sufficient).

In Introduction:
“Among these foods, we have fruits (Hern·ndez-Carranza et al., ….” The subject “we” is used in this expression. Care should be taken not to use general expressions with personal subjects.

What is the purpose of combining three different fruits used in the study? It would be useful to mention this in the purpose section.

Experimental design
In Materials and Methods:
The structure of the fruits used in the study also contains anthocyanins, which are flavonoids with antioxidant properties. Total Monomeric Anthocyanin and Antioxidant capacity analyzes will also contribute to the study in terms of how stable the fermented goat milk remains in its structure and how it contributes to antioxidant activity.

Validity of the findings
The conclusion section should be further developed regarding the findings obtained and the advantages of the experimental design method applied.

All underlying data have been provided; they are robust, statistically sound, & controlled.",Authentic
"A new metriacanthosaurid theropod dinosaur from the Middle Jurassic of Yunnan Province, China","Metriacanthosaurid theropods represent a basal-branching lineage of tetanurans. Members of this clade are mainly medium to large-sized and lived in Laurasia during the Middle Jurassic to the Early Cretaceous. In this clade, Sinraptor dongi, Sinraptor hepingensis, and Yangchuanosarus shangyouensis from the Late Jurassic are well represented by the nearly complete specimens, but the incompleteness of Middle Jurassic taxa hinders our knowledge of the origin and early evolution of Metriacanthosauridae. This paper describes a new genus and species of metriacanthosaurids, Yuanmouraptor jinshajiangensis gen. et sp. nov, from the Middle Jurassic Zhanghe Formation of Yunnan Province, China. The new taxon is represented by a cranium and the anterior section of the vertebral column including the complete cervical series and the first dorsal vertebra. Yuanmouraptor jinshajiangensis can be diagnosed based on the following autapomorphies: the anterior process of postorbital sheet-shaped and keeping consistent depth; ventral ramus of postorbital bearing a laterally twisted trough running along its lateral surface; ventral surface of axial intercentrum parallel with that of axial centrum; discontinuity of inclination on anterodorsal margin of the third and fourth cervical vertebrae; strongly posteriorly elongated epipophyses of anterior cervical vertebrae; deeply excavated pneumatic foramina on the third cervical vertebra; sheet-shaped and subrectangular neural spines of posterior cervical vertebrae. Phylogenetic analysis recovers Yuanmouraptor as the most basal-branching member within Metriacanthosauridae and provides a new alternative phylogenetic topology of non-coelurosaurian tetanurans.","Basic reporting
The English has improved—nothing else to comment.

Validity of the findings
This has a significant impact due to the need for a good description of Metriacanthosaurids from China. Congrats again to the authors. The description has improved considerably.

Additional comments
After the revision, the manuscript has significantly improved. I think they follow most of the suggestions of both reviewers, and I think the manuscript should be accepted as is. Congratulations to the authors for their study!",Generic
Using transformers and Bi-LSTM with sentence embeddings for prediction of openness human personality trait,"Understanding human personality traits is significant as it helps in decision making related to consumers’ behavior, career counselling, team building and top candidates’ selection for recruitment. Among various traits, openness is essential as it shows both diverse aspects of sensitive nature or intuitive nature. The individuals having a sensing nature tends to be more practical and prefer to focus on concrete information whereas the users having intuitive trait type is characterized by a focus on abstract ideas, creative thinking and future-oriented perspectives. In this research work, we aim to explore diverse natural language processing (NLP) based features and apply state of the art deep learning algorithms for openness trait prediction. Using standard Myers-Briggs Type Indicator (MBTI) dataset, we propose the use of the latest deep features of sentence embeddings which captures contextual semantics of the content to be used with deep learning models. For comparison, we explore textual features of Frequency-Inverse Document (TF-IDF) and parts of speech (POS) tagging with machine learning models and deep features of word2vec and global vectors for word representation (GloVe) with deep learning models. The comprehensive empirical analysis reveals that TF-IDF used with gradient boosting achieves high accuracy of 90% whereas, the deep feature of sentence embeddings when used and with deep model bidirectional long short-term memory (Bi-LSTM) achieves 90.5% accuracy. The best results have been achieved using the latest Transformer-based DistilBERT, which achieves the highest accuracy of 92% outperforming the existing studies in relevant literature.","Basic reporting
Strength: 1) the study employs a wide range of machine learning (ML) and deep learning (DL) models, including shallow ML, ensemble models, and state-of-the-art architectures like Bi-LSTM and BERT. This comprehensive comparison provides a robust foundation for understanding model performance on personality prediction tasks. 2) Feature engineering is well-executed and compared, incorporating a mix of traditional (TF-IDF, POS tagging) and deep learning (word2vec, GloVe, and sentence embeddings) approaches. This breadth provides comprehensive comparison and understanding.

Weakness: 1) the manuscript provides detailed performance metrics for various models but lacks sufficient explanation or interpretation of the experimental results. This gap limits the reader's ability to understand why certain models perform better than others and what insights can be drawn from these results. 2) The manuscript does not clearly articulate the specific research gap it aims to address. While the study emphasizes applying ML and DL models for predicting the openness personality trait, it fails to specify how this work advances the field beyond existing literature or addresses unresolved questions.


1. The second contribution of this work is “Examination of diverse shallow ML models, ensemble models and advanced DL algorithms like LSTM and Bi-LSTM and transformer-based model BERT."", which seems less convincing given that models like LSTM and BERT are no longer considered cutting-edge. Incorporating Large Language Models (LLMs), such as GPT or LLaMA, as alternatives for embedding generation could provide a more contemporary approach.

2. The English language in this paper requires refinement to enhance clarity and ensure it is easily understood by an international audience. The current phrasing poses challenges to comprehension, with issues such as grammatical errors observed in lines 24–28 and unclear wording in lines 37–40 and 73–75 (Each trait represents a spectrum, with individuals adapting in the degree to which user reveals each trait according to the behavior). Thoroughly proofreading your manuscript is essential to ensure clarity, coherence, and the elimination of errors.

3. Another important point is that in Table 10, reference [38] did not conduct experiments on the MBTI dataset. This raises questions about the source of the reported 87% result and its relevance to the comparison.


4. The relationship between the NS dimension and openness within the Big Five personality framework should be further clarified. In particular, the rationale behind Table 1 requires stronger justification. Linking it to foundational works on MBTI and the conceptualization of openness in the Big Five framework would provide more depth and context.

5. More automatic personality recognition studies should be discussed to provide reader for a more comprehensive contexts. The below are some examples:

[1] Ghassemi, Sina, Tianyi Zhang, Ward Van Breda, Antonis Koutsoumpis, Janneke K. Oostrom, Djurre Holtrop, and Reinout E. de Vries. ""Unsupervised multimodal learning for dependency-free personality recognition."" IEEE transactions on affective computing (2023).

[2] Leekha, Maitree, Shahid Nawaz Khan, Harshita Srinivas, Rajiv Ratn Shah, and Jainendra Shukla. ""VyaktitvaNirdharan: Multimodal Assessment of Personality and Trait Emotional Intelligence."" IEEE Transactions on Affective Computing (2024).

[3] Song, Siyang, Shashank Jaiswal, Enrique Sanchez, Georgios Tzimiropoulos, Linlin Shen, and Michel Valstar. ""Self-supervised learning of person-specific facial dynamics for automatic personality recognition."" IEEE Transactions on Affective Computing 14, no. 1 (2021): 178-195.

[4] Song, Siyang, Zilong Shao, Shashank Jaiswal, Linlin Shen, Michel Valstar, and Hatice Gunes. ""Learning person-specific cognition from facial reactions for automatic personality recognition."" IEEE Transactions on Affective Computing 14, no. 4 (2022): 3048-3065.

[5] Junior, Julio CS Jacques, Yağmur Güçlütürk, Marc Pérez, Umut Güçlü, Carlos Andujar, Xavier Baró, Hugo Jair Escalante et al. ""First impressions: A survey on vision-based apparent personality trait analysis."" IEEE Transactions on Affective Computing 13, no. 1 (2019): 75-95.

**PeerJ Staff Note:** It is PeerJ policy that additional references suggested during the peer-review process should only be included if the authors are in agreement that they are relevant and useful.

Experimental design
The experimental design are generall comprehensive and good

Validity of the findings
no comment

Additional comments
no comment",Authentic
Anticancer drug synergy prediction based on CatBoost,"Background
The research of cancer treatments has always been a hot topic in the medical field. Multi-targeted combination drugs have been considered as an ideal option for cancer treatment. Since it is not feasible to use clinical experience or high-throughput screening to identify the complete combinatorial space, methods such as machine learning models offer the possibility to explore the combinatorial space effectively.

Methods
In this work, we proposed a machine learning method based on CatBoost to predict the synergy scores of anticancer drug combinations on cancer cell lines, which utilized oblivious trees and ordered boosting technique to avoid overfitting and bias. The model was trained and tested using the data screened from NCI-ALMANAC dataset. The drugs were characterized with morgan fingerprints, drug target information, monotherapy information, and the cell lines were described with gene expression profiles.

Results
In the stratified 5-fold cross-validation, our method obtained excellent results, where, the receiver operating characteristic area under the curve (ROC AUC) is 0.9217, precision-recall area under the curve (PR AUC) is 0.4651, mean squared error (MSE) is 0.1365, and Pearson correlation coefficient is 0.5335. The performance is significantly better than three other advanced models. Additionally, when using SHapley Additive exPlanations (SHAP) to interpret the biological significance of the prediction results, we found that drug features played more prominent roles than cell line features, and genes associated with cancer development, such as PTK2, CCND1, and GNA11, played an important part in drug synergy prediction. Combining the experimental results, the model proposed in this study has a good prediction effect and can be used as an alternative method for predicting anticancer drug combinations.","Basic reporting
The study presents a novel approach to predicting anticancer drug synergy using the CatBoost algorithm, leveraging features such as morgan fingerprints and gene expression profiles. The model demonstrated superior performance compared to other methods, and the use of SHAP provided insights into the biological relevance of the predictions.

Experimental design
Methods described lack details. How the new Catboost is constructed is not well defined. No details were provided about data preprocessing. The model selection methods were not adequately described.

Validity of the findings
The manuscript lacks an independent datasets for validation.
is it generic or authentic?",Generic
Autonomous vehicle surveillance through fuzzy C-means segmentation and DeepSORT on aerial images,"The high mobility of uncrewed aerial vehicles (UAVs) has led to their usage in various computer vision applications, notably in intelligent traffic surveillance, where it enhances productivity and simplifies the process. Yet, there are still several challenges that must be resolved to automate these systems. One significant challenge is the accurate extraction of vehicle foregrounds in complex traffic scenarios. As a result, this article proposes a novel vehicle detection and tracking system for autonomous vehicle surveillance, which employs Fuzzy C-mean clustering to segment the aerial images. After segmentation, we employed the YOLOv4 deep learning algorithm, which is efficient in detecting small-sized objects in vehicle detection. Furthermore, an ID assignment and recovery algorithm based on Speed-Up Robust Feature (SURF) is used for multi-vehicle tracking across image frames. Vehicles are determined by counting in each image to estimate the traffic density at different time intervals. Finally, these vehicles were tracked using DeepSORT, which combines the Kalman filter with deep learning to produce accurate results. Furthermore, to understand the traffic flow direction, the path trajectories of each tracked vehicle is projected. Our proposed model demonstrates a noteworthy vehicle detection and tracking rate during experimental validation, attaining precision scores of 0.82 and 0.80 over UAVDT and KIT-AIS datasets for vehicle detection. For vehicle tracking, the precision is 0.87 over the UAVDT dataset and 0.83 for the KIT-AIS dataset.","Basic reporting
The paper focuses on developing a vehicle surveillance system using aerial images, employing a combination of fuzzy C-means (FCM) clustering for segmentation, YOLOv4 for vehicle detection, and DeepSORT for multi-vehicle tracking. Overall, the paper is interesting with detailed comments as follows.

The standards are mostly met. One question:
The significance of segmentation in image preprocessing is not sufficiently explained. A clearer explanation is needed to illustrate why segmentation is a critical step in the proposed system.

Experimental design
The approach is validated on two datasets (UAVDT and KIT-AIS) and demonstrates high precision in vehicle detection.

The standards are mostly met. One question:
The two datasets seem not big. What is the scalability for large-scale traffic scenarios of your proposed pipeline.

Validity of the findings
Key innovations include improved segmentation accuracy via FCM, enhanced vehicle tracking with ID assignment using Speed-Up Robust Features (SURF), and precise trajectory mapping.

The standards are mostly met.

Additional comments
1. The significance of segmentation in image preprocessing is not sufficiently explained. A clearer explanation is needed to illustrate why segmentation is a critical step in the proposed system.
2. The two datasets seem not big. What is the scalability for large-scale traffic scenarios of your proposed pipeline.
3. In Figure 7, why UAVDT and KIT-AIS dataset seem totally same. The detections results are also identical.
4. Can a UAV remain stationary at a single intersection for a period of time? Estimating traffic flow or conditions often requires observations over a defined time window to capture dynamic changes accurately",Authentic
From grit to flourishing: physical literacy’s mediating role in enhancing well-being among college students with obesity,"Objective
To investigate whether physical literacy mediates the relationship between grit and well-being among college students with obesity.

Methods
A total of 385 students with obesity were recruited. Participants completed validated questionnaires measuring grit, physical literacy, and well-being. Mediation analyses were performed to estimate indirect effects and generate bias-corrected 95% confidence intervals (CI).

Results
Grit was positively associated with physical literacy, and physical literacy was positively associated with well-being. Physical literacy partially mediated the relationship between grit and well-being, with the indirect effect accounting for 26.32% of the total effect (indirect effect = 0.20, 95% CI [0.09–0.31]). In a parallel mediation model analyzing the subdimensions of physical literacy, the “interaction with the environment” emerged as the strongest mediator (indirect effect = 0.15, 95% CI [0.10–0.21]), accounting for 19.74% of the total effect. The indirect effects through “motivation” and “confidence and physical competence” were also significant but accounted for smaller proportions of the total effect (6.58% and 5.26%, respectively).

Conclusions
These findings serve as an initial step in understanding how physical literacy, particularly the ability to interact with the environment, partially mediates the relationship between grit and well-being among college students with obesity. Future interventional research aiming to enhance physical literacy—especially environmental engagement—is needed to confirm whether it can amplify the positive impact of grit on well-being. A multifaceted approach that fosters both psychological traits and physical competencies may prove beneficial in improving the psychological and physical health of this population.","Basic Reporting
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The abstract succinctly summarizes the study's objectives, methods, and key findings. The introduction provides a comprehensive background, justifying the research gap and hypotheses. The methods section is detailed, allowing for reproducibility, and the results are presented clearly with appropriate figures and tables. The discussion contextualizes the findings within existing literature and acknowledges limitations. However, the paper could benefit from a clearer distinction between the study's novel contributions and prior work, particularly in the discussion section.

Experimental Design
The study employs a cross-sectional design to investigate the mediating role of physical literacy in the relationship between grit and well-being among college students with obesity. The use of validated questionnaires (Grit-S, PPLI-SC, and PERMA-Profiler) strengthens the study's reliability. The inclusion of a parallel mediation analysis to examine the subdimensions of physical literacy (motivation, confidence and physical competence, and interaction with the environment) adds depth to the analysis.

However, the study could be improved by:

Longitudinal Design: The cross-sectional nature limits causal inferences. A longitudinal design would better establish temporal relationships.

Objective Measures: Reliance on self-reported data may introduce biases. Incorporating objective measures of physical activity (e.g., accelerometers) and physical competence (e.g., fitness tests) would enhance validity.

Sample Diversity: The sample is limited to students from Guangzhou, China, which may affect generalizability. Including participants from diverse cultural and geographical backgrounds would strengthen external validity.

Validity of the Findings
The findings are internally valid, supported by robust statistical analyses (mediation models with bootstrapping) and careful control of confounding variables. The significant mediation effects align with existing literature on grit, physical literacy, and well-being.

Key strengths include:

The identification of ""interaction with the environment"" as the strongest mediator, highlighting the importance of social and environmental engagement in well-being.

The use of the PERMA model to comprehensively assess well-being, capturing multiple dimensions (Positive Emotion, Engagement, Relationships, Meaning, and Accomplishment).

Limitations affecting external validity:

The study's focus on obese college students in China may limit generalizability to other populations or cultural contexts.

The reliance on self-reported data may not fully capture the constructs of interest.

Additional Comments
Cultural Context: The discussion of collectivist cultural norms is insightful but could be expanded. For example, how might these norms specifically influence the development of grit and physical literacy in Chinese students?

Practical Implications: The suggestions for campus-based interventions are valuable. However, more specific recommendations (e.g., types of group activities, frequency) would be helpful for practitioners.

Future Research: The call for longitudinal studies is appropriate. Additionally, exploring potential moderators (e.g., social support, mental health) could provide a more nuanced understanding of the mechanisms involved.

Overall Recommendation
The manuscript makes a valuable contribution to understanding the interplay between grit, physical literacy, and well-being in college students with obesity. With minor revisions (e.g., addressing limitations, clarifying practical implications), it would be suitable for publication. The study advances the field by highlighting the mediating role of physical literacy and the importance of environmental interaction.",AI Generated
Unconscious information processing of table tennis athletes in a masked priming paradigm: an event-related potentials (ERP) study,"Background
Unconscious information processing is enhanced among athletes for sports-specific contexts. Whether this enhancement is transferable to general contexts is unknown. This study explored unconscious information processing and brain activity in highly trained table tennis athletes and non-athletes in general contexts.

Methods
Twenty table tennis athletes (six females, mean age = 20.38 ± 1.28, mean ± standard error) and 21 aged-matched college students (eight females, mean age = 19.81 ± 1.29) were recruited for this study. Each participant first performed a masked priming task. In this task, a prime stimulus (arrows pointing left or right) was presented, followed by a visual mask (arrows pointing in both directions) and then a target stimulus, the target stimulus consisted of arrows pointing in the same direction as the prime for congruent stimuli or in the opposite direction for incongruent trials, while the P3 component of the event-related potential was simultaneously recorded in the brain. As a control, participants then performed a prime identification task (the subjective threshold test and the objective threshold test) to determine whether they could consciously detect the priming arrows. Reaction times, error rates, P3 latency and P3 peak amplitude were analyzed to examine the unconscious information processing of table tennis athletes in general contexts.

Results
Participants responded with the direction of the target arrow and were not consciously aware of the priming stimulus. Athletes responded faster in comparison of non-athletes. Athletes and non-athletes responded faster and committed fewer errors in incongruent vs. congruent conditions. In addition, the years of table tennis training were negatively correlated with the magnitude of negative compatibility effect. Both groups displayed longer P3 latencies, a measure of inhibitory control, in the incongruent vs. congruent trials. However, athletes displayed higher P3 peak amplitudes, reflecting larger attention resource input, and longer P3 latencies than non-athletes in central brain sites.

Conclusion
Unconscious information processing among table tennis athletes is not prominent in general contexts, but may be limited to the sports-specific context or more complex cognitive tasks.","Basic Reporting
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The abstract succinctly summarizes the study's objectives, methods, and key findings. The introduction provides a comprehensive background, justifying the research gap and hypotheses. The methods section is detailed, allowing for reproducibility, and the results are presented clearly with appropriate figures and tables. The discussion contextualizes the findings within existing literature and acknowledges limitations. However, the paper could benefit from a clearer distinction between the study's novel contributions and prior work, particularly in the discussion section.

Experimental Design
The study employs a robust 2 (group: athletes vs. non-athletes) × 2 (condition: congruent vs. incongruent) mixed factorial design, which is appropriate for addressing the research questions. The use of both behavioral (reaction times, error rates) and electrophysiological (P3 latency and amplitude) measures strengthens the validity of the findings. The masked priming paradigm is well-executed, and the control tasks (subjective and objective threshold tests) ensure that priming stimuli were processed unconsciously.

However, the study could be improved by:

Sample Size and Diversity: The sample size, while justified by a power analysis, is relatively small, especially for exploring potential sex differences or higher-level athletes. Future studies should aim for larger, more diverse samples.

Fitness Level Control: The lack of fitness level assessment between groups is a notable limitation, as fitness could confound cognitive performance. Including this as a covariate would strengthen the design.

Task Complexity: The simplicity of the arrow-direction task may have led to a ceiling effect, masking potential group differences. Incorporating more complex or sport-specific stimuli could better differentiate athletes' unconscious processing advantages.

Validity of the Findings
The findings are internally valid, supported by appropriate statistical analyses (ANOVAs, correlation tests) and careful control of confounding variables (e.g., prime visibility). The negative compatibility effect (NCE) and ERP results align with existing literature, reinforcing the credibility of the conclusions.

Key strengths include:

The correlation between years of training and NCE magnitude, suggesting training enhances inhibitory control.

The ERP data revealing neurocognitive adaptations in athletes (longer P3 latency, higher amplitude), supporting the role of expertise in resource allocation.

Limitations affecting external validity:

The generalizability of findings is limited to right-handed, college-aged table tennis athletes. Including left-handed athletes and broader age ranges would enhance applicability.

The absence of sport-specific stimuli limits conclusions about domain-specific vs. general processing advantages.

Additional Comments
Error Monitoring: The study did not analyze error rates or post-error slowing, which could provide deeper insights into cognitive control mechanisms. Future work should incorporate these measures.

Theoretical Implications: The discussion could better integrate the results with theories like event coding (Hommel et al., 2001) to explain why athletes showed no NCE advantage in general contexts.

Practical Applications: The implications for training (e.g., designing drills to enhance unconscious processing) could be expanded.

Overall Recommendation
The manuscript makes a valuable contribution to understanding unconscious information processing in athletes. With minor revisions (e.g., addressing limitations, clarifying theoretical links), it would be suitable for publication. The study advances the field by highlighting the context-dependent nature of athletes' cognitive advantages and the neuroplasticity induced by specialized training.",AI Generated
"Time perception and lived experience in personality disorders: differences across types, dimensions and severity","Background
Altered temporal experience lies at the core of various psychiatric conditions, including borderline personality disorder (BPD). Mainstream research in psychopathology tends to explore BPD with scrutiny while neglecting other personality disorders (PD). At the same time, the dimensional approach to PD proposes looking through the disorders’ subtypes and tracing lived experience-based commonalities. This study is the first to explore the temporality of PD by investigating the relationship between symptom severity and lived time and combining objectified measures of time perception with phenomenological interpretation.

Methods
A total of 63 participants of various educational backgrounds, with personality disorders (36.5% male), following ICD-10 coding diagnosed with paranoid (3.2%), borderline (41.3%), narcissistic (33.3%), avoidant (4.8%), dependent (1.6%) and unspecified (15.9%) personality disorder. Levels of personality functioning and intensity of maladaptive trait domains were controlled with Level of Personality Functioning—Brief Scale 2.0 and Personality Inventory for ICD-11, respectively, resulting in the overall sample classification as comprising nine subclinical, 13 mild, 20 moderate, 16 severe, and five extremely severe conditions. Polish Short Version of the Zimbardo Time Perspective Inventory (PS-ZTPI) and Cottle’s Circles Test (CT) were used to assess the temporal experience.

Results
In comparison to healthy individuals, those with PD are more oriented toward past negative (4.01 vs. 2.98) and less toward past positive (2.31 vs. 3.71) and future (3.04 vs. 3.47), as measured with PS-ZTPI; their pre-reflective temporal experience, as measured with CT, is dominated either by the past or the future, while the present remains marginalized. BPD distinctiveness among other PD lies in higher orientation toward hedonistic present and lower orientation toward the future. While the general temporal profile of PD is independent of age and duration of hospitalization, it is related to the severity of the condition. The more severe the impairments in self-functioning, the higher the negative past perspective and pre-reflective past dominance, and the lower the positive and future perspective. The results of this study highlight temporality as an essential aspect of lived experience in PD, being possibly related to disturbed self-experience.","Basic Reporting
The manuscript is well-structured and clearly written, with a logical flow from introduction to discussion. The abstract succinctly summarizes the study's objectives, methods, and key findings. The introduction provides a comprehensive background on temporal experience in personality disorders (PD), particularly borderline personality disorder (BPD), and justifies the need for this study by highlighting gaps in the literature. The methods section is detailed, describing the sample, measures, and statistical analyses. The results are presented clearly, and the discussion effectively interprets the findings in the context of existing literature.

However, the manuscript could benefit from a clearer distinction between the study's primary and secondary objectives. Additionally, while the limitations are thoroughly discussed, some methodological details (e.g., exclusion criteria for the sample) could be expanded for greater transparency.
Experimental Design
The study employs a cross-sectional design to explore temporal experience in PD using both quantitative (PS-ZTPI, CT) and phenomenological approaches. The sample includes 63 participants with various PD diagnoses, primarily BPD and narcissistic PD (NPD), which aligns with the study's aims. The use of validated instruments (e.g., LPFS-BF 2.0, PICD) strengthens the methodological rigor.

Key strengths:

The combination of reflective (ZTPI) and pre-reflective (CT) measures provides a nuanced understanding of temporal experience.

The inclusion of both categorical (ICD-10) and dimensional (ICD-11) approaches to PD diagnosis is a notable contribution.

Areas for improvement:

The sample is skewed toward BPD and NPD, limiting generalizability to other PD types. A more balanced representation of PD categories would enhance the findings.

The lack of a healthy control group is a significant limitation, as comparisons rely on historical data. Future studies should include matched controls to strengthen validity.
Validity of the Findings
The findings are compelling and align with prior theoretical and empirical work on temporal disturbances in PD, particularly BPD. The results demonstrate that individuals with PD exhibit a heightened negative past orientation and diminished future perspective, consistent with phenomenological literature. The distinction between BPD and NPD in terms of present hedonism and past negativity is a valuable contribution.

However, several factors may affect the validity of the findings:

Depressive symptoms: The study did not control for comorbid depressive symptoms, which are known to influence time perspective (e.g., negative past bias). This confounding variable should be addressed in future research.

Instrument limitations: The Polish Short Version of ZTPI omits the present-fatalistic scale, and CT is a projective tool with inherent subjectivity. These limitations are acknowledged but warrant caution in interpretation.

Cross-sectional design: The study cannot establish causality or track changes over time. Longitudinal or interventional designs would provide stronger evidence for the relationship between PD severity and temporal experience.
Additional Comments
Clinical implications: The discussion could further elaborate on how these findings might inform therapeutic interventions (e.g., Time Perspective Therapy) for PD.

Phenomenological integration: While the study bridges quantitative and phenomenological approaches, a more explicit discussion of how the quantitative results map onto specific phenomenological constructs (e.g., ""intra-festum"" temporality) would enrich the interpretation.

Future directions: The manuscript identifies several avenues for future research (e.g., broader PD representation, longitudinal designs). Adding a call for studies using mixed-methods or experimental paradigms (e.g., temporal discounting tasks) could further strengthen this section.",AI Generated
Impact of parental marital status on self-harm in Chinese primary school students: the mediating role of depression and the moderating effect of classmate relationships,"Background
Self-harm is an increasing global public health concern, with a growing prevalence in younger children. This study investigates the associations between parental marital status and self-harm behaviors among primary school students, with a focus on the mediating role of depressive symptoms and the moderating effect of classmate relationships.

Methods
A cross-sectional survey was conducted among 33,285 students (grades 3–6; mean age = 10.36 years) in the Shapingba District of Chongqing, China, from September to December 2020. Self-report measures included the Children’s Depression Inventory (CDI), general demographic data, self-harm behaviors, and parental marital status. Data were analyzed using SPSS 26.0 for descriptive statistics and Mplus 8.1 for structural equation modeling (SEM), assessing the effects of parental marital status on self-harm.

Results
The reporting rates of depression and self-harm in grades 3–6 of primary school are 16.3% and 12.7%, respectively. Parental separation exhibited a more pronounced overall impact on self-harm (β = 0.120) compared to divorce (β = 0.105). Positive classmate relationships mitigated the indirect effect of separation on self-harm mediated by depression, reducing it from 0.098 to 0.072. Additionally, these relationships attenuated the direct effect of divorce on self-harm, decreasing it from 0.088 to 0.043. Depression significantly mediates the relationship between parental separation and children’s self-harm, with direct and indirect effects accounting for 53% (β = 0.057) and 47% (β = 0.063) of the total effect, respectively.

Conclusion
The marital status of parents, especially in cases of separation, has a significant impact on self-harm behaviors among primary school students, with depression acting as a key mediating factor. Supportive classmate relationships can alleviate this effect, highlighting their importance in mental health interventions. These findings offer valuable insights for the development of policies aimed at reducing self-harm and enhancing psychological well-being among children.","Basic Reporting
The paper is well-structured and provides clear reporting on the impact of parental marital status on self-harm in Chinese primary school students, focusing on the mediating role of depression and the moderating effect of classmate relationships. The introduction establishes the significance of the issue effectively, citing relevant literature. The figures and tables are informative and align with the study's objectives. However, some sections could benefit from improved clarity, particularly in explaining the statistical methods and results. The language is professional and adheres to academic standards.

Experimental Design
The study employs a cross-sectional survey with a robust sample size of 33,285 students, ensuring reliability and generalizability within the target population. The methodology is described comprehensively, detailing data collection, ethical considerations, and statistical analysis. The use of structural equation modeling (SEM) is appropriate for assessing mediating and moderating relationships. However, the study's reliance on self-reported data introduces potential bias, which is acknowledged but could be mitigated in future research with mixed methods.

Validity of the Findings
The findings are significant, demonstrating a clear relationship between parental marital status, depression, and self-harm, moderated by classmate relationships. The statistical analyses are rigorous, supporting the validity of the conclusions. Nonetheless, the cross-sectional design limits causal inferences. The discussion contextualizes the results effectively, comparing them with previous studies and offering plausible explanations for observed patterns. Suggestions for future research, including longitudinal studies, are well-justified.

Additional Comments
The study could expand on the implications of its findings for mental health interventions in schools and communities.

The discussion of the moderating role of classmate relationships could delve deeper into practical strategies for fostering supportive peer environments.

Consider rephrasing complex statistical explanations for broader accessibility to readers unfamiliar with SEM.",AI Generated
Effect of hypoxia conditioning on physical fitness in middle-aged and older adults—a systematic review and meta-analysis,"Background
Hypoxic conditioning has emerged as a promising intervention for enhancing physiological adaptations. This systematic review and meta-analysis of randomized controlled trials aims to investigate the efficacy of hypoxic conditioning on physical fitness measures in aging populations.

Methods
The Embase, PubMed, Cochrane Library, and Web of Science were searched from inception to November 2024 (Prospero registration: CRD42023474570). The Cochrane Evaluation Tool and Grading of Recommendations Assessment, Development and Evaluation (GRADE) framework were used for risk of bias assessment and evidence certainty evaluation. Mean differences (MD) and standardized mean differences (SMD) and 95% confidence intervals (CI) were calculated using the Review Manager software. Subgroup analysis was performed to explore possible associations between the study characteristics and the effectiveness of the intervention.

Results
A total of 13 randomized controlled trials (RCTs) with 368 subjects were included in the meta-analysis. High certainty evidence found hypoxic conditioning (HC) significantly improved peak oxygen uptake (VO2peak) (SMD = 0.31, 95% CI [0.01–0.61]; P < 0.05), while very low to moderate certainty evidence shown that hypoxic conditioning (HC) have not induced greater changes on functional outcomes (SMD = −0.21, 95% CI [−0.66–0.24]; P > 0.05), muscle strength (SMD = −0.19, 95% CI [−0.63–0.26]; P > 0.05), maximal power output (SMD = 0.29, 95% CI [−0.17–0.76]; P > 0.05), VO2max (SMD = −0.39, 95% CI [−1.12–1.90]; P > 0.05), and exercise workload (MD = −10.07, 95% CI [−34.95–14.80]; P > 0.05).

Conclusion
This study suggests that hypoxia conditioning has a greater effect on enhancing VO2peak compared to equivalent normoxic training in the middle-aged and older population. More high-quality RCTs are needed in the future to explore the optimal oxygen concentration and exercise intensity during hypoxia conditioning.","Basic Reporting
Clarity and Organization:

The manuscript is well-structured and written clearly, with a focused introduction that effectively outlines the significance of hypoxic conditioning (HC) for middle-aged and older adults.

However, the discussion of contradictory findings (e.g., VO2peak improvements) could benefit from greater emphasis to provide a balanced overview.

Literature Contextualization:

The inclusion of relevant studies is thorough, and the discussion highlights key gaps in existing research.

The authors might strengthen the contextual foundation by further discussing the implications of variability in individual responses to HC, as mentioned briefly in the discussion.

Figures and Tables:

Figures and tables are well-prepared and informative. However, detailed captions should ensure standalone comprehension. For instance, clarify abbreviations used in Table 2 for non-specialist readers.

Supplementary Material:

The integration of supplementary material is excellent. The inclusion of a supplemental search strategy adds transparency to the methodology.

Experimental Design
Study Rationale:

The justification for the meta-analysis is well-articulated, particularly the focus on middle-aged and older adults who face unique physical fitness challenges.

Methodological Rigor:

The methodology adheres to PRISMA guidelines and employs robust data extraction and risk-of-bias evaluation techniques.

A notable strength is the comprehensive subgroup analysis, though the rationale for specific subgroup choices (e.g., <8 weeks vs. ≥8 weeks) could be explained further.

Suggestions for Improvement:

Consider adding a flowchart to depict the experimental processes for clarity.

A more detailed explanation of sensitivity analysis results (e.g., why certain studies impacted pooled outcomes significantly) would enhance the reader’s understanding.

Validity of the Findings
Robustness and Interpretation:

The statistical analysis is solid, with reasonable subgroup considerations and interpretation of heterogeneity. The inclusion of sensitivity analyses adds credibility to the findings.

The evidence supporting the benefit of HC for VO2peak is compelling, but the discussion should address the limitations of relying heavily on a single influential study for this conclusion.

Limitations and Future Research:

The manuscript acknowledges key limitations, such as the small number of included studies and high heterogeneity in some analyses. These points are critical and appropriately highlighted.

Expanding on potential biases due to unclear randomization methods in many included RCTs could add depth to the discussion.

Practical Implications:

The authors make a strong case for HC as a complementary intervention to normoxic training, particularly for improving aerobic capacity. Further discussion on its practical applications for clinicians or exercise physiologists would enhance the impact.

Additional Comments
Conclusion:

The conclusions are well-supported by the data and appropriately cautious regarding limitations.

Emphasizing specific practical guidelines for implementing HC in real-world scenarios would improve the manuscript's utility for practitioners.

Formatting:

Ensure consistent use of abbreviations (e.g., VO2max vs. VO2peak) and provide expanded definitions upon first use.

Overall Recommendation
The manuscript provides valuable insights into the effects of hypoxic conditioning on physical fitness in middle-aged and older adults. With minor revisions, particularly in contextualizing findings, discussing practical applications, and clarifying methodological details, this study would be a strong contribution to the field of geriatric exercise science and sports medicine.",AI Generated
Experimental study on the impact of Speed-Agility-Quickness Training method on the agility performance of collegiate sanda specialty students,"
Research objective
This study investigates the effects of Speed, Agility, and Quickness (SAQ) training on the agility of collegiate sanda athletes at Henan Normal University.

Research methods
The experimental group (EG) (n = 12, Age: 19.58 ± 1.165, height: 176.592 ± 3.181 cm, weight: 71.38 ± 15.84 kg, training years: 2.92 ± 0.793) was trained by the SAQ training method, and the control group (CG) (n = 12, Age: 19.92 ± 1.084, 177.308 ± 2.171 cm, 71.63 ± 16.80 kg, training years: 2.75 ± 0.754) was trained by traditional agility. The data of the CG group and the EG group were compared by repeated measures of analysis of variance (ANOVA) in different periods of the test indexes (pro agility run, Illinois agility run, compass pointer run, cross quadrant jump, 15s repeated ring jump, and punch-kick combination test) in the CG group and EG group.

Research results
Post-experiment, the EG group showed significant differences in all six agility test indicators after adopting SAQ training. There were no significant differences in the Pro Agility Test, the Compass Pointer Test, and the 15s Repeated Ring Jump after the CG group used traditional agility performance training (P > 0.05). There were significant differences in the Illinois agility test (effect size D = 0.626), the cross quadrant jump test (effect size D = 0.558), and the punch-kick combination test (effect size D = 0.519) in the CG group after the experiment (P < 0.001). Similarly, the EG group showed significant differences in the Illinois agility test (effect size D = 0.894), the cross quadrant jump test (effect size D = 0.852), and the punch-kick combination test (effect size D = 0.896).

Research conclusion
SAQ training significantly enhances the agility performance of collegiate sanda specialty students. The effects of improving agility performance through SAQ training are superior to those achieved with traditional agility training methods.","Basic Reporting
The study is well-structured and clearly written, adhering to standard scientific reporting. The introduction provides a comprehensive background on the Speed, Agility, and Quickness (SAQ) training method and its relevance to sanda athletes, supported by appropriate references. The figures and tables are clearly labeled and effectively summarize the findings. The language is clear, though minor grammatical corrections would enhance readability. The research objective is explicitly stated, and the manuscript adheres to PeerJ's standards.

Experimental Design
The research question is well-defined and aligns with the scope of the study. The methodology is robust, involving a clear division between experimental and control groups with detailed training regimens. The 8-week intervention period is reasonable, and the repeated measures design adds depth to the analysis. However, the manual timing used for certain tests introduces potential biases. Utilizing automated systems would enhance precision. While the sample size is acceptable, future studies should aim for a larger, more diverse cohort to improve generalizability.

Validity of the Findings
The results are compelling, demonstrating significant improvements in agility among the experimental group subjected to SAQ training. The statistical analyses are appropriate, and the findings are supported by effect size indicators. The discussion effectively contextualizes the results within the broader literature. Nonetheless, the reliance on a specific demographic (male collegiate sanda athletes) limits the applicability of the findings. Additionally, the use of stopwatches for timing introduces a notable limitation, acknowledged by the authors.

Additional Comments

The manuscript would benefit from a more extensive exploration of the psychological aspects of SAQ training, particularly how it impacts cognitive and decision-making skills in competitive environments.

Including more diverse agility test indicators could provide a comprehensive assessment of SAQ training's effects.

A discussion on how the findings can be translated into practical training regimens for coaches and athletes in other martial arts disciplines would enhance the study's impact.

Ethical considerations are well-addressed, ensuring compliance with research standards.

Overall, the study makes a significant contribution to understanding the impact of SAQ training in enhancing the agility of sanda athletes and sets a foundation for future research in this domain.",AI Generated
The acute effects of simulated hypoxic training at different altitudes on oxidative stress and muscle damage in elite long-distance runners,"Background
Understanding the impact of altitude on muscle damage and oxidative stress is essential for optimizing training and recovery strategies for athletes exposed to high-altitude conditions. Therefore, this study aimed to investigate the effects of acute exercise at different altitudes on oxidative stress and muscle damage.

Methods
A total of twelve elite long-distance runners (mean age: 20.3 ± 1.5 years) from different branches participated in the study. The exercise protocol was the Bruce submaximal treadmill exercise test, which was conducted under three simulated hypoxic conditions (at 1,700 m, 2,450 m, and 3,200 m) and one normoxic condition (sea level). All measurements took place at the same time of the day. After the exercise protocol, 5 ml venous blood samples were taken from the participants, while heart rate and oxygen saturation were monitored at the 3rd, 6th, 9th, and 12th minutes during the exercise.

Results
Significant altitude-dependent variations were observed in oxidative stress markers, with total oxidant status (TOS) (p = 0.017) and malondialdehyde (MDA) (p < 0.001) levels increasing at higher altitudes, while total antioxidant status (TAS) (p < 0.001) exhibited an elevation and oxidative stress index (OSI) (p < 0.001) demonstrated a decline as altitude increased. However, no significant difference was found in creatine kinase (CK, p = 0.059) levels. Additionally, there were significant differences in the oxygen saturation measurement taken at the 3rd (p < 0.001), 6th (p < 0.001), 9th (p < 0.001), and 12th (p < 0.001), minutes following the exercise session. There was no difference in the pulse measurement taken at the 3rd and 12th minutes, but a difference was observed at the 6th and 9th minutes post-exercise (p < 0.01).

Conclusions
In conclusion, the study determined that endurance exercises performed under simulated normobaric hypoxia at different altitudes increased TAS and reduced OSI in elite long-distance runners. The increase in TAS and the reduction in OSI were more pronounced at higher altitudes, particularly at 2,450 m and 3,200 m, compared to sea level. These findings highlight the need for altitude-specific training and recovery strategies to minimize oxidative stress and muscle damage in athletes.","Basic Reporting:
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The title accurately reflects the study's focus, and the abstract provides a concise summary of the objectives, methods, results, and conclusions. The introduction effectively contextualizes the research within existing literature, highlighting the significance of oxidative stress and muscle damage in high-altitude training. The methods section is detailed, describing participant characteristics, study design, and procedures comprehensively. Results are presented clearly with appropriate figures and tables, and the discussion interprets the findings in relation to the hypothesis and prior research. The conclusion succinctly summarizes the key findings and their implications.

Experimental Design:
The study employs a robust quasi-experimental design with a single-group, four-treatment time series approach, which is appropriate for investigating the acute effects of different altitudes. The use of a crossover design and randomization of altitude conditions minimizes bias and enhances internal validity. The inclusion of elite long-distance runners ensures relevance to the research question, and the standardized diet and controlled environmental conditions (temperature, humidity) strengthen the study's reliability. However, the fixed order of altitude conditions (from sea level to the highest altitude) may introduce order effects, which could have been mitigated by counterbalancing. The sample size, though small, was justified by a power analysis, but generalizability may be limited due to the participants' residence at 1,700 m altitude.

Validity of the Findings:
The findings are valid and supported by appropriate statistical analyses, including repeated measures ANOVA and post-hoc tests. The use of well-established biomarkers (TAS, TOS, MDA, OSI, CK) and standardized measurement tools (e.g., Masimo Radical-7 Pulse Oximeter, Polar H10 heart rate monitor) enhances the credibility of the results. The significant altitude-dependent variations in oxidative stress markers and cardiovascular responses align with existing literature, reinforcing the study's conclusions. However, the lack of significant changes in CK levels, while plausibly explained by the nature of the exercise protocol and athlete adaptations, warrants further investigation. The immediate post-exercise measurement of biomarkers may also miss delayed peaks, suggesting a need for additional time-point assessments in future studies.

Additional Comments:

Strengths:

Comprehensive assessment of oxidative stress and muscle damage markers under controlled hypoxic conditions.

Clear documentation of methodological rigor, including randomization and blinding.

Practical implications for athletes and coaches in optimizing high-altitude training and recovery strategies.

Limitations:

The fixed order of altitude conditions may confound results due to potential acclimatization or fatigue effects.

Limited sample size and homogeneity (elite athletes residing at 1,700 m) may restrict generalizability.

Lack of follow-up measurements for biomarkers that may exhibit delayed responses (e.g., CK, MDA).

Suggestions for Improvement:

Randomize or counterbalance the order of altitude conditions to eliminate order effects.

Include additional time-point measurements (e.g., 24–48 hours post-exercise) to capture delayed biomarker responses.

Expand the participant pool to include athletes from sea-level or lower-altitude regions to enhance generalizability.

Overall Assessment:
The study makes a valuable contribution to understanding the acute effects of high-altitude training on oxidative stress and muscle damage in elite athletes. With minor methodological refinements, the findings could have broader applicability and deeper insights into adaptive mechanisms.",AI Generated
Acute effect of three functional fitness training designs with equalized load on inexperienced and experienced athletes,"Background
In the realm of functional fitness training (FFT), three common circuits—as many repetitions or round as possible (AMRAP), for time (FT), and every minute on a minute (EMOM)—are prevalent. We aimed to elucidate the immediate impacts on athletes, considering the experience, when performing three workout modalities with matched training loads.

Methods
Twenty-five healthy men and women, with at least three months of experience in FFT, were allocated into the Inexperienced group (IG) and Experienced group (EG). The cut point for allocating participant in each group was set at 24 months. All of them participated in three workouts (AMRAP, FT and EMOM) with three days of rest. A double comparison was performed between level of experience (IG and EG) and among kinds of training in rating of perceived exertion (RPE), lactate concentration (LAC), countermovement jump (CMJ), heart rate (HR) and heart rate variability (HRV) using ANOVA and post-hoc Bonferroni tests.

Results
Sex was initially analyzed but had no influence, leading to combined group analyses. The workout type significantly impacted performance, with AMRAP showing differences between expertise levels (ES = 0.81, p = .044). RPE varied by workout type (F(2,46) = 11.003; p < .001), with EG reporting FT as the most and EMOM as the least demanding. Lactate levels increased across all workouts, with FT showing the highest and EMOM the lowest levels (ES = 1.05, p < .001). CMJ performance declined post-AMRAP and FT in both groups, but not after EMOM. No expertise-level differences were found in HRmean or HRmax, but HRV changes were influenced by workout type (F(2,46) = 7.381; p < .01) and expertise (F(1,23) = 4.657; p = .034), with significant decreases in HRV after AMRAP and FT for IG.

Conclusion
The study demonstrates that FT produced greater LAC and RPE as compared to an AMRAP, whereas EMOM generated less neuromuscular fatigue and Lac, particularly in EG. These results underscore the importance of individualizing workout selection to expertise level to optimize performance. Future research should explore longitudinal adaptation to different workout types across diverse populations.","Basic Reporting:
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The title accurately reflects the study's focus, and the abstract provides a concise summary of the objectives, methods, results, and conclusions. The introduction effectively contextualizes the research within existing literature, emphasizing the importance of understanding different functional fitness training (FFT) modalities and their acute effects on athletes of varying experience levels. The methods section is detailed, describing participant characteristics, study design, and procedures comprehensively. Results are presented clearly with appropriate figures and tables, and the discussion interprets the findings in relation to the hypothesis and prior research. The conclusion succinctly summarizes the key findings and their practical implications.

Experimental Design:
The study employs a robust observational design with a crossover approach, where participants perform three different FFT modalities (AMRAP, FT, EMOM) with matched training loads. The inclusion of both inexperienced and experienced athletes adds depth to the analysis. Key strengths include:

Standardization: Workouts were homogenized based on AMRAP performance, ensuring consistent volume and intensity across modalities.

Control: Participants followed a standardized warm-up and rest periods (72 hours) between sessions to minimize carryover effects.

Measurement Tools: Validated tools (e.g., Borg CR-10 for RPE, Polar H10 for HR/HRV, Lactate Scout for LAC) were used to assess physiological and perceptual responses.

Limitations:

Order Effects: The fixed sequence of workouts (AMRAP → FT → EMOM) may introduce bias; randomization or counterbalancing would strengthen internal validity.

Sample Size: While the study met power criteria, a larger sample could enhance generalizability, especially for subgroup analyses (e.g., sex-specific responses).

Ecological Validity: The real-world setting (e.g., competition calendar constraints) limited protocol flexibility, such as reversing the workout order.

Validity of the Findings:
The findings are valid and supported by appropriate statistical analyses (two-way ANOVA, Bonferroni post-hoc tests). Key results align with existing literature:

FT elicited higher lactate and RPE than AMRAP, consistent with its time-pressured nature.

EMOM induced less neuromuscular fatigue (minimal CMJ decline) and lower lactate, likely due to structured rest intervals.

Experience-Level Differences: EG reported lower RPE in EMOM, highlighting the role of pacing strategies in advanced athletes.

Concerns:

Homogenization Method: Basing FT and EMOM workloads on AMRAP performance may not fully equate physiological demands, as FT was completed faster than AMRAP. Future studies could homogenize workloads using FT or EMOM as the reference.

HRV Interpretation: The significant drop in HRV post-exercise suggests high autonomic stress, but the clinical relevance of these acute changes warrants further exploration.

Additional Comments:

Strengths:

Novel comparison of AMRAP, FT, and EMOM with matched loads, addressing a gap in FFT literature.

Practical insights for coaches on tailoring workouts to athlete experience (e.g., EMOM for recovery-focused sessions).

Suggestions for Improvement:

Include longitudinal data to assess chronic adaptations to different modalities.

Explore sex-specific responses more thoroughly, given the unequal distribution (15 males, 10 females).

Clarify whether verbal encouragement was standardized across sessions to minimize motivational bias.

Overall Assessment:
The study provides valuable evidence on acute responses to FFT modalities, with clear implications for programming. Methodological refinements (e.g., randomized order, larger sample) could enhance future work.",AI Generated
"The large mammal fossil fauna of the Cradle of Humankind, South Africa: a review","South Africa’s Cradle of Humankind UNESCO World Heritage Site has remained the single richest source of hominin fossils for over ninety years. While its hominin specimens have been the subject of extensive research, the same is not true for its abundant faunal assemblages, despite their value in Plio-Pleistocene palaeoenvironmental reconstructions. Moreover, precise ages and depositional histories have been historically difficult to assess, though advancements in both relative and absolute dating techniques are changing this. This review explores the history of non-hominin large mammal faunal reporting, palaeoenvironmental reconstructions based on these fauna, and dating histories (with a focus on biochronology) at the following eight fossil-bearing sites of the Cradle that have been radiometrically dated with uranium-lead: Bolt’s Farm, Cooper’s Cave, Drimolen, Haasgat, Hoogland, Malapa, Sterkfontein and Swartkrans. Continued efforts to provide more precise and direct ages for sites using a variety of methods indicate that the bulk of Cradle deposits date to between 3 and 1.4 Ma. We find that, across almost all eight sites, there is little discussion or debate surrounding faunal reports, with some sites described by a single publication. Many of the reports are decades old with little review or reanalysis in the years following, emphasising the need for reviews such as this one. Our analysis of the data indicates that faunal-based paleoenvironmental reconstructions across sites commonly show a trend of wooded landscapes giving way to grasslands. We find that these reconstructions are primarily based on faunal abundance data, despite the availability of many other informative analytical techniques. The findings of this review highlight a need for more extensive and robust faunal reporting, as this will aid in understanding the context of these Cradle sites.","Basic Reporting
The manuscript provides a comprehensive review of the large mammal fossil fauna from eight key Plio-Pleistocene sites in the Cradle of Humankind, South Africa. The authors effectively summarize the history of faunal reporting, biochronology, and palaeoenvironmental reconstructions, while highlighting gaps and inconsistencies in existing literature. The structure is clear, with well-defined sections, and the inclusion of tables and figures aids in understanding the data. However, the manuscript could benefit from a more concise presentation of some repetitive points, particularly in the site-specific sections.

Experimental Design
The review is methodologically sound, drawing from a wide range of sources, including peer-reviewed articles, theses, and unpublished data. The focus on sites with uranium-lead (U-Pb) dating ensures a robust chronological framework. The authors’ decision to prioritize bovids due to their ecological significance is justified, though the limited discussion of micromammals and birds is a minor drawback. The inclusion of biochronological and geochronological comparisons strengthens the study’s interdisciplinary approach. A more explicit discussion of the limitations of biochronology (e.g., regional vs. local extinctions) would further enhance the design.

Validity of the Findings
The findings are well-supported by the cited literature, and the authors critically evaluate discrepancies between faunal and absolute dating methods. The conclusion that South Africa may have acted as a refugium is intriguing but would benefit from additional evidence, such as genetic or isotopic data, to strengthen the argument. The palaeoenvironmental reconstructions are plausible but occasionally rely heavily on presence/absence data, which can be biased by taphonomic factors. The authors acknowledge these limitations, but a deeper exploration of alternative proxies (e.g., stable isotopes, dental wear) would bolster the interpretations.

Additional Comments
Clarity and Flow: Some sections, particularly the site histories, are overly detailed and could be streamlined for better readability.

Figures and Tables: The figures are informative, but Figure 4 (proposed dates for deposits) is somewhat cluttered. Simplifying or splitting it into subplots might improve clarity.

Future Directions: The call for more robust faunal reporting is commendable. Suggesting specific techniques (e.g., ecomorphology, isotopic analysis) for future studies would be valuable.

Terminology: Ensure consistent use of terms like ""biotidal"" and ""refugium,"" which are central to the discussion but not always clearly defined.

Overall, this review is a significant contribution to paleoanthropology and paleoecology, providing a much-needed synthesis of the Cradle’s faunal record. With minor revisions, it will be a valuable resource for researchers in the field.",AI Generated
"Phylogenetic relationships of Neogene hamsters (Mammalia, Rodentia, Cricetinae) revealed under Bayesian inference and maximum parsimony","There is an ongoing debate about the internal systematics of today’s group of hamsters (Cricetinae), following new insights that are gained based on molecular data. Regarding the closely related fossil cricetids, however, most studies deal with only a limited number of genera and statements about their possible relationships are rare. In this study, 41 fossil species from the Late Miocene to the Pliocene, belonging to seven extinct cricetine genera, Collimys, Rotundomys, Neocricetodon, Pseudocricetus, Cricetulodon, Apocricetus and Hattomys are analysed in a phylogenetic framework using traditional maximum parsimony and Bayesian inference approaches. Following thorough model testing, a relaxed-clock Bayesian inference analysis is performed under tip-dating to estimate divergence times simultaneously. Furthermore, so-called ‘rogue’ taxa are identified and excluded from the final trees to improve the informative value of the shown relationships. Based on these resulting trees, the fit of the topologies to the stratigraphy is assessed and the ancestral states of the characters are reconstructed under a parsimonious approach and stochastic character mapping. The overall topologies resulting from Bayesian and parsimonious approaches are largely congruent to each other and confirm the monophyly of most of the genera. Additionally, synapomorphies can be identified for each of these genera based on the ancestral state reconstructions. Only Cricetulodon turns out to be paraphyletic, while ‘Cricetulodon’ complicidens is a member of Neocricetodon. Lastly, this work makes a contribution to a debate that went on for decades, as the genus Kowalskia can be confirmed as junior synonym of Neocricetodon.","Basic Reporting:
The manuscript is well-structured and clearly written, with a comprehensive introduction that contextualizes the study within the broader debate on cricetine systematics. The methods are described in sufficient detail, allowing for reproducibility, and the results are presented logically, supported by figures and supplementary materials. The discussion effectively interprets the findings in light of previous research, and the conclusion succinctly summarizes the key contributions of the study. The inclusion of supplemental materials (e.g., morphological matrix, phylogenetic trees) enhances transparency.

Experimental Design:
The study employs a robust phylogenetic framework, combining both maximum parsimony and Bayesian inference approaches, which strengthens the validity of the results. The authors conducted thorough model testing, including relaxed-clock Bayesian tip-dating, to estimate divergence times, and they addressed potential biases by identifying and excluding rogue taxa. The use of ancestral state reconstruction and stratigraphic congruence assessments further enriches the analysis. However, the exclusion of Pleistocene cricetine taxa and the limited number of extant species (only two) may introduce some uncertainty in the placement of extant lineages.

Validity of the Findings:
The findings are well-supported by the data and analyses. The congruence between Bayesian and parsimony topologies lends credibility to the proposed phylogenetic relationships. The identification of synapomorphies for key clades and the resolution of taxonomic uncertainties (e.g., the synonymy of Kowalskia with Neocricetodon) are convincing. The divergence time estimates are plausible and align with the fossil record. However, the low posterior probabilities for some clades (e.g., within Neocricetodon) suggest that certain relationships remain unresolved, which the authors appropriately acknowledge.

Additional Comments:

Strengths:

The study provides the first comprehensive phylogenetic analysis of Neogene cricetine hamsters using morphological data, filling a significant gap in the literature.

The combination of multiple analytical methods (Bayesian tip-dating, maximum parsimony, ancestral state reconstruction) is a major strength.

The discussion is thorough, addressing discrepancies with previous studies and proposing well-justified taxonomic revisions.

Suggestions for Improvement:

Include more extant taxa to better contextualize the fossil lineages and improve the accuracy of tip-dating.

Discuss potential limitations of the morphological dataset (e.g., homoplasy) and how they might affect the results.

Provide a clearer justification for the exclusion of Pleistocene taxa, as their inclusion could refine the placement of extant species.

Minor Points:

The dental terminology figure (Fig. 1) is helpful but could be improved with labels directly on the tooth diagrams for clarity.

Some node support values (e.g., bootstrap percentages) are relatively low; the authors could discuss potential reasons (e.g., incomplete sampling, homoplasy).

Overall Evaluation:
This is a significant contribution to the field of mammalian paleontology, offering new insights into the evolutionary history of cricetine rodents. The methodological rigor and thorough discussion make it a valuable reference for future studies.",AI Generated
