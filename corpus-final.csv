Paper Title,Review Text,Review Type,Abstract
Attention is All You Need,"This paper introduces the Transformer model, revolutionizing NLP tasks with self-attention.",AI-Generated,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
A Survey on Deep Learning,"An essential deep learning overview, combining theory with applications across fields.",AI-Generated,"Deep learning, a branch of machine learning, is a frontier for artificial intelligence, aiming to be closer to its primary goal—artificial intelligence. This paper mainly adopts the summary and the induction methods of deep learning. Firstly, it introduces the global development and the current situation of deep learning. Secondly, it describes the structural principle, the characteristics, and some kinds of classic models of deep learning, such as stacked auto encoder, deep belief network, deep Boltzmann machine, and convolutional neural network. Thirdly, it presents the latest developments and applications of deep learning in many fields such as speech processing, computer vision, natural language processing, and medical applications. Finally, it puts forward the problems and the future research directions of deep learning."
BERT: Pre-training of Deep Bidirectional Transformers,"BERT enhances NLP through bidirectional transformers, setting new benchmarks in understanding.",AI-Generated,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
Generative Adversarial Nets,The introduction of GANs offers an innovative adversarial framework for generating realistic data.,AI-Generated,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
Adam: A Method for Stochastic Optimization,"Adam optimizes gradient descent by blending AdaGrad and RMSProp, widely adopted for its adaptability.",AI-Generated,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
Deep Residual Learning for Image Recognition,"Residual networks address deep learning limitations, achieving new records in image recognition tasks.",AI-Generated,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
Recurrent Neural Networks for Language Modeling,"RNNs show significant improvements in sequential data tasks, particularly for language modeling.",AI-Generated,"The goal of the thesis is to explore the mechanisms and tools that enables
 efficient development of Recurrent Neural Networks, how to train them and what
 they can accomplish in regard to character level language modelling. Specifically
 Gated Recurrence Units and Long Short Term Memory are the focal point of
 the training and language modelling. Choice of data sets, hyper parameters and
 visualization methods, aims to reproduce parts of [KJL15]. More broadly RNN
 as a concept is explored through computational graphs and back propagation.
 Several concrete software tools written in python 3 is developed as part of the
 project, and discussed briefly in the thesis."
"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",The Lottery Ticket Hypothesis opens new avenues for creating efficient neural networks with fewer parameters.,AI-Generated,"Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.
We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the ""lottery ticket hypothesis:"" dense, randomly-initialized, feed-forward networks contain subnetworks (""winning tickets"") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.
We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy."
Neural Networks for Machine Translation,An exploration into neural networks significantly advancing the field of machine translation.,AI-Generated,"Machine translation (MT) is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation (NMT) has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions."
Convolutional Neural Networks for Visual Recognition,"CNNs become the cornerstone of visual recognition, laying the foundation for object detection.",AI-Generated,"This paper presents an empirical analysis of theperformance of popular convolutional neural networks (CNNs) for identifying objects in real time video feeds. The most popular convolution neural networks for object detection and object category classification from images are Alex Nets, GoogLeNet, and ResNet50. A variety of image data sets are available to test the performance of different types of CNN’s. The commonly found benchmark datasets for evaluating the performance of a convolutional neural network are anImageNet dataset, and CIFAR10, CIFAR100, and MNIST image data sets. This study focuses on analyzing the performance of three popular networks: Alex Net, GoogLeNet, and ResNet50. We have taken three most popular data sets ImageNet, CIFAR10, and CIFAR100 for our study, since, testing the performance of a network on a single data set does not reveal its true capability and limitations. It must be noted that videos are not used as a training dataset, they are used as testing datasets. Our analysis shows that GoogLeNet and ResNet50 are able to recognize objects with better precision compared to Alex Net. Moreover, theperformance of trained CNN’s vary substantially across different categories of objects and we, therefore, will discuss the possible reasons for this."
Reinforcement Learning: An Introduction,This paper outlines key advancements in reinforcement learning with substantial real-world applications.,AI-Generated,"The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning."
Self-Supervised Learning of Visual Features,"A major step forward in self-supervised learning, improving visual feature extraction performance.",AI-Generated,"Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning."
EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,"EfficientNet scales convolutional networks, outperforming previous models while reducing computational cost.",AI-Generated,"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.
To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters."
Neural Collaborative Filtering for Recommender Systems,Neural collaborative filtering transforms recommendation systems by improving user-item interaction predictions.,AI-Generated,"In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance."
Fast R-CNN for Object Detection,"Fast R-CNN enhances object detection speeds while maintaining accuracy, a breakthrough for vision tasks.",AI-Generated,"State-of-the-art object detection networks depend on region proposal algorithms
 to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5]
 have reduced the running time of these detection networks, exposing region pro
posal computation as a bottleneck. In this work, we introduce a Region Pro
posal Network (RPN) that shares full-image convolutional features with the de
tection network, thus enabling nearly cost-free region proposals. An RPN is a
 fully-convolutional network that simultaneously predicts object bounds and ob
jectness scores at each position. RPNs are trained end-to-end to generate high
quality region proposals, which are used by Fast R-CNN for detection. With a
 simple alternating optimization, RPN and Fast R-CNN can be trained to share
 convolutional features. For the very deep VGG-16 model [19], our detection
 system has a frame rate of 5fps (including all steps) on a GPU, while achieving
 state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP)
 and 2012 (70.4% mAP) using 300 proposals per image. Code is available at
 https://github.com/ShaoqingRen/faster_rcnn"
YOLO: Real-Time Object Detection,"YOLO presents real-time object detection with outstanding accuracy, impacting many fields.",AI-Generated,"This paper focuses on YOLO-LITE, a real-time object detection model developed to run on portable devices such as a laptop or cellphone lacking a Graphics Processing Unit (GPU). The model was first trained on the PASCAL VOC dataset then on the COCO dataset, achieving a mAP of 33.81% and 12.26% respectively. YOLO-LITE runs at about 21 FPS on a non-GPU computer and 10 FPS after implemented onto a website with only 7 layers and 482 million FLOPS. This speed is 3.8 × faster than the fastest state of art model, SSD MobilenetvI. Based on the original object detection algorithm YOLOV2, YOLO-LITE was designed to create a smaller, faster, and more efficient model increasing the accessibility of real-time object detection to a variety of devices."
Variational Autoencoders for Latent Representation Learning,Variational autoencoders innovate in representing data with probabilistic latent variables.,AI-Generated,"Latent variable models like the Variational Auto-Encoder (VAE) are commonly used to learn representations of images. However, for downstream tasks like semantic classification, the representations learned by VAE are less competitive than other non-latent variable models. This has led to some speculations that latent variable models may be fundamentally unsuitable for representation learning. In this work, we study what properties are required for good representations and how different VAE structure choices could affect the learned properties. We show that by using a decoder that prefers to learn local features, the remaining global features can be well captured by the latent, which significantly improves performance of a downstream classification task. We further apply the proposed model to semi-supervised learning tasks and demonstrate improvements in data efficiency."
Neural Style Transfer for Image Generation,Neural style transfer introduces a creative method for generating artistic imagery via deep learning.,AI-Generated,"Traditional methods of style transfer emphasize primarily the transfer of artistic styles. In recent years, style transfer has expanded beyond the realm of artistic expression to encompass fields such as medicine, industry, and literature. Currently, the style transfer algorithm generating the most attention is the Generative Adversarial Networks (GANs) approach. In this paper, we provide a summary and analysis of the style transfer algorithm based on convolutional neural networks from the perspective of GANs. We review the development process from traditional style transfer algorithms to convolutional neural network-based ones, evaluate their effectiveness and application value, and discuss future research directions and challenges for generative adversarial network-based style transfer algorithms."
U-Net: Convolutional Networks for Biomedical Image Segmentation,"U-Net improves biomedical image segmentation, advancing medical imaging tasks.",AI-Generated,"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL ."
AlphaGo: Mastering the Game of Go with Deep Neural Networks,AlphaGo's neural network architecture revolutionizes the use of AI in complex strategic games like Go.,AI-Generated,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away."
Deep Reinforcement Learning: A Comprehensive Survey,"This paper provides a thorough and detailed survey on deep reinforcement learning, highlighting its foundational techniques and significant advancements across various domains. It presents a well-structured overview of key methods such as Q-learning, policy gradients, and actor-critic methods. Additionally, the paper covers real-world applications in robotics, gaming, and decision-making systems. However, it could further benefit from a more in-depth discussion on the challenges and limitations of deploying these techniques in environments that require high degrees of safety and reliability.",AI-Generated,"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision"
Neural Architecture Search: Building Better Models,"Neural Architecture Search (NAS) is a pioneering concept that automates the design of neural networks, reducing the dependency on human expertise. This paper presents significant advancements in building better models by automating architecture discovery. Though effective, the computational cost of NAS can be prohibitive, which limits its accessibility for smaller institutions.",AI-Generated,"Neural Architecture Search methods are effective but often use complex algorithms to come up with the best architecture. We propose an approach with three basic steps that is conceptually much simpler. First we train N random architectures to generate N (architecture, validation accuracy) pairs and use them to train a regression model that predicts accuracies for architectures. Next, we use this regression model to predict the validation accuracies of a large number of random architectures. Finally, we train the top-K predicted architectures and deploy the model with the best validation result. While this approach seems simple, it is more than 
 as sample efficient as Regularized Evolution on the NASBench-101 benchmark. On ImageNet, it approaches the efficiency of more complex and restrictive approaches based on weight sharing such as ProxylessNAS while being fully (embarrassingly) parallelizable and friendly to hyper-parameter tuning."
EfficientDet: Scalable Object Detection,"EfficientDet presents an innovative scalable object detection architecture that improves the trade-off between speed and accuracy. By utilizing a weighted bi-directional feature pyramid network and compound scaling, this model achieves better performance with fewer parameters compared to its predecessors. Future iterations could focus on enhancing the detection of small and occluded objects.",AI-Generated,"Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at this https URL."
WaveNet: A Generative Model for Raw Audio,"WaveNet introduces a groundbreaking generative model for synthesizing raw audio waveforms. Its autoregressive structure produces highly realistic and human-like audio, making it an essential development for applications in speech synthesis. However, the model’s computational demands pose challenges for real-time applications, and further optimizations are needed.",AI-Generated,"This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition."
Neural Turing Machines: Memory-Augmented Neural Networks,"Neural Turing Machines augment traditional neural networks with external memory, allowing them to solve more complex tasks that involve sequential data and long-term dependencies. This approach represents a significant leap forward for tasks requiring memory, such as algorithm learning. However, the system's complexity and training instability need further research to reach practical applications.",AI-Generated,"Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural Networks, a new class of recurrent neural networks which decouple computation from memory by introducing an external memory unit. NTMs have demonstrated superior performance over Long Short-Term Memory Cells in several sequence learning tasks. A number of open source implementations of NTMs exist but are unstable during training and/or fail to replicate the reported performance of NTMs. This paper presents the details of our successful implementation of a NTM. Our implementation learns to solve three sequential learning tasks from the original NTM paper. We find that the choice of memory contents initialization scheme is crucial in successfully implementing a NTM. Networks with memory contents initialized to small constant values converge on average 2 times faster than the next best memory contents initialization scheme."
OpenAI GPT: Language Models are Unsupervised Multitask Learners,"GPT-2 from OpenAI revolutionizes natural language processing by demonstrating that large-scale unsupervised language models can excel at multiple tasks without task-specific training. The model’s performance across question answering, translation, and summarization sets new benchmarks in NLP. However, ethical concerns around the potential misuse of such powerful language models remain critical.",AI-Generated,"Natural language processing tasks, such as ques
tion answering, machine translation, reading com
prehension, and summarization, are typically
 approached with supervised learning on task
specific datasets. We demonstrate that language
 models begin to learn these tasks without any ex
plicit supervision when trained on a new dataset
 of millions of webpages called WebText. When
 conditioned on a document plus questions, the an
swers generated by the language model reach 55
 F1 on the CoQA dataset- matching or exceeding
 the performance of 3 out of 4 baseline systems
 without using the 127,000+ training examples.
 The capacity of the language model is essential
 to the success of zero-shot task transfer and in
creasing it improves performance in a log-linear
 fashion across tasks. Our largest model, GPT-2,
 is a 1.5B parameter Transformer that achieves
 state of the art results on 7 out of 8 tested lan
guage modeling datasets in a zero-shot setting
 but still underfits WebText. Samples from the
 model reflect these improvements and contain co
herent paragraphs of text. These findings suggest
 a promising path towards building language pro
cessing systems which learn to perform tasks from
 their naturally occurring demonstrations"
Meta-Learning: Learning to Learn,"Meta-learning, or learning to learn, provides models with the ability to generalize across tasks with minimal data, making it an essential strategy for few-shot and zero-shot learning. The paper outlines key algorithms like MAML and REPTILE, which help models adapt quickly to new tasks. Future work could explore more scalable methods for meta-learning in real-world environments.",AI-Generated,"Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field."
Capsule Networks: Beyond Convolutional Neural Networks,"Capsule Networks (CapsNets) introduce a novel architecture that captures spatial hierarchies more effectively than traditional convolutional networks. By representing objects through capsules, the model can better preserve orientation and spatial relationships. While promising, CapsNets currently face challenges with scalability and computational complexity, which require further exploration.",AI-Generated,"Convolutional neural networks use pooling and other downscaling operations to maintain translational invariance for detection of features, but in their architecture they do not explicitly maintain a representation of the locations of the features relative to each other. This means they do not represent two instances of the same object in different orientations the same way, like humans do, and so training them often requires extensive data augmentation and exceedingly deep networks. A team at Google Brain recently made news with an attempt to fix this problem: Capsule Networks. While a normal CNN works with scalar outputs representing feature presence, a CapsNet works with vector outputs representing entity presence. We want to stress test CapsNet in various incremental ways to better understand their performance and expressiveness. In broad terms, the goals of our investigation are: (1) test CapsNets on datasets that are like MNIST but harder in a specific way, and (2) explore the internal embedding space and sources of error for CapsNets."
Neural Ordinary Differential Equations,"Neural Ordinary Differential Equations (Neural ODEs) represent a new class of models that learn continuous-time dynamics for data-driven problems. This paper presents a unique approach to solving complex differential equations using neural networks, which has broad implications for physics, biology, and time-series forecasting. Though theoretically profound, practical applications are still in their infancy.",AI-Generated,"We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models."
Sparse Transformers for Long-Sequence Modeling,"Sparse Transformers offer an innovative solution for handling long-sequence data, a known limitation in traditional transformer architectures. By sparsifying the attention mechanism, this model reduces computational overhead while maintaining performance. It holds great promise for applications like document processing and video understanding, although further optimization could improve efficiency.",AI-Generated,"Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling, but it suffers from quadratic complexity in time and memory usage. Due to the sparsity of the attention matrix, much computation is redundant. Therefore, in this paper, we design an efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling. We provide a brand-new perspective for constructing sparse attention matrix, i.e. making the sparse attention matrix predictable. Two core sub-modules are: (1) A fast Fourier transform based hidden state cross module, which captures and pools L2 semantic combinations in ??(Llog L) time complexity. (2) A sparse attention matrix estimation module, which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module. By reparameterization and gradient truncation, FSAT successfully learned the index of dominant elements. The overall complexity about the sequence length is reduced from ??(L2) to ??(Llog L). Extensive experiments (natural language, vision, and math) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark."
DeepMind's AlphaStar: Mastering Real-Time Strategy Games,"AlphaStar is an impressive demonstration of deep reinforcement learning’s potential in complex, real-time strategy games like StarCraft II. By using multi-agent learning and extensive self-play, AlphaStar has surpassed human expert performance. This paper represents a major milestone in the application of AI to highly dynamic and strategic environments, though its generalization to other tasks remains to be tested.",AI-Generated,"General artificial intelligence requires an intelligent agent to understand or learn any intellectual tasks like a human being. Diverse and complex real-time strategy (RTS) game for artificial intelligence research is a promising stepping stone to achieve the goal. In the last decade, the strongest agents have either simplified the key elements of the game, or used expert rules with human knowledge, or focused on a specific environment. In this paper, we propose a unified learning model that can master various environments in RTS game without human knowledge. We use a multi-agent reinforcement learning algorithm that uses data from agents in a diverse league played on multiple maps to train the deep neural network model. We evaluate our model in microRTS, a simple real-time strategy game. The results show that the agent is competitive against the strong benchmarks in different environments."
Contrastive Learning for Self-Supervised Learning,"Contrastive learning has emerged as a powerful approach in self-supervised learning by focusing on distinguishing similar and dissimilar data points. The technique has dramatically improved the efficiency of representation learning, especially in computer vision. Future research may focus on applying these principles to more complex, multimodal data.",AI-Generated,"Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress."
Federated Learning: Collaborative Machine Learning without Centralized Data,"Federated learning introduces a paradigm shift in collaborative machine learning by enabling decentralized model training. This method allows multiple clients to train a shared model without exposing sensitive data. It holds great promise for privacy-preserving applications in healthcare and finance, though issues like communication overhead and model convergence still need addressing.",AI-Generated,"Federated learning is a promising approach for collaboratively training machine learning models while keeping the training data decentralized. This paper discusses recent advances and open problems in federated learning, focusing on the challenge of communication efficiency and the heterogeneous nature of data, models, and objectives among participating clients. Federated learning allows clients to jointly train a machine learning model without centralizing their private training data. Instead, each client computes an update to the current global model based on their local data, and only this update is communicated to a central server for aggregation. This paradigm is appealing for privacy-sensitive applications, as it avoids the risks associated with centralized data storage. However, federated learning faces several unique challenges compared to traditional centralized machine learning. The heterogeneous nature of the data, models, and objectives across different clients can lead to conflicts and slow convergence of the global model. Furthermore, communication efficiency is critical, as clients typically have unreliable and relatively slow network connections. Recent work has proposed various strategies to improve the communication efficiency of federated learning, such as model compression techniques and selective client participation. Other research has explored ways to handle the heterogeneous nature of federated learning, for example, by allowing clients to train their customized models and share them with the federation."
BigGAN: Large Scale GANs for Synthesizing High-Resolution Images,"BigGAN pushes the boundaries of generative adversarial networks by scaling them to generate high-resolution, photorealistic images. Its ability to synthesize fine details makes it a landmark in the field of image generation. However, its training requires significant computational resources, which may not be feasible for all users.",AI-Generated,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6."
Explainable AI: Interpreting Complex Models,"Explainable AI (XAI) is essential for understanding and trusting decisions made by complex machine learning models. This paper reviews various techniques for making models more interpretable, especially in high-stakes fields like healthcare and finance. While progress has been made, balancing transparency and model performance remains an ongoing challenge.",AI-Generated,"Recent years have seen a tremendous growth in Artificial Intelligence (AI)-based methodological development in a broad range of domains. In this rapidly evolving field, large number of methods are being reported using machine learning (ML) and Deep Learning (DL) models. Majority of these models are inherently complex and lacks explanations of the decision making process causing these models to be termed as 'Black-Box'. One of the major bottlenecks to adopt such models in mission-critical application domains, such as banking, e-commerce, healthcare, and public services and safety, is the difficulty in interpreting them. Due to the rapid proleferation of these AI models, explaining their learning and decision making process are getting harder which require transparency and easy predictability. Aiming to collate the current state-of-the-art in interpreting the black-box models, this study provides a comprehensive analysis of the explainable AI (XAI) models. To reduce false negative and false positive outcomes of these back-box models, finding flaws in them is still difficult and inefficient. In this paper, the development of XAI is reviewed meticulously through careful selection and analysis of the current state-of-the-art of XAI research. It also provides a comprehensive and in-depth evaluation of the XAI frameworks and their efficacy to serve as a starting point of XAI for applied and theoretical researchers. Towards the end, it highlights emerging and critical issues pertaining to XAI research to showcase major, model-specific trends for better explanation, enhanced transparency, and improved prediction accuracy."
NASNet: Learning Scalable Architectures,"NASNet introduces a framework for learning scalable neural network architectures, automating the design process to create more efficient models. By leveraging reinforcement learning, NASNet discovers architectures that outperform hand-designed networks. The approach's success shows great potential, although the high computational costs of architecture search remain a limitation.",AI-Generated,"Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the ""NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or ""cell"") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named ""NASNet architecture"". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."
DALL-E: Generating Images from Text Descriptions,DALL-E extends the capabilities of generative models by generating detailed and contextually relevant images from text descriptions. Its ability to combine elements in novel ways sets a new standard for creativity in AI. Future work might focus on reducing the model's reliance on large-scale training data to improve accessibility.,AI-Generated,"DALL·E is a 12-billion parameter version of GPT-3(opens in a new window) trained to generate images from text descriptions, using a dataset of text–image pairs. We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images."
CLIP: Connecting Text and Images through Pretraining,"CLIP bridges the gap between text and image understanding by learning joint representations through pretraining. Its performance on multimodal tasks like image classification and zero-shot learning is exceptional. While it marks a significant step forward, challenges in handling more abstract or subjective visual concepts remain.",AI-Generated,"Contrastive Language-Image Pre-training (CLIP), consisting of a simplified version of ConVIRT trained from scratch, is an efficient method of image representation learning from natural language supervision. , CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes."
Switch Transformers: Scaling to Trillion Parameter Models,"Switch Transformers represent a novel approach to scaling transformer models, enabling trillion-parameter models while maintaining training efficiency. This architecture introduces a mixture of experts technique to dynamically select which parameters to activate, reducing computational costs. While promising, the model's complexity poses challenges for real-time applications.",AI-Generated,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model."
MoCo: Momentum Contrast for Unsupervised Visual Representation Learning,MoCo advances unsupervised visual representation learning by introducing a momentum contrast framework that leverages dynamic dictionaries for contrastive learning. It has proven effective in tasks such as image classification and object detection. The potential for scaling this approach to multimodal data could be a fruitful avenue for future research.,AI-Generated,"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"BERT represents a major breakthrough in NLP by pre-training deep bidirectional transformers for contextualized language representations. Its bidirectional approach enables it to capture both left and right context, making it particularly effective for tasks like question answering and sentence classification. However, fine-tuning BERT on specific tasks requires substantial computational resources, limiting its accessibility.",AI-Generated,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
GPT-3: Language Models are Few-Shot Learners,"GPT-3 extends the GPT architecture to an unprecedented 175 billion parameters, significantly improving few-shot learning capabilities. Its performance in tasks like translation, summarization, and even code generation demonstrates the potential of large-scale language models. However, its size also introduces challenges related to environmental impact and accessibility, raising ethical concerns about sustainability.",AI-Generated,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
Vision Transformers: An Image is Worth 16x16 Words,"Vision Transformers (ViTs) bring the transformer architecture, traditionally used in NLP, to the domain of image classification. By treating images as sequences of patches, ViTs challenge the dominance of convolutional networks. The results are promising, but the model's data and computational requirements make it less practical for small-scale applications without large datasets.",AI-Generated,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"T5 presents a unified framework for NLP tasks by treating all problems as text-to-text transformations. This approach simplifies transfer learning and has achieved state-of-the-art results across several benchmarks. Its versatility is its strength, but it requires careful tuning for specific applications, especially in low-resource scenarios.",AI-Generated,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code."
Graph Neural Networks: A Comprehensive Review,"Graph Neural Networks (GNNs) extend deep learning to graph-structured data, enabling applications in fields like chemistry, social networks, and recommendation systems. This paper offers a thorough overview of GNN variants, including Graph Convolutional Networks and Graph Attention Networks. However, scalability remains a challenge, especially for large, dynamic graphs.",AI-Generated,"This review provides a comprehensive overview of the state-of-the-art methods of graph-based networks from a deep learning perspective. Graph networks provide a generalized form to exploit non-euclidean space data. A graph can be visualized as an aggregation of nodes and edges without having any order. Data-driven architecture tends to follow a fixed neural network trying to find the pattern in feature space. These strategies have successfully been applied to many applications for euclidean space data. Since graph data in a non-euclidean space does not follow any kind of order, these solutions can be applied to exploit the node relationships. Graph Neural Networks (GNNs) solve this problem by exploiting the relationships among graph data. Recent developments in computational hardware and optimization allow graph networks possible to learn the complex graph relationships. Graph networks are therefore being actively used to solve many problems including protein interface, classification, and learning representations of fingerprints. To encapsulate the importance of graph models, in this paper, we formulate a systematic categorization of GNN models according to their applications from theory to real-life problems and provide a direction of the future scope for the applications of graph models as well as highlight the limitations of existing graph networks."
"DistilBERT: A Smaller, Faster, Cheaper Version of BERT","DistilBERT offers a lighter version of BERT, reducing the model size while maintaining much of the original's performance. Its smaller footprint makes it more accessible for applications with limited computational resources, like mobile devices. However, the trade-off between efficiency and accuracy may limit its effectiveness for complex tasks requiring fine-grained understanding.",AI-Generated,"As Transfer Learning from large-scale pre-trained models becomes more prevalent
 in Natural Language Processing (NLP), operating these large models in on-the
edge and/or under constrained computational training or inference budgets remains
 challenging. In this work, we propose a method to pre-train a smaller general
purpose language representation model, called DistilBERT, which can then be fine
tuned with good performances on a wide range of tasks like its larger counterparts.
 While most prior work investigated the use of distillation for building task-specific
 models, we leverage knowledge distillation during the pre-training phase and show
 that it is possible to reduce the size of a BERT model by 40%, while retaining 97%
 of its language understanding capabilities and being 60% faster. To leverage the
 inductive biases learned by larger models during pre-training, we introduce a triple
 loss combining language modeling, distillation and cosine-distance losses. Our
 smaller, faster and lighter model is cheaper to pre-train and we demonstrate its
 capabilities for on-device computations in a proof-of-concept experiment and a
 comparative on-device study."
StyleGAN: A Generative Adversarial Network for High-Fidelity Image Synthesis,"StyleGAN introduces a new approach to GAN architecture, enabling high-fidelity image synthesis with control over style at different levels of detail. The model has demonstrated exceptional results in generating photorealistic human faces, among other applications. However, the reliance on large-scale datasets for training poses challenges in terms of data diversity and representation.",AI-Generated,"In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality."
Deep Convolutional Generative Adversarial Networks (DCGAN),"DCGAN is one of the most influential GAN architectures, combining convolutional layers with generative modeling. It has been widely adopted for tasks like image generation, super-resolution, and data augmentation. Despite its success, challenges related to training stability and mode collapse still persist, limiting its reliability for complex generation tasks.",AI-Generated,"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
RoBERTa: A Robustly Optimized BERT Pretraining Approach,"RoBERTa builds on BERT by introducing optimizations in pretraining, such as removing the next sentence prediction task and training with larger batches. These modifications lead to improved performance on various NLP benchmarks. However, its computational cost is higher than that of BERT, making it less feasible for smaller organizations without significant resources.",AI-Generated,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,"Transformer-XL addresses the limitation of fixed-length context in transformers, allowing for longer dependency modeling. This is particularly useful for tasks involving long documents, such as text summarization and story generation. While it extends the capabilities of standard transformers, its increased complexity and memory requirements make it more difficult to deploy at scale.",AI-Generated,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
Detecting rumors in social media using emotion based deep learning approach,"The dataset that was used for this research is a plain text database. The experiment did not properly mention the limitations. I would like to see how the results of the new method called SEMTEC produce results with emojis and conversation order. If we don't take the order of the conversation into account, there is a possibility that the results tag is wrong, which can further be treated as biased results. The way we handle biases is not mentioned in the paper. I want to address that and check the results using the SEMTEC method. While the authors discussed labels like sad and fear ( sentiments), there is no clear understanding of how the feelings are mapped to the rumors. Rumors are different, and sentiments are different. Rumors spreading appear more attractive and eye-catching to get more visibility for the given tweet. There is a lack of focus on this area.",Authentic,"Social media, an undeniable facet of the modern era, has become a primary pathway for disseminating information. Unverified and potentially harmful rumors can have detrimental effects on both society and individuals. Owing to the plethora of content generated, it is essential to assess its alignment with factual accuracy and determine its veracity. Previous research has explored various approaches, including feature engineering and deep learning techniques, that leverage propagation theory to identify rumors. In our study, we place significant importance on examining the emotional and sentimental aspects of tweets using deep learning approaches to improve our ability to detect rumors. Leveraging the findings from the previous analysis, we propose a Sentiment and EMotion driven TransformEr Classifier method (SEMTEC). Unlike the existing studies, our method leverages the extraction of emotion and sentiment tags alongside the assimilation of the content-based information from the textual modality, i.e., the main tweet. This meticulous semantic analysis allows us to measure the user’s emotional state, leading to an impressive accuracy rate of 92% for rumor detection on the “PHEME” dataset. The validation is carried out on a novel dataset named “Twitter24”. Furthermore, SEMTEC exceeds standard methods accuracy by around 2% on “Twitter24” dataset."
Gamify4LexAmb: a gamification-based approach to address lexical ambiguity in natural language requirements,"The title of the paper is too long. In general, it is not recommended to exceed 10 words. The paper has several typos. Authors need to proofread the paper to eliminate all of them. There are many irrelevant references. Authors should remove them to keep those that are closely related to the topic of the paper. The authors should first give an overview of their solution before explaining the details.",Generic,"Ambiguity is a common challenge in specifying natural language (NL) requirements. One of the reasons for the occurrence of ambiguity in software requirements is the lack of user involvement in requirements elicitation and inspection phases. Even if they get involved, it is hard for them to understand the context of the system, and ultimately unable to provide requirements correctly due to a lack of interest. Previously, the researchers have worked on ambiguity avoidance, detection, and removal techniques in requirements. Still, less work is reported in the literature to actively engage users in the system to reduce ambiguity at the early stages of requirements engineering. Traditionally, ambiguity is addressed during inspection when requirements are initially specified in the SRS document. Resolving or removing ambiguity during the inspection is time-consuming, costly, and laborious. Also, traditional elicitation techniques have limitations like lack of user involvement, inactive user participation, biases, incomplete requirements, etc. Therefore, in this study, we have designed a framework, Gamification for Lexical Ambiguity (Gamify4LexAmb), for detecting and reducing ambiguity using gamification. Gamify4LexAmb engages users and identifies lexical ambiguity in requirements, which occurs in polysemy words where a single word can have several different meanings. We have also validated Gamify4LexAmb by developing an initial prototype. The results show that Gamify4LexAmb successfully identifies lexical ambiguities in given requirements by engaging users in requirements elicitation. In the next part of our research, an industrial case study will be performed to understand the effects of gamification on real-time data for detecting and reducing NL ambiguity."
Gamify4LexAmb: a gamification-based approach to address lexical ambiguity in natural language requirements,"The paper should justify its choices for a specific ambiguity taxonomy and game elements. Including a comparison with existing methods and showcasing the tool's impact on requirement clarity would strengthen the research. Finally, discussing future plans to validate the tool's effectiveness in real-world scenarios would improve the conclusion.",Generic,"Ambiguity is a common challenge in specifying natural language (NL) requirements. One of the reasons for the occurrence of ambiguity in software requirements is the lack of user involvement in requirements elicitation and inspection phases. Even if they get involved, it is hard for them to understand the context of the system, and ultimately unable to provide requirements correctly due to a lack of interest. Previously, the researchers have worked on ambiguity avoidance, detection, and removal techniques in requirements. Still, less work is reported in the literature to actively engage users in the system to reduce ambiguity at the early stages of requirements engineering. Traditionally, ambiguity is addressed during inspection when requirements are initially specified in the SRS document. Resolving or removing ambiguity during the inspection is time-consuming, costly, and laborious. Also, traditional elicitation techniques have limitations like lack of user involvement, inactive user participation, biases, incomplete requirements, etc. Therefore, in this study, we have designed a framework, Gamification for Lexical Ambiguity (Gamify4LexAmb), for detecting and reducing ambiguity using gamification. Gamify4LexAmb engages users and identifies lexical ambiguity in requirements, which occurs in polysemy words where a single word can have several different meanings. We have also validated Gamify4LexAmb by developing an initial prototype. The results show that Gamify4LexAmb successfully identifies lexical ambiguities in given requirements by engaging users in requirements elicitation. In the next part of our research, an industrial case study will be performed to understand the effects of gamification on real-time data for detecting and reducing NL ambiguity."
Machine learning and natural language processing to assess the emotional impact of influencers’ mental health content on Instagram,"The study is imperative in the realm of exhaustive use of social media platforms. In general, the research provides a detailed overview of the intersection between mental health and social media, covering statistics, prevalence rates, and societal attitudes towards mental health disclosures. In addition, the research employs a number of machine learning algorithms with extensive experiments to discuss the idea of exploring emotional impact. Furthermore, the study also suggests the recent state-of-the-art to explore the idea of impact generated by public figures to further strengthen the importance of this study.",Generic,"Background
This study aims to examine, through artificial intelligence, specifically machine learning, the emotional impact generated by disclosures about mental health on social media. In contrast to previous research, which primarily focused on identifying psychopathologies, our study investigates the emotional response to mental health-related content on Instagram, particularly content created by influencers/celebrities. This platform, especially favored by the youth, is the stage where these influencers exert significant social impact, and where their analysis holds strong relevance. Analyzing mental health with machine learning techniques on Instagram is unprecedented, as all existing research has primarily focused on Twitter.

Methods
This research involves creating a new corpus labelled with responses to mental health posts made by influencers/celebrities on Instagram, categorized by emotions such as love/admiration, anger/contempt/mockery, gratitude, identification/empathy, and sadness. The study is complemented by modelling a set of machine learning algorithms to efficiently detect the emotions arising when faced with these mental health disclosures on Instagram, using the previous corpus.

Results
Results have shown that machine learning algorithms can effectively detect such emotional responses. Traditional techniques, such as Random Forest, showed decent performance with low computational loads (around 50%), while deep learning and Bidirectional Encoder Representation from Transformers (BERT) algorithms achieved very good results. In particular, the BERT models reached accuracy levels between 86–90%, and the deep learning model achieved 72% accuracy. These results are satisfactory, considering that predicting emotions, especially in social networks, is challenging due to factors such as the subjectivity of emotion interpretation, the variability of emotions between individuals, and the interpretation of emotions in different cultures and communities.

Discussion
This cross-cutting research between mental health and artificial intelligence allows us to understand the emotional impact generated by mental health content on social networks, especially content generated by influential celebrities among young people. The application of machine learning allows us to understand the emotional reactions of society to messages related to mental health, which is highly innovative and socially relevant given the importance of the phenomenon in societies. In fact, the proposed algorithms’ high accuracy (86–90%) in social contexts like mental health, where detecting negative emotions is crucial, presents a promising research avenue. Achieving such levels of accuracy is highly valuable due to the significant implications of false positives or false negatives in this social context."
piRNA-disease association prediction based on multi-channel graph variational autoencoder,"The manuscript contains several instances of unclear sentences. For example, Line 268-270. ""In order to verify that the GCN encoder module can effectively obtain piRNA and disease feature representation, Firstly, w a neural network encoder-based Variational Autoencoder (VAE) without GCN was designed."" This sentence is not only grammatically incorrect but also lacks clarity. It should be revised for grammatical accuracy and to clearly convey the intended message.",Authentic,"Piwi-interacting RNA (piRNA) is a type of non-coding small RNA that is highly expressed in mammalian testis. PiRNA has been implicated in various human diseases, but the experimental validation of piRNA-disease associations is costly and time-consuming. In this article, a novel computational method for predicting piRNA-disease associations using a multi-channel graph variational autoencoder (MC-GVAE) is proposed. This method integrates four types of similarity networks for piRNAs and diseases, which are derived from piRNA sequences, disease semantics, piRNA Gaussian Interaction Profile (GIP) kernel, and disease GIP kernel, respectively. These networks are modeled by a graph VAE framework, which can learn low-dimensional and informative feature representations for piRNAs and diseases. Then, a multi-channel method is used to fuse the feature representations from different networks. Finally, a three-layer neural network classifier is applied to predict the potential associations between piRNAs and diseases. The method was evaluated on a benchmark dataset containing 5,002 experimentally validated associations with 4,350 piRNAs and 21 diseases, constructed from the piRDisease v1.0 database. It achieved state-of-the-art performance, with an average AUC value of 0.9310 and an AUPR value of 0.9247 under five-fold cross-validation. This demonstrates the method’s effectiveness and superiority in piRNA-disease association prediction."
piRNA-disease association prediction based on multi-channel graph variational autoencoder,This paper develops an algorithm of prediction using a multi-channel graph variational autoencoder and applying the algorithm on piRNA-disease association. Overall the idea was well motivated and the methodological design is reasonable.,Generic,"Piwi-interacting RNA (piRNA) is a type of non-coding small RNA that is highly expressed in mammalian testis. PiRNA has been implicated in various human diseases, but the experimental validation of piRNA-disease associations is costly and time-consuming. In this article, a novel computational method for predicting piRNA-disease associations using a multi-channel graph variational autoencoder (MC-GVAE) is proposed. This method integrates four types of similarity networks for piRNAs and diseases, which are derived from piRNA sequences, disease semantics, piRNA Gaussian Interaction Profile (GIP) kernel, and disease GIP kernel, respectively. These networks are modeled by a graph VAE framework, which can learn low-dimensional and informative feature representations for piRNAs and diseases. Then, a multi-channel method is used to fuse the feature representations from different networks. Finally, a three-layer neural network classifier is applied to predict the potential associations between piRNAs and diseases. The method was evaluated on a benchmark dataset containing 5,002 experimentally validated associations with 4,350 piRNAs and 21 diseases, constructed from the piRDisease v1.0 database. It achieved state-of-the-art performance, with an average AUC value of 0.9310 and an AUPR value of 0.9247 under five-fold cross-validation. This demonstrates the method’s effectiveness and superiority in piRNA-disease association prediction."
"A comprehensive review of deep learning in EEG-based emotion recognition: classifications, trends, and practical implications","This article is well written and easy to understand. Its structure is appropriate and it contains the sources expected for the topic. As a literature review, it fits in with the scope of the journal and covers an existing gap: The background of the topic as well as the existing literature (including reviews) is named and it is pointed out what the current gap is.",Generic,"Emotion recognition utilizing EEG signals has emerged as a pivotal component of human–computer interaction. In recent years, with the relentless advancement of deep learning techniques, using deep learning for analyzing EEG signals has assumed a prominent role in emotion recognition. Applying deep learning in the context of EEG-based emotion recognition carries profound practical implications. Although many model approaches and some review articles have scrutinized this domain, they have yet to undergo a comprehensive and precise classification and summarization process. The existing classifications are somewhat coarse, with insufficient attention given to the potential applications within this domain. Therefore, this article systematically classifies recent developments in EEG-based emotion recognition, providing researchers with a lucid understanding of this field’s various trajectories and methodologies. Additionally, it elucidates why distinct directions necessitate distinct modeling approaches. In conclusion, this article synthesizes and dissects the practical significance of EEG signals in emotion recognition, emphasizing its promising avenues for future application."
"A comprehensive review of deep learning in EEG-based emotion recognition: classifications, trends, and practical implications","In the author's paper, mention of ""physiological signals such as EEG, facial expressions, and speech signals"" appears in line 39-42. However, it should be noted that facial expressions and voice signals are considered non-biological signals. Lines 48-58 of the paper discuss the ability of convolutional neural networks and recurrent neural networks to enhance EEG features, and the potential impact on future research directions. While these networks are still commonly used, it is important to acknowledge the relevance of transformer networks. Recent literature should be consulted to demonstrate the importance of incorporating this method into future research endeavors. Lastly, the manuscript contains numerous grammatical errors which could be rectified through a thorough proofreading by a fluent English language speaker.",Authentic,"Emotion recognition utilizing EEG signals has emerged as a pivotal component of human–computer interaction. In recent years, with the relentless advancement of deep learning techniques, using deep learning for analyzing EEG signals has assumed a prominent role in emotion recognition. Applying deep learning in the context of EEG-based emotion recognition carries profound practical implications. Although many model approaches and some review articles have scrutinized this domain, they have yet to undergo a comprehensive and precise classification and summarization process. The existing classifications are somewhat coarse, with insufficient attention given to the potential applications within this domain. Therefore, this article systematically classifies recent developments in EEG-based emotion recognition, providing researchers with a lucid understanding of this field’s various trajectories and methodologies. Additionally, it elucidates why distinct directions necessitate distinct modeling approaches. In conclusion, this article synthesizes and dissects the practical significance of EEG signals in emotion recognition, emphasizing its promising avenues for future application."
Learning label smoothing for text classification,Text classification is a traditional area of natural language processing. This paper aims to the problem of overconfidence in it and proposes a learning-based smoothing method in order to make the model more robust. This paper is clear and easy to understand.,Generic,"Training with soft labels instead of hard labels can effectively improve the robustness and generalization of deep learning models. Label smoothing often provides uniformly distributed soft labels during the training process, whereas it does not take the semantic difference of labels into account. This article introduces discrimination-aware label smoothing, an adaptive label smoothing approach that learns appropriate distributions of labels for iterative optimization objectives. In this approach, positive and negative samples are employed to provide experience from both sides, and the performances of regularization and model calibration are improved through an iterative learning method. Experiments on five text classification datasets demonstrate the effectiveness of the proposed method."
"Identification of a novel immune infiltration-related gene signature, MCEMP1, for coronary artery disease","The research presents a comprehensive approach to identifying a novel gene signature for coronary artery disease (CAD), which is a significant contribution to the field. The study's innovation lies in its integrative analysis of immune cell infiltration's role in CAD pathogenesis, alongside the exploration of the MCEMP1 gene's function under ox-LDL treatment in HUVECs. This dual focus provides a nuanced understanding of the complex interplay between genetic factors and cellular responses in the context of CAD. All underlying data appear to have been provided, which enhances the transparency of the study. The data are robust and have been statistically soundly analyzed, indicating a well-controlled experimental design. This is commendable and aligns with best practices in scientific research. The conclusions drawn by the authors are well-stated and are linked to the original research question. They are limited to supporting the results presented in the study, which demonstrates a careful and appropriate interpretation of the findings.",Authentic,"Identification of a novel immune infiltration-related gene signature, MCEMP1, for coronary artery disease"
Multiple imputation methods: a case study of daily gold price,"The article discusses the multiple imputation approach, which is used in a variety of domains, but it does not include several relevant references. While several imputation methods are used, the article does not explain the link between multiple imputation and the handling methods used. The multiple imputation methodology lacks a detailed description, including how the authors used it. Furthermore, the findings are insufficiently summarised to demonstrate how this article contributes to the knowledge gap in the application of multiple imputation in the financial market.",Generic,"Data imputation strategies are necessary to address the prevalent difficulty of missing values in data observation and recording operations. This work utilizes diverse imputation methods to forecast and complete absent values inside a financial time-series dataset, specifically the daily prices of gold. The predictive accuracy of imputed data is assessed in comparison to the original entire dataset to ensure its robustness. The imputation methods are validated using actual closing price data obtained from a daily gold price website. The examined approaches include mean imputation, k-nearest neighbor (KNN), hot deck, random forest, support vector machine (SVM), and spline imputation. Their performance is evaluated based on several metrics, including mean error (ME), mean absolute error (MAE), root mean square error (RMSE), mean percentage error (MPE), and mean absolute percentage error (MAPE). The results indicate that the KNN approach consistently performs better than other methods in terms of all accuracy measures. Nevertheless, the precision of all techniques decreases as the proportion of missing data rises. Therefore, the KNN approach is suggested because to its exceptional performance and dependability in imputation tasks."
Multiple imputation methods: a case study of daily gold price,"Under the discussion and results section, the observations of the experiment has been presented. It would be nice to discuss the significance of the findings, for example you have used ME or mean error for determining . In line 388, authors have mentioned that at high level imputations , KNN had performed relatively best and similarly at lower imputation levels , Spline Imputation performed the best. Now should it be considered satisfactory evidence to conclude that at higher missing frequencies , KNN is the best determinant of missing numbers in a time series model or will there be other factors such as MAR and RMAE that have a more explanation power. Conclusion is very brief and misses a discussion on the relative performance of the 6 methods. What are the assumptions that have been made , going into the observations ? What considerations need to be made to apply this research across other time series missing imputations?",Authentic,"Data imputation strategies are necessary to address the prevalent difficulty of missing values in data observation and recording operations. This work utilizes diverse imputation methods to forecast and complete absent values inside a financial time-series dataset, specifically the daily prices of gold. The predictive accuracy of imputed data is assessed in comparison to the original entire dataset to ensure its robustness. The imputation methods are validated using actual closing price data obtained from a daily gold price website. The examined approaches include mean imputation, k-nearest neighbor (KNN), hot deck, random forest, support vector machine (SVM), and spline imputation. Their performance is evaluated based on several metrics, including mean error (ME), mean absolute error (MAE), root mean square error (RMSE), mean percentage error (MPE), and mean absolute percentage error (MAPE). The results indicate that the KNN approach consistently performs better than other methods in terms of all accuracy measures. Nevertheless, the precision of all techniques decreases as the proportion of missing data rises. Therefore, the KNN approach is suggested because to its exceptional performance and dependability in imputation tasks."
Analyzing the critical steps in deep learning-based stock forecasting: a literature review,"The article presents a well-developed and convincingly supported argument that effectively meets the goals outlined in the Introduction. The logical progression of ideas and the robust evidence provided throughout the review ensure that the initial objectives are thoroughly addressed and accomplished. The Conclusion is particularly strong, as it thoughtfully identifies unresolved questions and gaps in the current research. It also outlines future directions, providing valuable guidance for subsequent studies. This forward-looking perspective enhances the review's contribution to the field by suggesting avenues for further exploration and development.",Generic,"Stock market or individual stock forecasting poses a significant challenge due to the influence of uncertainty and dynamic conditions in financial markets. Traditional methods, such as fundamental and technical analysis, have been limited in coping with uncertainty. In recent years, this has led to a growing interest in using deep learning-based models for stock prediction. However, the accuracy and reliability of these models depend on correctly implementing a series of critical steps. These steps include data collection and analysis, feature extraction and selection, noise elimination, model selection and architecture determination, choice of training-test approach, and performance evaluation. This study systematically examined deep learning-based stock forecasting models in the literature, investigating the effects of these steps on the model’s forecasting performance. This review focused on the studies between 2020–2024, identifying influential studies by conducting a systematic literature search across three different databases. The identified studies regarding seven critical steps essential for creating successful and reliable prediction models were thoroughly examined. The findings from these examinations were summarized in tables, and the gaps in the literature were detailed. This systematic review not only provides a comprehensive understanding of current studies but also serves as a guide for future research."
Detecting rumors in social media using emotion based deep learning approach,"Through grammar check is recommended, provide tabular representation between the existing methods and proposed method in section 2. Authors needs to justify why proposed method needs is efficient for sentiment analysis
2. Why authors use RNNs? There 3rd generation neural networks called Spiking neural networks and suggest authors to look at spiking neural networks and if there are any NLP/Sentiment analysis models exists, how it can be integrated to existing work
3. Recommended authors to add some details about datasets and the label size 2 is very small to come to a conclusion that the proposed work is efficient.",Generic,"Social media, an undeniable facet of the modern era, has become a primary pathway for disseminating information. Unverified and potentially harmful rumors can have detrimental effects on both society and individuals. Owing to the plethora of content generated, it is essential to assess its alignment with factual accuracy and determine its veracity. Previous research has explored various approaches, including feature engineering and deep learning techniques, that leverage propagation theory to identify rumors. In our study, we place significant importance on examining the emotional and sentimental aspects of tweets using deep learning approaches to improve our ability to detect rumors. Leveraging the findings from the previous analysis, we propose a Sentiment and EMotion driven TransformEr Classifier method (SEMTEC). Unlike the existing studies, our method leverages the extraction of emotion and sentiment tags alongside the assimilation of the content-based information from the textual modality, i.e., the main tweet. This meticulous semantic analysis allows us to measure the user’s emotional state, leading to an impressive accuracy rate of 92% for rumor detection on the “PHEME” dataset. The validation is carried out on a novel dataset named “Twitter24”. Furthermore, SEMTEC exceeds standard methods accuracy by around 2% on “Twitter24” dataset."
Decoding Bitcoin: leveraging macro- and micro-factors in time series analysis for price prediction,"Although the paper investigates various window sizes (short-term, mid-term, long-term), the explanation of how these window sizes impact prediction performance is somewhat limited. More detailed analysis and discussion on why certain window sizes perform better than others could enhance understanding. While the paper compares several models (CNN, RNN, LSTM, etc.), it does not provide a comprehensive comparison against baseline models or simpler statistical methods. Including such comparisons would highlight the advantages and limitations of advanced models more clearly??.",Generic,"Predicting Bitcoin prices is crucial because they reflect trends in the overall cryptocurrency market. Owing to the market’s short history and high price volatility, previous research has focused on the factors influencing Bitcoin price fluctuations. Although previous studies used sentiment analysis or diversified input features, this study’s novelty lies in its utilization of data classified into more than five major categories. Moreover, the use of data spanning more than 2,000 days adds novelty to this study. With this extensive dataset, the authors aimed to predict Bitcoin prices across various timeframes using time series analysis. The authors incorporated a broad spectrum of inputs, including technical indicators, sentiment analysis from social media, news sources, and Google Trends. In addition, this study integrated macroeconomic indicators, on-chain Bitcoin transaction details, and traditional financial asset data. The primary objective was to evaluate extensive machine learning and deep learning frameworks for time series prediction, determine optimal window sizes, and enhance Bitcoin price prediction accuracy by leveraging diverse input features. Consequently, employing the bidirectional long short-term memory (Bi-LSTM) yielded significant results even without excluding the COVID-19 outbreak as a black swan outlier. Specifically, using a window size of 3, Bi-LSTM achieved a root mean squared error of 0.01824, mean absolute error of 0.01213, mean absolute percentage error of 2.97%, and an R-squared value of 0.98791. Additionally, to ascertain the importance of input features, gradient importance was examined to identify which variables specifically influenced prediction results. Ablation test was also conducted to validate the effectiveness and validity of input features. The proposed methodology provides a varied examination of the factors influencing price formation, helping investors make informed decisions regarding Bitcoin-related investments, and enabling policymakers to legislate considering these factors."
Increasing the explainability and success in classification: many-objective classification rule mining based on chaos integrated SPEA2,"The authors propose a new methodology for the problem of rule mining task in explainable artificial intelligence studies. The proposed intelligent optimization based method is capable of performing automatic rule extraction by simultaneously optimizing four distinct success metrics (Accuracy, Precision, Recall, and F1). ""Many-objective rule mining"" has been proposed in the literature and this seems important for leading to high-performance explainable and interpretable artificial intelligence studies. The paper is well-structured and well-written.",Generic,"Classification rule mining represents a significant field of machine learning, facilitating
 informed decision-making through the extraction of meaningful rules from complex
 data. Many classification methods cannot simultaneously optimize both explainability
 and different performance metrics at the same time. Metaheuristic optimization-based
 solutions, inspired by natural phenomena, offer a potential paradigm shift in this
 field, enabling the development of interpretable and scalable classifiers. In contrast
 to classical methods, such rule extraction-based solutions are capable of classification
 by taking multiple purposes into consideration simultaneously. To the best of our
 knowledge, although there are limited studies on metaheuristic based classification,
 there is not any method that optimize more than three objectives while increasing
 the explainability and interpretability for classification task. In this study, data sets are
 treated as the search space and metaheuristics as the many-objective rule discovery
 strategy and study proposes a metaheuristic many-objective optimization-based rule
 extraction approach for the first time in the literature. Chaos theory is also integrated
 to the optimization method for performance increment and the proposed chaotic rule
based SPEA2algorithmenables the simultaneous optimization of four different success
 metrics and automatic rule extraction. Another distinctive feature of the proposed
 algorithm is that, in contrast to classical random search methods, it can mitigate issues
 such as correlation and poor uniformity between candidate solutions through the use
 of a chaotic random search mechanism in the exploration and exploitation phases.
 The efficacy of the proposed method is evaluated using three distinct data sets, and
 its performance is demonstrated in comparison with other classical machine learning
 results"
Towards generalisable hate speech detection: a review on obstacles and solutions,"The paper is well written and clearly organized.
In the revised version of the paper the references have been updated wrt the previous version, including missing papers too, and better organizing the discussion of the cited literature.
The aspect of tables should be improved, first of all by applying to them all the same layout and the same character size. According to my knowledge, the paper provides a careful investigation, not always systematic, but quite complete and correct.",Generic,"Hate speech is one type of harmful online content which directly attacks or promotes hate towards a group or an individual member based on their actual or perceived aspects of identity, such as ethnicity, religion, and sexual orientation. With online hate speech on the rise, its automatic detection as a natural language processing task is gaining increasing interest. However, it is only recently that it has been shown that existing models generalise poorly to unseen data. This survey paper attempts to summarise how generalisable existing hate speech detection models are, reason why hate speech models struggle to generalise, sums up existing attempts at addressing the main obstacles, and then proposes directions of future research to improve generalisation in hate speech detection."
A bilingual benchmark for evaluating large language models,This paper compares the skill of chatGPT and GPT4 in answering GAT exam questions in Arabic and English. The idea is to provide a benchmark data that can be used to evaluate the skill of the models with respect to the same task but formulated in different languages. Samples of GAT exam questions in Arabic and English were fed to the two models and performance was reported and discussed. The paper is well-written and easy to follow.,Generic,"This work introduces a new benchmark for the bilingual evaluation of large language models (LLMs) in English and Arabic. While LLMs have transformed various fields, their evaluation in Arabic remains limited. This work addresses this gap by proposing a novel evaluation method for LLMs in both Arabic and English, allowing for a direct comparison between the performance of the two languages. We build a new evaluation dataset based on the General Aptitude Test (GAT), a standardized test widely used for university admissions in the Arab world, that we utilize to measure the linguistic capabilities of LLMs. We conduct several experiments to examine the linguistic capabilities of ChatGPT and quantify how much better it is at English than Arabic. We also examine the effect of changing task descriptions from Arabic to English and vice-versa. In addition to that, we find that fastText can surpass ChatGPT in finding Arabic word analogies. We conclude by showing that GPT-4 Arabic linguistic capabilities are much better than ChatGPT's Arabic capabilities and are close to ChatGPT's English capabilities."
A bilingual benchmark for evaluating large language models,"The paper presents a new benchmark dataset for multilingual large language models in Arabic and English. The dataset was collected using GAT exams verbal section. The paper primarily focuses on detailing the data collection process, with no substantial technical contributions. The paper also has various limitations, which are outlined as follows.

• The introduction lacks a clear structure of introducing the problem and providing summary of effort in the field and gap in the research area.
• In the introduction, the author stated that one of the contributions of the paper is to propose a new way of evaluating LLMs in Arabic and English. It's unclear how the author introduced a novel approach to evaluating LLMS. While data collection is crucial, it doesn't necessarily imply that the author has proposed a new method for assessing LLMS.
• The background section appears extensive and may not be necessary since it mainly describes other proposed approaches (transformer-based methods) not sure how these methods are relevant to this paper.",Authentic,"This work introduces a new benchmark for the bilingual evaluation of large language models (LLMs) in English and Arabic. While LLMs have transformed various fields, their evaluation in Arabic remains limited. This work addresses this gap by proposing a novel evaluation method for LLMs in both Arabic and English, allowing for a direct comparison between the performance of the two languages. We build a new evaluation dataset based on the General Aptitude Test (GAT), a standardized test widely used for university admissions in the Arab world, that we utilize to measure the linguistic capabilities of LLMs. We conduct several experiments to examine the linguistic capabilities of ChatGPT and quantify how much better it is at English than Arabic. We also examine the effect of changing task descriptions from Arabic to English and vice-versa. In addition to that, we find that fastText can surpass ChatGPT in finding Arabic word analogies. We conclude by showing that GPT-4 Arabic linguistic capabilities are much better than ChatGPT's Arabic capabilities and are close to ChatGPT's English capabilities."
A neural machine translation method based on split graph convolutional self-attention encoding,"The authors communicated the methods, research problem, and their findings in unambiguous and professional English. The descriptions of tables and figures are appropriate and will be easily understood by a person with a medium or advanced level of English skills. Their mathematical expressions and the descriptions of each variable in each equation adhere to academic standards. However, there are some places where a little effort is required to understand the notation.

The paper covers enough literature to understand the background of the study, and it is concise in explaining and inferring its objectives. However, more explanation towards various datasets and their backgrounds would have been beneficial. Readers might need to check the recent dataset or have prior knowledge about it.

The article's structure is well-organized, and references to tables and figures are appropriate, providing a seamless reading experience. There are no redundant references, and every reference to the table or diagram in the entire article is appropriate.",Generic,"With the continuous advancement of deep learning technologies, neural machine translation (NMT) has emerged as a powerful tool for enhancing communication efficiency among the members of cross-language collaborative teams. Among the various available approaches, leveraging syntactic dependency relations to achieve enhanced translation performance has become a pivotal research direction. However, current studies often lack in-depth considerations of non-Euclidean spaces when exploring interword correlations and fail to effectively address the model complexity arising from dependency relation encoding. To address these issues, we propose a novel approach based on split graph convolutional self-attention encoding (SGSE), aiming to more comprehensively utilize syntactic dependency relationships while reducing model complexity. Specifically, we initially extract syntactic dependency relations from the source language and construct a syntax dependency graph in a non-Euclidean space. Subsequently, we devise split self-attention networks and syntactic semantic self-attention networks, integrating them into a unified model. Through experiments conducted on multiple standard datasets as well as datasets encompassing scenarios related to team collaboration and enterprise management, the proposed method significantly enhances the translation performance of the utilized model while effectively mitigating model complexity. This approach has the potential to effectively enhance communication among cross-language team members, thereby ameliorating collaborative efficiency."
Validation of system usability scale as a usability metric to evaluate voice user interfaces,"Line 56 - there seems to me to be a typo in ""de-developed""; also the sentence doesn't make much sense, because how can a questionnaire help the user to get to know a system? I think the sentence is confusing, because questionnaires are actually used to evaluate the user's understanding of the interface.

Line 92 - the paragraph says that the word system (original SUS) has been replaced by the word interface, but it's not clear in which situation. Was it for the study in question? I believe so, but it could have been made more explicit.

Line 100 - it's stated that one of the recruitment criteria was that the participants didn't own an Alexa-type device or similar, but I was left wondering what all the criteria used to recruit the participants were - all the recruitment criteria should be stated in the article.

Line 108 - how was the study designed (between or within subject)? I understood that it was within subject but it is not clear if the interaction with the devices was counterbalanced. Did all participants start the tasks with the same device? If so, this may have biased the study, as the participant is influenced by the second device in relation to their interaction with the first device. This study design needs to be better presented in the article.
It is difficult to trust the results presented without knowing the study design in detail.",Authentic,"In recent years, user experience (UX) has gained importance in the field of interactive systems. To ensure its success, interactive systems must be evaluated. As most of the standardized evaluation tools are dedicated to graphical user interfaces (GUIs), the evaluation of voice-based interactive systems or voice user interfaces is still in its infancy. With the help of a well-established evaluation scale, the System Usability Scale (SUS), two prominent, widely accepted voice assistants were evaluated. The evaluation, with SUS, was conducted with 16 participants who performed a set of tasks on Amazon Alexa Echo Dot and Google Nest Mini. We compared the SUS score of Amazon Alexa Echo Dot and Google Nest Mini. Furthermore, we derived the confidence interval for both voice assistants. To enhance understanding for usability practitioners, we analyzed the Adjective Rating Score of both interfaces to comprehend the experience of an interface's usability through words rather than numbers. Additionally, we validated the correlation between the SUS score and the Adjective Rating Score. Finally, a paired sample t-test was conducted to compare the SUS score of Amazon Alexa Echo Dot and Google Nest Mini. This resulted in a huge difference in scores. Hence, in this study, we corroborate the utility of the SUS in voice user interfaces and conclude by encouraging researchers to use SUS as a usability metric to evaluate voice user interfaces."
Diffusion models in text generation: a survey,"The paper conducts a comprehensive survey on the application of diffusion models in text generation. It categorizes text generation into conditional, unconstrained, and multi-mode text generation, providing a detailed exploration of each. The survey also compares diffusion models with autoregressive-based pre-training models (PLMs), discussing their advantages and limitations, and suggesting the integration of PLMs into diffusion models as a promising research direction. Current challenges and potential future research avenues, such as enhancing sampling speed for scalability and exploring multi-modal text generation, are discussed.
This survey is good enough for publication. The presentation is very clear and concise. Literature references is sufficient. The structure of suvery is good, readers can easy to understand the development of diffusion models in text generation. Furthermore, this survey also points some potential future research directions. These research direction is novel and interesting. Overall, I strong recommand to accept this survey with some revisions.",Generic,"Diffusion models are a kind of math-based model that were first applied to image generation. Recently, they have drawn wide interest in natural language generation (NLG), a sub-field of natural language processing (NLP), due to their capability to generate varied and high-quality text outputs. In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction. In addition, considering that autoregressive-based pre-training models (PLMs) have recently dominated text generation, we conduct a detailed comparison between diffusion models and PLMs in multiple dimensions, highlighting their respective advantages and limitations. We believe that integrating PLMs into diffusion is a valuable research avenue. We also discuss current challenges faced by diffusion models in text generation and propose potential future research directions, such as improving sampling speed to address scalability issues and exploring multi-modal text generation. By providing a comprehensive analysis and outlook, this survey will serve as a valuable reference for researchers and practitioners interested in utilizing diffusion models for text generation tasks."
Categorization of tweets for damages: infrastructure and human damage assessment using fine-tuned BERT model,"There are few typos.
Line 82 nn-damage is likely to be a typo, please fix.
Line 178–183, please be consistent with syntax. For example in line 180 replace -> replacement of
Line 236 reached on the conclusion -> reached the conclusion
Line 239: Determining the number of epochs in the fine-tuning stage is a common issue is training the weights of deep learning models.",Authentic,"Identification of infrastructure and human damage assessment tweets is beneficial to disaster management organizations as well as victims during a disaster. Most of the prior works focused on the detection of informative/situational tweets, and infrastructure damage, only one focused on human damage. This study presents a novel approach for detecting damage assessment tweets involving infrastructure and human damages. We investigated the potential of the Bidirectional Encoder Representations from Transformer (BERT) model to learn universal contextualized representations targeting to demonstrate its effectiveness for binary and multi-class classification of disaster damage assessment tweets. The objective is to exploit a pre-trained BERT as a transfer learning mechanism after fine-tuning important hyper-parameters on the CrisisMMD dataset containing seven disasters. The effectiveness of fine-tuned BERT is compared with five benchmarks and nine comparable models by conducting exhaustive experiments. The findings show that the fine-tuned BERT outperformed all benchmarks and comparable models and achieved state-of-the-art performance by demonstrating up to 95.12% macro-f1-score, and 88% macro-f1-score for binary and multi-class classification. Specifically, the improvement in the classification of human damage is promising."
Using Bidirectional Encoder Representations from Transformers (BERT) to predict criminal charges and sentences from Taiwanese court judgments,"This paper’s discussion is not satisfactory. For one thing, the English is weak throughout, with many grammatical errors and usage mistakes (e.g., line 95, “The crime of negligent injury may be constituted.”; line 117, “In the Legal AI domain.” For another, the literature review is highly abstract and does not reflect familiarity with the field. Papers such as Zheng et al. 2021, Henderson et al. 2022, Agarwal, Xu & Grabmair 2022 discuss in great detail the issues with the length of legal decisions for the successful use of large language models for legal tasks. And closer to their own work, the authors do not address the burgeoning literature on legal judgment prediction (see, e.g., Santosh et al. 2023, Santosh et al. 2022). The authors are apparently unaware of this work. Also, several of the figures are quite confusing. For example, Figure 4 does not explain exactly how the chunks were defined from that paragraph. Figure 5 completely excludes the chunking approach. Much more effort is required for the reader to be able to clearly follow this discussion and for it to be appropriately situated in its scholarly context.",Authentic,"People unfamiliar with the law may not know what kind of behavior is considered criminal behavior or the lengths of sentences tied to those behaviors. This study used criminal judgments from the district court in Taiwan to predict the type of crime and sentence length that would be determined. This study pioneers using Taiwanese criminal judgments as a dataset and proposes improvements based on Bidirectional Encoder Representations from Transformers (BERT). This study is divided into two parts: criminal charges prediction and sentence prediction. Injury and public endangerment judgments were used as training data to predict sentences. This study also proposes an effective solution to BERT’s 512-token limit. The results show that using the BERT model to train Taiwanese criminal judgments is feasible. Accuracy reached 98.95% in predicting criminal charges and 72.37% in predicting the sentence in injury trials, and 80.93% in predicting the sentence in public endangerment trials."
Transfer learning-based English translation text classification in a multimedia network environment,"In this paper, the overall quality of the paper is good. The authors provided adequate explanations for the incentive, motivatin, background of the topic, and explained their approach to solve the problem of using transfer learning for translation task. The authors also used adequate tables and figures to illustrate their findings intuitively and visually.",Generic,"In recent years, with the rapid development of the Internet and multimedia technology, English translation text classification has played an important role in various industries. However, English translation remains a complex and difficult problem. Seeking an efficient and accurate English translation method has become an urgent problem to be solved. The study first elucidated the possibility of the development of transfer learning technology in multimedia environments, which was recognized. Then, previous research on this issue, as well as the Bidirectional Encoder Representations from Transformers (BERT) model, the attention mechanism and bidirectional long short-term memory (Att-BILSTM) model, and the transfer learning based cross domain model (TLCM) and their theoretical foundations, were comprehensively explained. Through the application of transfer learning in multimedia network technology, we deconstructed and integrated these methods. A new text classification technology fusion model, the BATCL transfer learning model, has been established. We analyzed its requirements and label classification methods, proposed a data preprocessing method, and completed experiments to analyze different influencing factors. The research results indicate that the classification system obtained from the study has a similar trend to the BERT model at the macro level, and the classification method proposed in this study can surpass the BERT model by up to 28%. The classification accuracy of the Att-BILSTM model improves over time, but it does not exceed the classification accuracy of the method proposed in this study. This study not only helps to improve the accuracy of English translation, but also enhances the efficiency of machine learning algorithms, providing a new approach for solving English translation problems."
Deep learning-based information retrieval with normalized dominant feature subset and weighted vector model,"Clear and unambiguous English used throughout.
Relevant prior literature is appropriately referenced.
The structure of the article conforms to the acceptable format. Figures are relevant to the content of the article.",Generic,"Multimedia data, which includes textual information, is employed in a variety of practical computer vision applications. More than a million new records are added to social media and news sites every day, and the text content they contain has gotten increasingly complex. Finding a meaningful text record in an archive might be challenging for computer vision researchers. Most image searches still employ the tried and true language-based techniques of query text and metadata. Substantial work has been done in the past two decades on content-based text retrieval and analysis that still has limitations. The importance of feature extraction in search engines is often overlooked. Web and product search engines, recommendation systems, and question-answering activities frequently leverage these features. Extracting high-quality machine learning features from large text volumes is a challenge for many open-source software packages. Creating an effective feature set manually is a time-consuming process, but with deep learning, new actual feature demos from training data are analyzed. As a novel feature extraction method, deep learning has made great strides in text mining. Automatically training a deep learning model with the most pertinent text attributes requires massive datasets with millions of variables. In this research, a Normalized Dominant Feature Subset with Weighted Vector Model (NDFS-WVM) is proposed that is used for feature extraction and selection for information retrieval from big data using natural language processing models. The suggested model outperforms the conventional models in terms of text retrieval. The proposed model achieves 98.6% accuracy in information retrieval."
Emotion generation method in online physical education teaching based on data mining of teacher-student interactions,"This study presents a sentiment generation model tailored for teacher-student course interactions. The model encompasses data preprocessing and enhancement techniques, a multimodal sentiment recognition model for dialogue texts, and a topic-extended sentiment dialogue generation model Although the results of this paper have excellent value, there are also some problems, some revisions needed to be revised to make sure that the manuscript can be accepted. The commonly problems are as follows:

1. You highlighted the problem of limited interaction in online education compared to traditional methods. However, it would be beneficial to include a brief statistical reference or source to emphasize the significance of this issue.

2. The abstract should mention what kind of limitations the existing research on emotion generation models faces in more concrete terms. Specify examples of these limitations, if possible.",Generic,"Different from conventional educational paradigms, online education lacks the direct interplay between instructors and learners, particularly in the sphere of virtual physical education. Regrettably, extant research seldom directs its focus toward the intricacies of emotional arousal within the teacher-student course dynamic. The formulation of an emotion generation model exhibits constraints necessitating refinement tailored to distinct educational cohorts, disciplines, and instructional contexts. This study proffers an emotion generation model rooted in data mining of teacher-student course interactions to refine emotional discourse and enhance learning outcomes in the realm of online physical education. This model includes techniques for data preprocessing and augmentation, a multimodal dialogue text emotion recognition model, and a topic-expanding emotional dialogue generation model based on joint decoding. The encoder assimilates the input sentence into a fixed-length vector, culminating in the final state, wherein the vector produced by the context recurrent neural network is conjoined with the preceding word’s vector and employed as the decoder’s input. Leveraging the long-short-term memory neural network facilitates the modeling of emotional fluctuations across multiple rounds of dialogue, thus fulfilling the mandate of emotion prediction. The evaluation of the model against the DailyDialog dataset demonstrates its superiority over the conventional end-to-end model in terms of loss and confusion values. Achieving an accuracy rate of 84.4%, the model substantiates that embedding emotional cues within dialogues augments response generation. The proposed emotion generation model augments emotional discourse and learning efficacy within online physical education, offering fresh avenues for refining and advancing emotion generation models."
Using artificial intelligence to explore sound symbolic expressions of gender in American English,"The reporting is very clear, the paper is extremely well written, reads very easily and is easy to follow.
The literature review is thorough, I am familiar with this field and did not notice anything missing. I felt the amount of literature review was also really good, not too much and not too little, and it was presented very well.
Structure is clear and easy to follow.
The tables were very good. I did notice however that the image resolution in Figure 1 was quite low. I was also wondering, if the journal allows colour, whether using colour to distinguish the male and female vowels might be easier on readers than using the asterisks, though I leave this to the discretion of the authors and editor.
The raw data is supplied in a neatly organized OSF repository. I particularly commend the authors on their R code which is very well commented and easy to follow.",Generic,"This study investigates the extent to which gender can be inferred from the phonemes that make up given names and words in American English. Two extreme gradient boosted algorithms were constructed to classify words according to gender, one using a list of the most common given names (N?1,000) in North America and the other using the Glasgow Norms (N?5,500), a corpus consisting of nouns, verbs, adjectives, and adverbs which have each been assigned a psycholinguistic score of how they are associated with male or female behaviour. Both models report significant findings, but the model constructed using given names achieves a greater accuracy despite being trained on a smaller dataset suggesting that gender is expressed more robustly in given names than in other word classes. Feature importance was examined to determine which features were contributing to the decision-making process. Feature importance scores revealed a general pattern across both models, but also show that not all word classes express gender the same way. Finally, the models were reconstructed and tested on the opposite dataset to determine whether they were useful in classifying opposite samples. The results showed that the models were not as accurate when classifying opposite samples, suggesting that they are more suited to classifying words of the same class."
Enhanced industrial text classification via hyper variational graph-guided global context integration,"The paper is written in clear and professional English overall. Some minor grammatical issues exist but do not impede understanding.

The introduction provides sufficient background on text classification and reviews relevant literature. Main techniques like TF-IDF, SVM, CNNs, RNNs, attention mechanisms, graph neural networks are covered.

The paper follows a standard structure for a machine learning research paper - introduction, related work, methods, experiments, results, conclusion. Figures and tables are appropriately used to illustrate the model architecture, results, etc.

The results presented align with the proposed research goals and hypotheses described. Comparative evaluations demonstrate improved performance over baseline methods. The claims appear properly supported.
In summary, the paper meets basic expectations for quality and rigor based on a preliminary review.",Generic,"Background: Joint local context that is primarily processed by pre-trained models has emerged as a prevailing technique for text classification. Nevertheless, there are relatively few classification applications on small sample of industrial text datasets.

Methods: In this study, an approach of employing global enhanced context representation of the pre-trained model to classify industrial domain text is proposed. To achieve the application of the proposed technique, we extract primary text representations and local context information as embeddings by leveraging the BERT pre-trained model. Moreover, we create a text information entropy matrix through statistical computation, which fuses features to construct the matrix. Subsequently, we adopt BERT embedding and hyper variational graph to guide the updating of the existing text information entropy matrix. This process is subjected to iteration three times. It produces a hypergraph primary text representation that includes global context information. Additionally, we feed the primary BERT text feature representation into capsule networks for purification and expansion as well. Finally, the above two representations are fused to obtain the final text representation and apply it to text classification through feature fusion module.

Results: The effectiveness of this method is validated through experiments on multiple datasets. Specifically, on the CHIP-CTC dataset, it achieves an accuracy of 86.82% and an F1 score of 82.87%. On the CLUEEmotion2020 dataset, the proposed model obtains an accuracy of 61.22% and an F1 score of 51.56%. On the N15News dataset, the accuracy and F1 score are 72.21% and 69.06% respectively. Furthermore, when applied to an industrial patent dataset, the model produced promising results with an accuracy of 91.84% and F1 score of 79.71%. All four datasets are significantly improved by using the proposed model compared to the baselines. The evaluation result of the four dataset indicates that our proposed model effectively solves the classification problem."
"Topic models with elements of neural networks: investigation of stability, coherence, and determining the optimal number of topics","The language of this article is professional, clear and technical.The introduction and background part have elaborated the gap and problem this work aiming to solve. Related literatures or prior works have been reviewed in introduction part. The structure of this article is identical to the requirement of acceptable format. Figures in this article also support the corresponding content. However, in my option, all figures should be pictured by Orgin. All the results in this article and supplementary materials can support the hypotheses or problem definition in this article. Symbol definitions are clear and relevant.",Generic,"Topic modeling is a widely used instrument for the analysis of large text collections. In the last few years, neural topic models and models with word embeddings have been proposed to increase the quality of topic solutions. However, these models were not extensively tested in terms of stability and interpretability. Moreover, the question of selecting the number of topics (a model parameter) remains a challenging task. We aim to partially fill this gap by testing four well-known and available to a wide range of users topic models such as the embedded topic model (ETM), Gaussian Softmax distribution model (GSM), Wasserstein autoencoders with Dirichlet prior (W-LDA), and Wasserstein autoencoders with Gaussian Mixture prior (WTM-GMM). We demonstrate that W-LDA, WTM-GMM, and GSM possess poor stability that complicates their application in practice. ETM model with additionally trained embeddings demonstrates high coherence and rather good stability for large datasets, but the question of the number of topics remains unsolved for this model. We also propose a new topic model based on granulated sampling with word embeddings (GLDAW), demonstrating the highest stability and good coherence compared to other considered models. Moreover, the optimal number of topics in a dataset can be determined for this model."
STTA: enhanced text classification via selective test-time augmentation,"Overall Evaluation: Accept with Minor Revisions

The manuscript presents Selective Test-Time Augmentation (STTA), a novel approach aiming to enhance text classification by carefully selecting transformed samples for aggregation based on confidence and similarity scores. The study is well-structured, providing a comprehensive introduction to Test-Time Augmentation (TTA) and its challenges in Natural Language Processing (NLP), as well as a detailed description of the proposed method and extensive experiments to validate its effectiveness.",Authentic,"Test-time augmentation (TTA) is a well-established technique that involves aggregating transformed examples of test inputs during the inference stage. The goal is to enhance model performance and reduce the uncertainty of predictions. Despite its advantages of not requiring additional training or hyperparameter tuning, and being applicable to any existing model, TTA is still in its early stages in the field of NLP. This is partly due to the difficulty of discerning the contribution of different transformed samples, which can negatively impact predictions. In order to address these issues, we propose Selective Test-Time Augmentation, called STTA, which aims to select the most beneficial transformed samples for aggregation by identifying reliable samples. Furthermore, we analyze and empirically verify why TTA is sensitive to some text data augmentation methods and reveal why some data augmentation methods lead to erroneous predictions. Through extensive experiments, we demonstrate that STTA is a simple and effective method that can produce promising results in various text classification tasks."
Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks,"1. The paper contains several sentences that are not clear or lack coherence.
2. Errors in grammar and sentence structure.
3. References are not links within the text.
4. Size of the Figures are inconsistently large
5. Mistakes in Numbering under the heading EXPERIMENT
6. In abstract ""Ablation studies and model experiments have demonstrated the effectiveness of our model."" Write clear results. This doesn't show anything.",Generic,"The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes. Last but not least, the model defines nodes with different levels of granularity—ranging from documents and topics to sentences—and these various nodes necessitate different propagation widths and depths for capturing intricate relationships in the information being disseminated. Adaptive measures are taken to learn the importance and correlation between nodes of different granularities in terms of both width and depth. Experimental evidence from two benchmark datasets highlights the superior performance of the proposed model, as assessed by ROUGE metrics, in comparison to existing approaches, even in the absence of pre-trained language models. Additionally, an ablation study confirms the positive impact of each individual module on the model's ROUGE scores."
Digital marketing program design based on abnormal consumer behavior data classification and improved homomorphic encryption algorithm,"The paper provides references and sufficient domain background/context. But there are still some places to improve. This paper aims to explore the design of a digital marketing scheme that relies on consumer data and homomorphic encryption. The approach involves using GridSearch to align and store the leaf nodes obtained after training the CatBoost model. These leaf node data are then used as input for the Radial Basis Function (RBF) layer, where the mapping of leaf nodes to the hidden layer space occurs. This process results in the classification of user online consumption data in the output layer. Moreover, a refinement is introduced to the traditional homomorphic encryption algorithm to enhance privacy preservation during consumption data processing. This enhancement expands the scope of homomorphic encryption to include rational numbers. The Chinese remainder theorem is integrated to decrypt consumption information. Empirical results demonstrate the exceptional generalization performance of the fusion model, showcasing an AUC (Area Under the Curve) value of 0.66, a classification accuracy of 98.56% for online consumption data, and an F1-Score of 98.41. Should the authors explain the reason for using these four models (LR, LightGBM, CatBoost, CatBoost-RBF)? The figure 1 and 2 are a bit blurry; the figures quality should be improved. Conclusions should be rewritten based on the results and discussion.",Generic,"This article endeavors to delve into the conceptualization of a digital marketing framework grounded in consumer data and homomorphic encryption. The methodology entails employing GridSearch to harmonize and store the leaf nodes acquired post-training of the CatBoost model. These leaf node data subsequently serve as inputs for the radial basis function (RBF) layer, facilitating the mapping of leaf nodes into the hidden layer space. This sequential process culminates in the classification of user online consumption data within the output layer. Furthermore, an enhancement is introduced to the conventional homomorphic encryption algorithm, bolstering privacy preservation throughout the processing of consumption data. This augmentation broadens the applicability of homomorphic encryption to encompass rational numbers. The integration of the Chinese Remainder Theorem is instrumental in the decryption of consumption-related information. Empirical findings unveil the exceptional generalization performance of the amalgamated model, exemplifying an AUC (area under the curve) value of 0.66, a classification accuracy of 98.56% for online consumption data, and an F1-score of 98.41. The enhanced homomorphic encryption algorithm boasts attributes of stability, security, and efficiency, thus fortifying our proposed solution in facilitating companies' access to precise, real-time market insights. Consequently, this aids in the optimization of digital marketing strategies and enables pinpoint positioning within the target market."
PluDG: enhancing task-oriented dialogue system with knowledge graph plug-in module,"In the manuscript, the authors have utilized language that is easy to understand and relatively specific.
The main technical content can be understood easily.
A good visual support is offered by the figures included in the manuscript. The resolution of the figures is high enough that readers will be able to make out the essential particulars. The descriptions and labels of each figure are written in an understandable and comprehensive manner.
The authors have provided nearly all of the results that are relevant to their hypothesis in their presentation. This indicates that the subject has been discussed in great detail. Nevertheless, the quality of the manuscript will be elevated if the authors add additional specifics, as will be discussed in the Additional comments that follow.",Generic,"Task-oriented dialogue systems continue to face significant challenges as they require not only an understanding of dialogue history but also domain-specific knowledge. However, knowledge is often dynamic, making it difficult to effectively integrate into the learning process. Existing large language model approaches primarily treat knowledge bases as textual resources, neglecting to capture the underlying relationships between facts within the knowledge base. To address this limitation, we propose a novel dialogue system called PluDG. We regard the knowledge as a knowledge graph and propose a knowledge extraction plug-in, Kg-Plug, to capture the features of the graph and generate prompt entities to assist the system's dialogue generation. Besides, we propose Unified Memory Integration, a module that enhances the comprehension of the sentence's internal structure and optimizes the knowledge base's encoding location. We conduct experiments on three public datasets and compare PluDG with several state-of-the-art dialogue models. The experimental results indicate that PluDG achieves significant improvements in both accuracy and diversity, outperforming the current state-of-the-art dialogue system models and achieving state-of-the-art performance."
Vehicle target detection method based on improved YOLO V3 network model,"In order to strengthen the detection of small vehicle targets, this paper improves the YOLO V3 network model, introduces the orientation angle, and improves the training efficiency of the network model by presetting the size of the candidate frame. It has certain theoretical significance. But there are following problems:
1 Figure 2 shows the convergence curve of the network loss function of the improved model. Under the same conditions, whether the convergence of other models is fast or slow, it is recommended to add the convergence curve of the comparison model in Figure 2.
2 In formulas 4 and 5, what is the range of the letter i, and the letter g should indicate the corresponding meaning.
3 The aerial photography data set VEDAI contains 9 types of targets, which 9 types?
4 Individual sentences need to be revised, and it is recommended to proofread the grammar of the full text.
5 Some of the cited documents need to be summarized in one step, and the problems existing in them should be pointed out, not just an overview of the content of the documents.",Authentic,"For the problem of insufficient small target detection ability of the existing network model, a vehicle target detection method based on the improved YOLO V3 network model is proposed in the article. The improvement of the algorithm model can effectively improve the detection ability of small target vehicles in aerial photography. The optimization and adjustment of the anchor box and the improvement of the network residual module have improved the small target detection effect of the algorithm. Furthermore, the introduction of the rectangular prediction frame with orientation angles into the model of this article can improve the vehicle positioning efficiency of the algorithm, greatly reduce the problem of wrong detection and missed detection of vehicles in the model, and provide ideas for solving related problems. Experiments show that the accuracy rate of the improved algorithm model is 89.3%. Compared to the YOLO V3 algorithm, it is improved by 15.9%. The recall rate is improved by 16%, and the F1 value is also improved by 15.9%, which greatly increased the detection efficiency of aerial vehicles."
BiMGCL: rumor detection via bi-directional multi-level graph contrastive learning,"Literature references are outdated, irrelevant, and insufficient. Must be improved.
Raw data is not shared.",Generic,"The rapid development of large language models has significantly reduced the cost of producing rumors, which brings a tremendous challenge to the authenticity of content on social media. Therefore, it has become crucially important to identify and detect rumors. Existing deep learning methods usually require a large amount of labeled data, which leads to poor robustness in dealing with different types of rumor events. In addition, they neglect to fully utilize the structural information of rumors, resulting in a need to improve their identification and detection performance. In this article, we propose a new rumor detection framework based on bi-directional multi-level graph contrastive learning, BiMGCL, which models each rumor propagation structure as bi-directional graphs and performs self-supervised contrastive learning based on node-level and graph-level instances. In particular, BiMGCL models the structure of each rumor event with fine-grained bidirectional graphs that effectively consider the bi-directional structural characteristics of rumor propagation and dispersion. Moreover, BiMGCL designs three types of interpretable bi-directional graph data augmentation strategies and adopts both node-level and graph-level contrastive learning to capture the propagation characteristics of rumor events. Experimental results on real datasets demonstrate that our proposed BiMGCL achieves superior detection performance compared against the state-of-the-art rumor detection methods."
Comparing neural sentence encoders for topic segmentation across domains: not your typical text similarity task,"I have read the paper entitled (Comparing neural sentence encoders for topic segmentation across domains: not your typical text similarity task). The manuscript is centered on an interesting topic. Organization of the paper is good and the proposed method is quite novel. Nevertheless, the manuscript does not link well with recent literature on Topic Segmentation appeared in relevant top-tier journals.

Though I found this paper well-written and there are only few grammatical errors which need to be corrected.
- Most sentences did not understand or they are grammatically incorrect.
- Proofreading is necessary for the manuscript for unabiguous sentences.

Equation numbers and its references not found. Table 2 placing is incorrect. Table 5 Placing is incorrect. Increse the resolution for Figure 2. In Figure 3 word - Non Boundary is half visible.",Authentic,"Neural sentence encoders (NSE) are effective in many NLP tasks, including topic segmentation. However, no systematic comparison of their performance in topic segmentation has been performed. Here, we present such a comparison, using supervised and unsupervised segmentation models based on NSEs. We first compare results with baselines, showing that the use of NSEs does often provide improvements, except for specific domains such as news shows. We then compare over three different datasets a range of existing NSEs and a new NSE based on ad hoc pre-training strategy. We show that existing literature documenting general performance gains of NSEs does not always conform to the results obtained by the same NSEs in topic segmentation. If Transformers-based encoders do improve over previous approaches, fine-tuning in sentence similarity tasks or even on the same topic segmentation task we aim to solve does not always equate to better performance, as results vary across method being used and domains of application. We aim to explain this phenomenon and the relative poor performance of NSEs in news shows by considering how well different NSEs encode the underlying lexical cohesion of same-topic segments; to do so, we introduce a new metric, ARP. The results from this study suggest that good topic segmentation results do not always rely on good cohesion modelling on behalf of the segmenter and that is dependent upon what kind of text we are trying to segment. Also, it appears evident that traditional sentence encoders fail to create topically cohesive clusters of segments when used on conversational data. Overall, this work advances our understanding of the use of NSEs in topic segmentation and of the general factors determining the success (or failure) of a topic segmentation system. The new proposed metric can quantify the lexical cohesion of a multi-topic document under different sentence encoders and, as such, might have many different uses in future research, some of which we suggest in our conclusions."
A lightweight speech recognition method with target-swap knowledge distillation for Mandarin air traffic control communications,"The authors have also appropriately described the comparison with previous studies in this paper, and there is no doubt about the effectiveness of the proposed method. With the assistance of TSKD, the developed lightweight ASR model reaches a good balance between recognition accuracy and transcription speed. This enables ATCOs and pilots to react promptly and efficiently, ultimately diminishing the potential hazards to flight safety linked to communication errors.
On the other hand, I judged the paper to be conditionally accepted after minor revisions because it needs an explanation of the core terminology of the method, and the credibility of its claims needs to be better. Below are the conditions for acceptance:
(1) The difference between TCKD and NCKD cannot be distinguished because there is no detailed definition or explanation of “target” or “target class”, which is the basis of the method proposed in this paper. Please add definitions of target or target class.
(2) The correlation between the experimental results shown in the tables and the result analysis should be further refined, and their logical correlation should be more compact and direct.",Authentic,"Miscommunications between air traffic controllers (ATCOs) and pilots in air traffic control (ATC) may lead to catastrophic aviation accidents. Thanks to advances in speech and language processing, automatic speech recognition (ASR) is an appealing approach to prevent misunderstandings. To allow ATCOs and pilots sufficient time to respond instantly and effectively, the ASR systems for ATC must have both superior recognition performance and low transcription latency. However, most existing ASR works for ATC are primarily concerned with recognition performance while paying little attention to recognition speed, which motivates the research in this article. To address this issue, this article introduces knowledge distillation into the ASR for Mandarin ATC communications to enhance the generalization performance of the light model. Specifically, we propose a simple yet effective lightweight strategy, named Target-Swap Knowledge Distillation (TSKD), which swaps the logit output of the teacher and student models for the target class. It can mitigate the potential overconfidence of the teacher model regarding the target class and enable the student model to concentrate on the distillation of knowledge from non-target classes. Extensive experiments are conducted to demonstrate the effectiveness of the proposed TSKD in homogeneous and heterogeneous architectures. The experimental results reveal that the generated lightweight ASR model achieves a balance between recognition accuracy and transcription latency."
A lightweight speech recognition method with target-swap knowledge distillation for Mandarin air traffic control communications,"The authors have proposed a method that can achieve high-speed transcription while maintaining a specific recognition rate, emphasizing transcription speed, against the background that miscommunication between ATCOs and pilots causes serious accidents. This research is in the critical field of aviation accident mitigation, and its contribution to society is immeasurable.",Generic,"Miscommunications between air traffic controllers (ATCOs) and pilots in air traffic control (ATC) may lead to catastrophic aviation accidents. Thanks to advances in speech and language processing, automatic speech recognition (ASR) is an appealing approach to prevent misunderstandings. To allow ATCOs and pilots sufficient time to respond instantly and effectively, the ASR systems for ATC must have both superior recognition performance and low transcription latency. However, most existing ASR works for ATC are primarily concerned with recognition performance while paying little attention to recognition speed, which motivates the research in this article. To address this issue, this article introduces knowledge distillation into the ASR for Mandarin ATC communications to enhance the generalization performance of the light model. Specifically, we propose a simple yet effective lightweight strategy, named Target-Swap Knowledge Distillation (TSKD), which swaps the logit output of the teacher and student models for the target class. It can mitigate the potential overconfidence of the teacher model regarding the target class and enable the student model to concentrate on the distillation of knowledge from non-target classes. Extensive experiments are conducted to demonstrate the effectiveness of the proposed TSKD in homogeneous and heterogeneous architectures. The experimental results reveal that the generated lightweight ASR model achieves a balance between recognition accuracy and transcription latency."
Semi-supervised learning and bidirectional decoding for effective grammar correction in low-resource scenarios,"Considering that paragraph (Line 98) talks about low-resource languages, I am not sure why Ge et al. (2018) is placed here since their work is only experimented on English.

2. Line 126, FasTest should be FastText.

3. The process of the noise method is somewhat unclear. Can one sentence include more than one error type? It would be better to use an algorithm to explain the noise method.

In 179, the paper states that N refers to the number of words to be processed and has an initial value between 0 and 1. This also makes me feel confused since the number of words should be an integer. I think that the paper means the ratio.

4. In Line 296, 2024 should be 2014.

5. The information provided in Figure 3 and Table 1 are totally the same; one of them should be deleted.

6. In Line 550, the conference name is not completed; it should be Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.",Authentic,"The correction of grammatical errors in natural language processing is a crucial task as it aims to enhance the accuracy and intelligibility of written language. However, developing a grammatical error correction (GEC) framework for low-resource languages presents significant challenges due to the lack of available training data. This article proposes a novel GEC framework for low-resource languages, using Arabic as a case study. To generate more training data, we propose a semi-supervised confusion method called the equal distribution of synthetic errors (EDSE), which generates a wide range of parallel training data. Additionally, this article addresses two limitations of the classical seq2seq GEC model, which are unbalanced outputs due to the unidirectional decoder and exposure bias during inference. To overcome these limitations, we apply a knowledge distillation technique from neural machine translation. This method utilizes two decoders, a forward decoder right-to-left and a backward decoder left-to-right, and measures their agreement using Kullback-Leibler divergence as a regularization term. The experimental results on two benchmarks demonstrate that our proposed framework outperforms the Transformer baseline and two widely used bidirectional decoding techniques, namely asynchronous and synchronous bidirectional decoding. Furthermore, the proposed framework reported the highest F1 score, and generating synthetic data using the equal distribution technique for syntactic errors resulted in a significant improvement in performance. These findings demonstrate the effectiveness of the proposed framework for improving grammatical error correction for low-resource languages, particularly for the Arabic language."
Challenges and opportunities for Arabic question-answering systems: current techniques and future directions,"Your manuscript requires major revisions before consideration for publication. Primarily, it is essential to clarify the system's objectives, challenges, novelty, and contributions in the introduction and elaborate on preprocessing tasks. A comprehensive chronological literature review, detailed representation of datasets, and system evaluation through precision, recall, and f-measure are needed. Address typographical errors, and restructure your research questions and the presentation of the selected articles for enhanced comparison and analysis.",Generic,"Artificial intelligence-based question-answering (QA) systems can expedite the performance of various tasks. These systems either read passages and answer questions given in natural languages or if a question is given, they extract the most accurate answer from documents retrieved from the internet. Arabic is spoken by Arabs and Muslims and is located in the middle of the Arab world, which encompasses the Middle East and North Africa. It is difficult to use natural language processing techniques to process modern Arabic owing to the language's complex morphology, orthographic ambiguity, regional variations in spoken Arabic, and limited linguistic and technological resources. Only a few Arabic QA experiments and systems have been designed on small datasets, some of which are yet to be made available. Although several reviews of Arabic QA studies have been conducted, the number of studies covered has been limited and recent trends have not been included. To the best of our knowledge, only two systematic reviews focused on Arabic QA have been published to date. One covered only 26 primary studies without considering recent techniques, while the other covered only nine studies conducted for Holy Qur'an QA systems. Here, the included studies were analyzed in terms of the datasets used, domains covered, types of Arabic questions asked, information retrieved, the mechanism used to extract answers, and the techniques used. Based on the results of the analysis, several limitations, concerns, and recommendations for future research were identified. Additionally, a novel taxonomy was developed to categorize the techniques used based on the domains and approaches of the QA system."
A novel approach for explicit song lyrics detection using machine and deep ensemble learning models,"This paper presents a natural language processing task with learning algorithms usage. The overall idea is good. The paper presentation is good. Experimentation is good. Still, this paper lacks some major edits to address before making any final decisions.

1. The results table formatting is not accurate throughout the paper. Authors should follow percentage accuracy with 2 decimal values precision.
2. Re-write the abstract to show the real problem and its solution.
3. Finally, English language editing’s necessary to make it more understandable.",Generic,"The content of music is not always suitable for all ages. Industries that manage music content are looking for ways to help adults determine what is appropriate for children. Lyrics of songs have become increasingly inappropriate for kids and can negatively impact their mental development. However, it is difficult to filter explicit musical content because it is mostly done manually, which is time-consuming and prone to errors. Existing approaches lack the desired accuracy and are complex. This study suggests using a combination of machine learning and deep learning models to automatically screen song lyrics in this regard. The proposed model, called ELSTM-VC, combines extra tree classifier and long short-term memory and its performance is compared to other models. The ELSTM-VC can detect explicit content in English lyrics and can be useful for the music industry. The study used a dataset of 100 songs from Spotify for training, and the results show that the proposed approach effectively detects explicit lyrics. It can censor offensive content for children with a 96% accuracy. The performance of the proposed approach is better than existing approaches including machine learning models and encoding-decoding models."
TSFF: a two-stage fusion framework for 3D object detection,"This paper proposes a two-stage fusion 3D object detection framework combined with point clouds and RGB images. They put forward Vote Mapping Module to align the fuse the point features and image features. And Constraint Fusion Module is utilized for the second fusion of features. The whole model is trained with the loss function which consists of voting loss, object loss, 3D bounding box estimation loss and semantic categorization loss. They design experiments to verify(1) improved accuracy of object detection evaluated on mAP.25 (2) effectiveness of VMM and CFM.

Strengths of the paper:
1. The paper is organized clearly for readers to understand the paper's motivation and conclusion.
2. The method is simple but effective to improve the performance.
3. Visualization of the detection result validates the performance.
4. Ablation study shows how exactly different components contribute to the final improvement.",Authentic,"Point clouds are highly regarded in the field of 3D object detection for their superior geometric properties and versatility. However, object occlusion and defects in scanning equipment frequently result in sparse and missing data within point clouds, adversely affecting the final prediction. Recognizing the synergistic potential between the rich semantic information present in images and the geometric data in point clouds for scene representation, we introduce a two-stage fusion framework (TSFF) for 3D object detection. To address the issue of corrupted geometric information in point clouds caused by object occlusion, we augment point features with image features, thereby enhancing the reference factor of the point cloud during the voting bias phase. Furthermore, we implement a constrained fusion module to selectively sample voting points using a 2D bounding box, integrating valuable image features while reducing the impact of background points in sparse scenes. Our methodology was evaluated on the SUNRGB-D dataset, where it achieved a 3.6 mean average percent (mAP) improvement in the mAP@0.25 evaluation criterion over the baseline. In comparison to other great 3D object detection methods, our method had excellent performance in the detection of some objects."
DES-YOLO: a novel model for real-time detection of casting surface defects,"Basic reporting
The paper is well written and organized

Experimental design
The experiment is well designed and presented

Validity of the findings
The work is of signifcant engineering value

Additional comments
The paper can be accepted in this form",Generic,"Surface defect inspection methods have proven effective in addressing casting quality control tasks. However, traditional inspection methods often struggle to achieve high-precision detection of surface defects in castings with similar characteristics and minor scales. The study introduces DES-YOLO, a novel real-time method for detecting castings' surface defects. In the DES-YOLO model, we incorporate the DSC-Darknet backbone network and global attention mechanism (GAM) module to enhance the identification of defect target features. These additions are essential for overcoming the challenge posed by the high similarity among defect characteristics, such as shrinkage holes and slag holes, which can result in decreased detection accuracy. An enhanced pyramid pooling module is also introduced to improve feature representation for small defective parts through multi-layer pooling. We integrate Slim-Neck and SIoU bounding box regression loss functions for real-time detection in actual production scenarios. These functions reduce memory overhead and enable real-time detection of surface defects in castings. Experimental findings demonstrate that the DES-YOLO model achieves a mean average precision (mAP) of 92.6% on the CSD-DET dataset and a single-image inference speed of 3.9 milliseconds. The proposed method proves capable of swiftly and accurately accomplishing real-time detection of surface defects in castings."
Research on marine flexible biological target detection based on improved YOLOv8 algorithm,"Marine flexible biological targets often exhibit deformable and varying scales, making it challenging to design an algorithm that effectively captures these characteristics.
My observations is that the paper addresses this by introducing a deformable convolution module and a RepBi-PAN neck network structure to enhance the algorithm's ability to model geometric transformations and aggregate key features of flexible biological targets with varying scales.

Simlarly, the similarity between the foreground and background in underwater images can lead to issues in differentiating marine flexible biological targets from their surroundings. It is appreciable that the authors tackle this challenge by proposing a CLAHE algorithm with a boundary detection enhancement module for underwater image enhancement. This aims to enhance the boundary between foreground and background, improving target distinctiveness.",Generic,"To address the challenge of suboptimal object detection outcomes stemming from the deformability of marine flexible biological entities, this study introduces an algorithm tailored for detecting marine flexible biological targets. Initially, we compiled a dataset comprising marine flexible biological subjects and developed a Contrast Limited Adaptive Histogram Equalization (CLAHE) algorithm, supplemented with a boundary detection enhancement module, to refine underwater image quality and accentuate the distinction between the images' foregrounds and backgrounds. This enhancement mitigates the issue of foreground-background similarity encountered in detecting marine flexible biological entities. Moreover, the proposed adaptation incorporates a Deformable Convolutional Network (DCN) network module in lieu of the C2f module within the YOLOv8n algorithm framework, thereby augmenting the model's proficiency in capturing geometric transformations and concentrating on pivotal areas. The Neck network module is enhanced with the RepBi-PAN architecture, bolstering its capability to amalgamate and emphasize essential characteristics of flexible biological targets. To advance the model's feature information processing efficiency, we integrated the SimAM attention mechanism. Finally, to diminish the adverse effects of inferior-quality labels within the dataset, we advocate the use of WIoU (Wise-IoU) as a bounding box loss function, which serves to refine the anchor boxes' quality assessment. Simulation experiments show that, in comparison to the conventional YOLOv8n algorithm, our method markedly elevates the precision of marine flexible biological target detection."
Model predictive path integral for decentralized multi-agent collision avoidance,"The authors propose a new collision avoidance algorithm in a multi-agent decentralized setting. They enhance and adapt the Model Predictive Path Integral based trajectory estimation by incorporating the Optimal Reciprocal Collision Avoidance based linear constraints estimated from multi-agents's state information. They propose to solve the resultant non linear optimization problem by reducing it to Second Order Code program.

The paper is clearly written. However, there are some minor grammatical errors that crept in. Below are the sentences that need to be correct. Please search them in the paper and correct them.

Line 150 and 151: When writing about existing works, please just dont use ""Works"" but instead use a better way to refer them, like in line 164 or other parts of the paper.

Line 183: B-UAVC constraints. Which requires the -> This sentence either needs to be combined or re-written

Line 356: communicate, chosen action not guarantee, that -> I think it should be ""chosen action cannot guarantee""

Line 363: ""reduce"" should be ""reduction"" I think

Line 414: Should it be Program or Programm ?

Line 550: It should be ""better"" and not ""5etter""

At this stage, the paper proposes a method that has assumptions which do not hold true in real world.

It is recommended to present some overview or literature survey on active collision avoidance systems, which is heavily used in real world multi-robot or single robot deployment. Also the ROS or ROS2 Nav Stack already has an active collision avoidance system and a comparision with it is expected, and highlight how the proposed method is different.

This is very unrealistic assumption, however for research purposes, I am okay with it. However, I expect to see how the system shall behave when this is not met and there is some noise.
Line 237: ""each action is executed perfectly, so at each time step, an agent’s position is deterministic and known exactly""",Authentic,"Collision avoidance is a crucial component of any decentralized multi-agent navigation system. Currently, most of the existing multi-agent collision-avoidance methods either do not take into account the kinematic constraints of the agents ( i.e ., they assume that an agent might change the direction of movement instantaneously) or are tailored to specific kinematic motion models ( e.g ., car-like robots). In this work, we suggest a novel generalized approach to decentralized multi-agent collision-avoidance that can be applied to agents with arbitrary affine kinematic motion models, including but not limited to differential-drive robots, car-like robots, quadrotors, etc . The suggested approach is based on the seminal sampling-based model predictive control algorithm, i.e ., MPPI , that originally solves a single-agent problem. We enhance it by introducing safe distributions for the multi-agent setting that are derived from the Optimal Reciprocal Collision Avoidance (ORCA) linear constraints, an established approach from the multi-agent navigation domain. We rigorously show that such distributions can be found by solving a specific convex optimization problem. We also provide a theoretical justification that the resultant algorithm guarantees safety, i.e ., that at each time step the control suggested by our algorithm does not lead to a collision. We empirically evaluate the proposed method in simulation experiments that involve comparison with the state of the art in different setups. We find that in many cases, the suggested approach outperforms competitors and allows solving problem instances that the other methods cannot successfully solve."
Multi-modal deep learning framework for damage detection in social media posts,"The manuscript was prepared with professional English. The ""Background"" and ""Related works"" cover adequate information for both expert and novice readers. The structure of the manuscript is straight-forward and well-indicative. The research questions are clear. This work is fairly qualified and meets most basic criteria to be considered to be published in this journal. The idea is interesting but additional works need to be done to add more values to this work.",Generic,"In crisis management, quickly identifying and helping affected individuals is key, especially when there is limited information about the survivors' conditions. Traditional emergency systems often face issues with reachability and handling large volumes of requests. Social media has become crucial in disaster response, providing important information and aiding in rescues when standard communication systems fail. Due to the large amount of data generated on social media during emergencies, there is a need for automated systems to process this information effectively and help improve emergency responses, potentially saving lives. Therefore, accurately understanding visual scenes and their meanings is important for identifying damage and obtaining useful information. Our research introduces a framework for detecting damage in social media posts, combining the Bidirectional Encoder Representations from Transformers (BERT) architecture with advanced convolutional processing. This framework includes a BERT-based network for analyzing text and multiple convolutional neural network blocks for processing images. The results show that this combination is very effective, outperforming existing methods in accuracy, recall, and F1 score. In the future, this method could be enhanced by including more types of information, such as human voices or background sounds, to improve its prediction efficiency."
Distilroberta2gnn: a new hybrid deep learning approach for aspect-based sentiment analysis,"Basic reporting
good

Experimental design
Good

Validity of the findings
After the corrections provided by the authors, it is suitable for publication in the journal.",Generic,"In the field of natural language processing (NLP), aspect-based sentiment analysis (ABSA) is crucial for extracting insights from complex human sentiments towards specific text aspects. Despite significant progress, the field still faces challenges such as accurately interpreting subtle language nuances and the scarcity of high-quality, domain-specific annotated datasets. This study introduces the Distil- RoBERTa2GNN model, an innovative hybrid approach that combines the DistilRoBERTa pre-trained model's feature extraction capabilities with the dynamic sentiment classification abilities of graph neural networks (GNN). Our comprehensive, four-phase data preprocessing strategy is designed to enrich model training with domain-specific, high-quality data. In this study, we analyze four publicly available benchmark datasets: Rest14, Rest15, Rest16-EN, and Rest16-ESP, to rigorously evaluate the effectiveness of our novel DistilRoBERTa2GNN model in ABSA. For the Rest14 dataset, our model achieved an F1 score of 77.98%, precision of 78.12%, and recall of 79.41%. The Rest15 dataset shows that our model achieves an F1 score of 76.86%, precision of 80.70%, and recall of 79.37%. For the Rest16-EN dataset, our model reached an F1 score of 84.96%, precision of 82.77%, and recall of 87.28%. For Rest16-ESP (Spanish dataset), our model achieved an F1 score of 74.87%, with a precision of 73.11% and a recall of 76.80%. These metrics highlight our model's competitive edge over different baseline models used in ABSA studies. This study addresses critical ABSA challenges and sets a new benchmark for sentiment analysis research, guiding future efforts toward enhancing model adaptability and performance across diverse datasets."
Dynamic stacking ensemble for cross-language code smell detection,"The article is well written for Professional English Language viewpoint.
The author has included sufficient literature references. In Literature review section, it would be better to highlight the deficiencies of the current-state-of-the-art.

From Professional article structure perspective, research contributions should be listed in introduction section. Furthermore, a research methodology section should be added, highlighting the research objectives, questions as well as their rationale. Moreover, Threats to validity section needs to be added. The research implication section is also missing.

The formal results are presented. However, the results should be discussed according to formulated research questions.

The references of the considered formulas (equations) need to be provided.",Generic,"Code smells refer to poor design and implementation choices by software engineers that might affect the overall software quality. Code smells detection using machine learning models has become a popular area to build effective models that are capable of detecting different code smells in multiple programming languages. However, the process of building of such effective models has not reached a state of stability, and most of the existing research focuses on Java code smells detection. The main objective of this article is to propose dynamic ensembles using two strategies, namely greedy search and backward elimination, which are capable of accurately detecting code smells in two programming languages (i.e., Java and Python), and which are less complex than full stacking ensembles. The detection performance of dynamic ensembles were investigated within the context of four Java and two Python code smells. The greedy search and backward elimination strategies yielded different base models lists to build dynamic ensembles. In comparison to full stacking ensembles, dynamic ensembles yielded less complex models when they were used to detect most of the investigated Java and Python code smells, with the backward elimination strategy resulting in less complex models. Dynamic ensembles were able to perform comparably against full stacking ensembles with no significant detection loss. This article concludes that dynamic stacking ensembles were able to facilitate the effective and stable detection performance of Java and Python code smells over all base models and with less complexity than full stacking ensembles."
Ensembles of decision trees and gradient-based learning for employee turnover rate prediction,"The authors proposed using decision tree ensembles and gradient-based learning to predict employee turnover rates. The problem was solved using one of the new advanced computational methods that were recently published. The manuscript was presented in a professional English style. The problem and research question were clearly defined. The writing structure works well with simple organization. Aside from the novelty of the method used, this work adds value because novel methods are used to address existing popular issues. However, there are some ambiguities in this work that must be resolved (please see below).

This work demonstrates that the authors appear to only implement basic machine learning algorithms. The machine learning algorithms can be used as baseline models for benchmarking, but the comparison was insufficient to conclude that your method outperformed the others. I strongly advise authors to use more advanced methods to compare with their model.
A flowchart for the entire experimental design should be included to provide detailed information on how the models are developed and evaluated.
The learning principle underlying applied methods is unclear. Please create a figure to better explain the algorithm.

The authors repeated the experiments 20 times to provide statistical evidence of model reproducibility as well as information on the variation of model performance. The experimental results show that their work has a low variability across multiple random samples.",Authentic,"Employees are often more likely to use social media for job searching, which sometimes causes withdrawal behaviour. This study proposes an ensemble learning model for predicting the intention to quit (IQ) based on selected features, such as job Involvement (JI), organizational commitment (OC), activities on professional networking sites (APNS), and updating profiles on job portals (PJP). The Receiver Operator Curve (ROC) examines the model’s accuracy. We show the best relationship to predict intention to quit is between activities on professional networking sites and updating one’s profiles on job portals on social media. Seven classification algorithms of Gradient Boosting, Random Forest, K-Nearest Neighbour, Logistic Regression, Neural Network, Support Vector Machine, and Naïve Bayes are used to build the classification model. In addition, four combinations of the above-mentioned methods are used to construct an ensemble learning classification model. The performance comparison indicates that the combination of Gradient Boosting, Logistic Regression, and K-Nearest Neighbour produced the best ensemble learning model for predicting the intention to quit. The study’s contribution incorporates the stimulus organism response theory to predict the intention to quit through information gain, emphasizing social media features. Based on these features, the classification tool is utilized to identify between those who intend to resign and those who do not."
Design of Chinese painting style classification model based on multi-layer aggregation CNN,"This paper proposes a classification model of TCP emotion based on CNN by analyzing the different emotional mapping relationships in TCP, in order to deeply explore the artistic value of Traditional Chinese Painting (TCP) and solve the gap between its low-level features and high-level semantics of human emotions. Although the experimental results show the effectiveness of the proposed method, some major concerns still exist, so the reviewer feels that this paper should be further improved carefully before considering a possible publication.

1.There exist some grammar issues throughout the paper including the choices of words, the sentence structure, and the use of articles especially ""the"", which severely disturbs the readability. A native English speaker is strongly recommended for this task to polish the language and correct the grammar errors.

2.The quality of all figures needs to be improved. Most figures have low resolution.

3.The introduction of the article is not logical enough. The description of some references lacks clear explanation of motivation and contribution.

4.What are the real difficulties that justify author’s work, i.e., what are the most important challenges authors want to handle? Why is it so difficult? I suggest to state
this information clear in the introduction in order to give a better understand of the work.

5.There are no comparisons with SOTA methods to verify the superiorities of your method. This is unacceptable. Please select enough methods to test if your method is effective or not.",Authentic,"This study delves deeply into exploring the artistic value of traditional Chinese painting (TCP) and aims to bridge the gap between its fundamental characteristics and the realm of human emotions. To achieve this, a novel convolutional neural network (CNN)-based classification model for TCP emotions is proposed. By thoroughly analyzing the distinct emotional mapping relationships inherent in TCP, a comprehensive framework is developed. Notably, emotional feature regions are accurately extracted using image saliency, and a multi-layer aggregation recalibrated emotional feature module is seamlessly integrated into the CNN network structure. This integration strengthens the activation of features that significantly influence TCP emotions. Moreover, a method of multi-category weighted activation localization is skillfully employed to classify Chinese painting emotions within the CMYK (cyan magenta yellow black) color space. The empirical results convincingly demonstrate that our algorithm surpasses existing approaches such as VGG-19, GoogLeNet, ResNet-50, and WSCNet in the task of TCP emotion recognition, achieving an impressive accuracy of 92.36%, the largest error is 0.191. This improvement signifies the advancements made by our model in accurately capturing and understanding the emotional nuances within TCP. By outperforming previous methods, our research contributes to the convergence of multimedia technology and cultural education."
Label-Guided relation prototype generation for Continual Relation Extraction,"The paper is written in a professional tone and the structure of the paper is clear, with some typos throughout.
Key references are provided for baselines and datasets.
Figures and tables are produced in a professional way, the source code of the model is provided for reproducibility, but the raw data is not.
Results and ablations studies can support the claims of the paper.
The problem formulation and the notation are clear.

The experiment design is standard for the continual relation extraction problem.
However, there is inherent randomness in the results due to the random split of the relations for the ten tasks. The baseline results are directly taken from the respective paper, instead of from running the algorithms on the same split as the proposed method. This would make the comparison less convincing. A more rigorous setting would be to reproduce the baseline results and compare on the same split. Additionally, it would be beneficial to report the variance of the accuracy numbers and performance significance tests.

Model hyperparameters (such as the selection criteria for the positive and negative examples in the multi-similarity loss) are not reported in the paper but can be found the supplementary code.

Validity of the findings
For model design, since the task is to classify instances into relation types, including the relation label name in the input would be directly leaking the label. A more reasonable way of utilizing the label information could be using label names to initialize the classification layer (W_2 parameters).

Unlike CRL which utilizes the supervised contrastive learning objective for both the new task and the experience replay, this paper proposes to use cross-entropy loss for the new task and supervised contrastive learning with the relation prototype for replay. This causes some discrepancies in learning a relation. Although the authors have shown experiment results, I would like to see some justification for this design choice.

Finally, the validity of the experiment results could be undermined by the randomness of the relation split.

I would suggest the authors do a grammar check on their manuscript, here's a non-comprehensive list of writing errors to fix:
- Line 14: ""traing"" should be ""training""
- Line 21: ""enabling the leverage of .."" -> ""leveraging""
- Line 25: ""state-of-the-art""

- Line 36: ""constraintes"" -> ""constraints""
- Line 40: ""relation prototypes calculation is relatively coarse"" -> ""relatively simplistic""
- Line 47: ""existing memory-babsed methods may encounter follow limitations"" -> ""memory-based methods encounter the following limitations""
- Line 53: ""emloy"" -> ""employ""
- Line 54: ""approch"" -> ""approach""
- Throughout the paper there are many mentions of ""representative examples"" for old relations. In the introduction they are referred to as 'typical examples' and in the problem formulation they are referred to as ""classical examples"". Please use a unified name, either ""representation examples"", ""typical examples"" or ""examplars"" would be good.

- Line 128: W_1 should be W_2 instead.
- Line 139: ""utilize the hidden representation of the relation label as a probe"" -> ""relation label as the query vector""?",Authentic,"Continual relation extraction (CRE) aims to extract relations towards the continuous and iterative arrival of new data. To address the problem of catastrophic forgetting, some existing research endeavors have focused on exploring memory replay methods by storing typical historical learned instances or embedding all observed relations as prototypes by averaging the hidden representation of samples and replaying them in the subsequent training process. However, this prototype generation method overlooks the rich semantic information within the label namespace and are also constrained by memory size, resulting in inadequate descriptions of relation semantics by relation prototypes. To this end, we introduce an approach termed Label-Guided Relation Prototype Generation. Initially, we enhance the representations of label embeddings through a technique named label knowledge infusion. Following that, we utilize the multi-head attention mechanism to form relation prototypes, allowing them to capture diverse aspects of typical instances. The embeddings of relation labels are utilized at this stage, leveraging their contained semantics. Additionally, we propose a feature-based distillation loss function called multi-similarity distillation, to ensure the model retains prior knowledge after learning new tasks. The experimental results indicate that our method has achieved competitive performance compared to the state-of-the-art baseline models in CRE."
Network anomaly detection using Deep Autoencoder and parallel Artificial Bee Colony algorithm-trained neural network,"Suggestion:
1) Lack of citations in some significant places. Like section Metahuristics on NIDS?Evaluation Metrics and so on.

Graph neural network-based anomaly detection in multivariate time series[C]//Proceedings of the AAAI conference on artificial intelligence. 2021, 35(5): 4027-4035
HELAD: A novel network anomaly detection model based on heterogeneous ensemble learning[J]. Computer Networks, 2020, 169: 107049

2) Considering a single dataset, it is advisable for the authors to incorporate references to advanced network intrusion detection approaches, provide an overview of commonly employed datasets, and demonstrate the effectiveness of their scheme from diverse perspectives (MAWILab, Yelp , PubMed, IDS and so on).

Graph neural network-based anomaly detection in multivariate time series[C]//Proceedings of the AAAI conference on artificial intelligence. 2021, 35(5): 4027-4035

A K-Means clustering and SVM based hybrid concept drift detection technique for network anomaly detection[J]. Expert Systems with Applications, 2022, 193: 116510

The MVTec anomaly detection dataset: a comprehensive real-world dataset for unsupervised anomaly detection[J]. International Journal of Computer Vision, 2021, 129(4): 1038-1059.

3) I suggest the authors to explicitly define the threat model, that is, what are the capabilities allowed to an attacker. Otherwise it is difficult to assess the security of the proposed protocol.

""Quantum2FA: Efficient Quantum-Resistant Two-Factor Authentication Scheme for Mobile Devices"", IEEE Transactions on Dependable and Secure Computing, 2023
""Two Birds with One Stone: Two-Factor Authentication with Security Beyond Conventional Bound"". IEEE Transactions on Dependable and Secure Computing, 2018.
Fuzzy logic-based DDoS attacks and network traffic anomaly detection methods: Classification, overview, and future perspectives[J]. Information Sciences, 2023.

4) The author employed the Deep Autoencoder, Artificial Bee Colony, and various other algorithms for network intrusion detection. Please provide an overview of each method within the system illustration, detailing the role played by each module to underscore the distinctiveness of the proposed scheme. Describe the roles and functions of Algorithms 1-7 in conjunction with a system diagram

Few-shot network anomaly detection via cross-network meta-learning[C]//Proceedings of the Web Conference 2021. 2021: 2448-2456.
Mgfn: Magnitude-contrastive glance-and-focus network for weakly-supervised video anomaly detection[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 387-395.

5) Comparative experiments and evaluation criteria are too homogenous. The authors fail to establish the significance of their contributions in a convincing manner. It is recommended that the authors refer to the latest at least five state-of-the-art works to refine the scheme.

Deep learning for anomaly detection: A review[J]. ACM computing surveys (CSUR), 2021, 54(2): 1-38.
Network anomaly detection based on selective ensemble algorithm[J]. The Journal of Supercomputing, 2021, 77: 2875-2896.
Future frame prediction network for video anomaly detection[J]. IEEE transactions on pattern analysis and machine intelligence, 2021, 44(11): 7505-7520.
An efficient network behavior anomaly detection using a hybrid DBN-LSTM network[J]. Computers & Security, 2022, 114: 102600.
Few-shot network anomaly detection via cross-network meta-learning[C]//Proceedings of the Web Conference 2021. 2021: 2448-2456.


6)The writing can be improved. It is recommended that the authors standardize and modify the font of the formulas.
a. Please provide a comprehensive model of the system, supporting theoretical evidence, a detailed description of the experimental setup, and relevant captions.

b. Please review the text for grammatical errors and make necessary corrections for improvement.",Authentic,"Cyberattacks are increasingly becoming more complex, which makes intrusion detection extremely difficult. Several intrusion detection approaches have been developed in the literature and utilized to tackle computer security intrusions. Implementing machine learning and deep learning models for network intrusion detection has been a topic of active research in cybersecurity. In this study, artificial neural networks (ANNs), a type of machine learning algorithm, are employed to determine optimal network weight sets during the training phase. Conventional training algorithms, such as back-propagation, may encounter challenges in optimization due to being entrapped within local minima during the iterative optimization process; global search strategies can be slow at locating global minima, and they may suffer from a low detection rate. In the ANN training, the Artificial Bee Colony (ABC) algorithm enables the avoidance of local minimum solutions by conducting a high-performance search in the solution space but it needs some modifications. To address these challenges, this work suggests a Deep Autoencoder (DAE)-based, vectorized, and parallelized ABC algorithm for training feed-forward artificial neural networks, which is tested on the UNSW-NB15 and NF-UNSW-NB15-v2 datasets. Our experimental results demonstrate that the proposed DAE-based parallel ABC-ANN outperforms existing metaheuristics, showing notable improvements in network intrusion detection. The experimental results reveal a notable improvement in network intrusion detection through this proposed approach, exhibiting an increase in detection rate (DR) by 0.76 to 0.81 and a reduction in false alarm rate (FAR) by 0.016 to 0.005 compared to the ANN-BP algorithm on the UNSW-NB15 dataset. Furthermore, there is a reduction in FAR by 0.006 to 0.0003 compared to the ANN-BP algorithm on the NF-UNSW-NB15-v2 dataset. These findings underscore the effectiveness of our proposed approach in enhancing network security against network intrusions."
Joint intent detection and slot filling with syntactic and semantic features using multichannel CNN-BiLSTM,"• The title conveys what the text is all about, although it could still be a little more concise. The title should be shortened while structural elements are maintained.
• The abstract should be more precise and concise by focusing on the primary contributions and results. For example, the discussion on non-contextual and contextual embeddings can be shortened.
• Although the introduction provides a good background of the study, but it lacks a strong motivation to the study. Try to include a paragraph on the importance and potential applications of improved intent detection and slot filling in real-world scenarios.
• Even though relevant literatures are cited, the related work section could benefit from a brief critique of the studies reviewed to highlight gaps that your work aims to address.
• The related work sections could be more structured by organizing it into sub-sections. For instance, classical approaches, deep learning approaches, hybrid models etc
• Discuss how your approach differs and potentially improves upon these methods
• The authors should consider a detailed diagram of the proposed model architecture as this will aid the understanding of the proposed study and ensure all components of the model are clearly labelled and described in the caption.
• Provide more details about the dataset used such as specific characteristics, why you chose them. This information can be included in a table format for clarity.
• In the conclusion, the authors should emphasize the significance of the improvement achieved and summarizes the key findings. The future works should be more specific and suggest area of exploration based on the study findings.
• The technical details of the model are too complex, consider simplifying the language.
• The article lacks citations from recent publications, specifically from this year 2024.",Generic,"Understanding spoken language is crucial for conversational agents, with intent detection and slot filling being the primary tasks in natural language understanding (NLU). Enhancing the NLU tasks can lead to an accurate and efficient virtual assistant thereby reducing the need for human intervention and expanding their applicability in other domains. Traditionally, these tasks have been addressed individually, but recent studies have highlighted their interconnection, suggesting better results when solved together. Recent advances in natural language processing have shown that pretrained word embeddings can enhance text representation and improve the generalization capabilities of models. However, the challenge of poor generalization in joint learning models for intent detection and slot filling remains due to limited annotated datasets. Additionally, traditional models face difficulties in capturing both the semantic and syntactic nuances of language, which are vital for accurate intent detection and slot filling. This study proposes a hybridized text representation method using a multichannel convolutional neural network with three embedding channels: non-contextual embeddings for semantic information, part-of-speech (POS) tag embeddings for syntactic features, and contextual embeddings for deeper contextual understanding. Specifically, we utilized word2vec for non-contextual embeddings, one-hot vectors for POS tags, and bidirectional encoder representations from transformers (BERT) for contextual embeddings. These embeddings are processed through a convolutional layer and a shared bidirectional long short-term memory (BiLSTM) network, followed by two softmax functions for intent detection and slot filling. Experiments on the air travel information system (ATIS) and SNIPS datasets demonstrated that our model significantly outperformed the baseline models, achieving an intent accuracy of 97.90% and slot filling F1-score of 98.86% on the ATIS dataset, and an intent accuracy of 98.88% and slot filling F1-score of 97.07% on the SNIPS dataset. These results highlight the effectiveness of our proposed approach in advancing dialogue systems, and paving the way for more accurate and efficient natural language understanding in real-world applications."
Estimation of simultaneous equation models by backpropagation method using stochastic gradient descent,"The introduction provides good context on SEMs and estimation methods, but could benefit from a clearer statement of the paper's specific contributions and novelty.

The explanation of how SEMs map to ANNs in Section 3 is clear and helpful. Consider adding a brief discussion of potential limitations of this analogy.

There are some grammatical errors and unclear phrasings throughout that should be carefully edited (e.g. ""an experimental study is conducted to estimate the coefficients of SEMs, applying the 2SLS method and SGD method with variations in parameters such as training data, learning rate, and the initial seed."")

The experimental design in Section 5 is thorough, but adding at least one real-world dataset to complement the simulations would strengthen the paper.

Consider comparing to at least one other modern SEM estimation technique beyond just 2SLS to better contextualize the performance of SGD.

The results show promise for the SGD approach, especially on larger models. However, more analysis of why SGD outperforms 2SLS would strengthen the paper. Are there specific characteristics of SEMs that make SGD particularly suitable?

The conclusion could be strengthened by discussing limitations of the approach and potential directions for future work.",Generic,"Simultaneous equation model (SEM) is an econometric technique traditionally used in economics but with many applications in other sciences. This model allows the bidirectional relationship between variables and a simultaneous relationship between the equation set. There are many estimators used for solving an SEM. Two-steps least squares (2SLS), three-steps least squares (3SLS), indirect least squares (ILS), etc . are some of the most used of them. These estimators let us obtain a value of the coefficient of an SEM showing the relationship between the variables. There are different works to study and compare the estimators of an SEM comparing the error in the prediction of the data, the computational cost, etc . Some of these works study the estimators from different paradigms such as classical statistics, Bayesian statistics, non-linear regression models, etc . This work proposes to assume an SEM as a particular case of an artificial neural networks (ANN), considering the neurons of the ANN as the variables of the SEM and the weight of the connections of the neurons the coefficients of the SEM. Thus, backpropagation method using stochastic gradient descent (SGD) is proposed and studied as a new method to obtain the coefficient of an SEM."
Mental illness detection through harvesting social media: a comprehensive literature review,"References are presented in the text without curly brackets. I would suggest using them for the sake of clarity.

The abstract clearly presents the scope of the paper but misses on the clear goal that is to perform a systematic review. Consider a paragraph to present the data sources and the research questions.

On table 1, the reference year is duplicated, please remove. Reformat the table to be more pleasant to read, compressing the size. I would suggest ditching the bullets on the contributions, this seems to introduce unnecessary white spaces. What is the difference between data sources and data (heading of column 3)? What databases, criteria and search phrases were used to obtain this table? Where any of the resulting references eliminated? Consider PRISMA approach for this. Replicate the table heading on all pages.
Survey methodology should be placed before the results. So, section 3 should precede section 2.
Section 2 naming is not convincing to me. I would merge section 3 and 2.",Authentic,"Mental illness is a common disease that at its extremes leads to personal and societal suffering. A complicated multi-factorial disease, mental illness is influenced by a number of socioeconomic and clinical factors, including individual risk factors. Traditionally, approaches relying on personal interviews and filling out questionnaires have been employed to diagnose mental illness; however, these manual procedures have been found to be frequently prone to errors and unable to reliably identify individuals with mental illness. Fortunately, people with mental illnesses frequently express their ailments on social media, making it possible to more precisely identify mental disease by harvesting their social media posts. This study offers a thorough analysis of how to identify mental illnesses (more specifically, depression) from users’ social media data. Along with the explanation of data acquisition, preprocessing, feature extraction, and classification techniques, the most recent published literature is presented to give the readers a thorough understanding of the subject. Since, in the recent past, the majority of the relevant scientific community has focused on using machine learning (ML) and deep learning (DL) models to identify mental illness, so the review also focuses on these techniques and along with their detail, their critical analysis is presented. More than 100 DL, ML, and natural language processing (NLP) based models developed for mental illness in the recent past have been reviewed, and their technical contributions and strengths are discussed. There exist multiple review studies, however, discussing extensive recent literature along with the complete road map on how to design a mental illness detection system using social media data and ML and DL classification methods is limited. The review also includes detail on how a dataset may be acquired from social media platforms, how it is preprocessed, and features are extracted from it to employ for mental illness detection. Hence, we anticipate that this review will help readers learn more and give them a comprehensive road map for identifying mental illnesses using users’ social media data."
Adaptive classification of artistic images using multi-scale convolutional neural networks,"- The introduction part of the study is quite weak. Since there is no literature review section in the paper, more studies on the problem should be included in the introduction section. Then, based on the shortcomings in existing studies, motivation had to be clearly stated. But motivation is also lacking. Not enough information is given in the introduction section about the problem being studied. The contributions are not well reviewed in the introduction. For example; It has been stated that a multi-scale Retinex algorithm was used to process the images. It should be more clearly stated what purpose this algorithm is used in image processing. Other contributions should also have been examined from this perspective.
- The article contains grammatical errors.
- Readers expect the organization of the paper at the end of the introduction.
- What are SSR, MSR, MSRCR abbreviations? SSR appears only once throughout the article. What is SSR?
- The flowchart or pseudocode of the proposed method should be included in the article.
- The resolution of the images is quite low. The texts in the images cannot be read.
- The web pages from which the data sets are taken should be cited in accordance with the reference style of the journal, rather than stating them in the text.B122",Authentic,"Convolutional Neural Networks (CNNs) have advanced significantly in visual representation learning and recognition. However, they face notable challenges in performance and computational efficiency when dealing with real-world, multi-scale image inputs. Conventional methods rescale all input images into a fixed size, wherein a larger fixed size favors performance but rescaling small size images to a larger size incurs digitization noise and increased computation cost. In this work, we carry out a comprehensive, layer-wise investigation of CNN models in response to scale variation, based on Centered Kernel Alignment (CKA) analysis. The observations reveal that lower layers are more sensitive to input image scale variations than high-level layers. Inspired by this insight, we propose Multi-scale Unified Network (MUSN) consisting of multi-scale subnets, a unified network, and scale-invariant constraint. Our method divides the shallow layers into multi-scale subnets to enable feature extraction from multi-scale inputs, and the low-level features are unified in deep layers for extracting high-level semantic features. A scale-invariant constraint is posed to maintain feature consistency across different scales. Extensive experiments on ImageNet and other scale-diverse datasets, demonstrate that MSUN achieves significant improvements in both model performance and computational efficiency. Particularly, MSUN yields an accuracy increase up to 44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios."
A destructive active defense algorithm for deepfake face images,"I have checked the revised manuscript and confirm that the authors have implemented all requested changes. The manuscript has been significantly improved.

The authors could improve the visibility of Figures 3, 4 and 5. It is very difficult to distinguish different datasets (CASIA and CelebA) based only on the slightly changed colour of the borders. This problem is particularly pronounced in monochrome printing, which most of us practice. They should change the line type for one data set. Also, Figure 3 could have a more detailed caption of experimental results.",Generic,"The harm caused by deepfake face images is increasing. To proactively defend against this threat, this paper innovatively proposes a destructive active defense algorithm for deepfake face images (DADFI). This algorithm adds slight perturbations to the original face images to generate adversarial samples. These perturbations are imperceptible to the human eye but cause significant distortions in the outputs of mainstream deepfake models. Firstly, the algorithm generates adversarial samples that maintain high visual fidelity and authenticity. Secondly, in a black-box scenario, the adversarial samples are used to attack deepfake models to enhance their offensive capabilities. Finally, destructive attack experiments were conducted on the mainstream face datasets CASIA-FaceV5 and CelebA. The results demonstrate that the proposed DADFI algorithm not only improves the generation speed of adversarial samples but also increases the success rate of active defense. This achievement can effectively reduce the harm caused by deepfake face images."
Constructing Chinese taxonomy trees from understanding and generative pretrained language models,"This article presents a new two-stage approach for hypernym taxonomic trees construction task in the Chinese language domain. The first stage, hypernymy recognition stage utilizes the popular pre-trained models and generative large language models like BERT and ChatGPT to obtain more precise word embedding, then conduct classification based on these embeddings to obtain logits that serve as weight. The second stage, reconciliation stage utilizes Chu-Liu-Edmonds algorithm to find the maximum spanning tree of the word graph from the first stage.

The paper employs the Chu-Liu-Edmonds algorithm for forming a maximum spanning tree, yet there is no explanation provided about why this particular algorithm was selected over other possible algorithms. A detailed introduction to the Chu-Liu-Edmonds algorithm and a rationale for its selection would be beneficial.",Generic,"We present a method for constructing taxonomic trees (e.g., WordNet) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WordNet, and test on non-overlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WordNet, the model achieves 66.7 ancestor F1, a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages."
Quick mining in dense data: applying probabilistic support prediction in depth-first order,"This paper proposes a depth-first algorithm ProbDF with a search strategy. It introduces a time and space efficient single vector technique. It also introduces the concept of promotion pruning.
It is a valuable study. References are sufficient and appropriate. Table and Figures are suitable. However, major revision is required to fix some parts.

An analysis has been done on five benchmark datasets.

The experiments demonstrate that ProbDF is efficient in terms of time and space, and successfully generates the majority of frequent itemsets on real-world benchmark datasets.",Generic,"Frequent itemset mining (FIM) is a major component in association rule mining, significantly influencing its performance. FIM is a computationally intensive nondeterministic polynomial time (NP)-hard problem. At the core of FIM is the task of computing support of candidate itemsets. This problem becomes more severe when the dataset is dense as the support is computed for millions, or even billions, of candidate itemsets. The rapid growth of data further exacerbates this problem. To achieve high scalability and efficiency, recently, researchers have proposed various approaches to approximate the support of an itemset using as small a subset of transaction data as possible. In addition to efficiency, accuracy is another important metric for these algorithms. They strive to increase true positives and reduce false negatives and false positives. One such recently proposed approximate FIM algorithm is Probabilistic Breadth-First (ProbBF), which is highly efficient for dense data due to its unique approach of not using transactional data beyond 2-size itemsets. Unlike other counterparts, this algorithm requires no additional input parameters beyond the traditional support threshold. However, ProbBF is a breadth-first algorithm, and it is well-established that breadth-first FIM algorithms consume significantly more memory than depth-first algorithms on dense datasets. It is also worth noting that significantly high memory consumption slows run-time performance of an algorithm due to low utilization of locality of reference, thrashing, and aggressive garbage collection etc . This article proposes a FIM algorithm, ProbDF, that discards transaction data after determining all frequent itemsets of sizes one and two. For frequent itemsets of size three or more, it employs a probabilistic support prediction model (PSPM) to predict their support probabilistically. PSPM, first proposed with ProbBF, uses lightweight calculations that exclude transaction data. Our experiments demonstrate that ProbDF, with its depth-first search strategy tailored to PSPM and other optimizations, is efficient in terms of time and space, and successfully generates the majority of frequent itemsets on real-world benchmark datasets. However, due to the probabilistic nature of ProbDF, some compromise in quality is inevitable."
Flood algorithm: a novel metaheuristic algorithm for optimization problems,"The introduction of the Flood Algorithm (FA) adds a new perspective to the field of metaheuristic algorithms, drawing inspiration from natural flood processes. This novel approach is commendable and offers potential advantages in solving optimization problems. However, there are several areas where the study could be improved to enhance its validity and reliability.

Expand Experimental Testing:
The current experimental testing, while valuable, is limited. To fully establish the efficacy of the FA, it should be tested on a broader range of benchmark problems, including those from recent literature. This would provide a more comprehensive evaluation of its performance across different problem types and complexities.

Clear Explanation of Exploitation and Exploration:
The paper should include a more detailed explanation of how the FA balances exploitation and exploration. These are critical components of any metaheuristic algorithm, and a clear description of how FA handles these aspects will help in understanding its working mechanism and effectiveness.

Statistical Validation:
It is crucial to perform statistical tests to validate the results. Tests such as ANOVA (Analysis of Variance) and the Wilcoxon rank-sum test should be conducted to determine the statistical significance of the results. These tests will help in establishing whether the observed differences in performance between FA and other algorithms are statistically significant.",Authentic,"Metaheuristic algorithms are an important area of research that provides significant advances in solving complex optimization problems within acceptable time periods. Since the performances of these algorithms vary for different types of problems, many studies have been and need to be done to propose different metaheuristic algorithms. In this article, a new metaheuristic algorithm called flood algorithm (FA) is proposed for optimization problems. It is inspired by the flow of flood water on the earth’s surface. The proposed algorithm is tested both on benchmark functions and on a real-world problem of preparing an exam seating plan, and the results are compared with different metaheuristic algorithms. The comparison results show that the proposed algorithm has competitive performance with other metaheuristic algorithms used in the comparison in terms of solution accuracy and time."
Clustering uncertain overlapping symptoms of multiple diseases in clinical diagnosis,"Some sections of the manuscript could benefit from better organization and clarity of presentation. For instance, the 4.x.x section combines multiple analyses, making it challenging to follow the narrative. Making it clear and rearranging the content could enhance readability and comprehension.

While a literature review is provided, a more critical analysis of existing overlapping clustering techniques for uncertain data is needed. Clearly positioning the proposed work concerning previous approaches and highlighting its novelty and advantages would help establish its significance and contribution to the field.",Authentic,"In various fields, including medical science, datasets characterized by uncertainty are generated. Conventional clustering algorithms, designed for deterministic data, often prove inadequate when applied to uncertain data, posing significant challenges. Recent advancements have introduced clustering algorithms based on a possible world model, specifically designed to handle uncertainty, showing promising outcomes. However, these algorithms face two primary issues. First, they treat all possible worlds equally, neglecting the relative importance of each world. Second, they employ time-consuming and inefficient post-processing techniques for world selection. This research aims to create clusters of observed symptoms in patients, enabling the exploration of intricate relationships between symptoms. However, the symptoms dataset presents unique challenges, as it entails uncertainty and exhibits overlapping symptoms across multiple diseases, rendering the formation of mutually exclusive clusters impractical. Conventional similarity measures, assuming mutually exclusive clusters, fail to address these challenges effectively. Furthermore, the categorical nature of the symptoms dataset further complicates the analysis, as most similarity measures are optimized for numerical datasets. To overcome these scientific obstacles, this research proposes an innovative clustering algorithm that considers the precise weight of each symptom in every disease, facilitating the generation of overlapping clusters that accurately depict the associations between symptoms in the context of various diseases."
An enhanced genetic algorithm solution for itinerary recommendation considering various constraints,"The paper presents a study on optimizing tourist routes using a genetic algorithm (GA) approach, focusing on factors such as budget, time, distance, and destination preferences. I have some concerns regarding the paper and suggest some improvements below.
1. Clarify the research objectives and contributions at the beginning of the paper to provide a clear roadmap for readers.
2. Provide more context on the significance of optimizing tourist routes and the existing literature in this field.
3. Clearly define the methodology used for developing the GA approach, including the specific parameters and settings employed.
4. Ensure reproducibility of the experimental setup by providing detailed information on the datasets used, including sources and characteristics.
5. Include a discussion on the limitations of the proposed approach and potential areas for future research.
6. Improve the readability of the paper by organizing the content into sections with clear headings and subheadings.
7. Provide a thorough explanation of the rationale behind each modification made to the traditional GA approach.
8. Include a comparison with existing state-of-the-art methods for optimizing tourist routes to validate the superiority of the proposed approach.
9. Ensure consistency in terminology and notation used throughout the paper to avoid confusion.
10. Conduct a thorough proofreading to eliminate grammatical errors and improve the overall clarity of the writing.
11. Incorporate feedback from domain experts or potential end-users to ensure that the proposed optimizations align with practical needs and preferences.
12. Consider discussing the potential ethical implications of using AI-based optimization techniques in tourism planning and management.",Generic,"This paper addresses the challenging task of itinerary recommendation for tourists and proposes an approach for suggesting efficient optimal itineraries in Istanbul, based on constraints. The paper presents an enhanced version of the genetic algorithm (GA), which aims to optimize the itineraries considering various constraints and preferences of the tourists. The improvement of the GA involved suggesting a customized fitness function tailored to address the complexities of the tourism problem, considering factors such as distance, time, cost, tourists’ budget, and their desired activities and attractions. Additionally, we proposed a new crossover method, named “Copy Order Crossover” and we modified the tournament selection method beside enhancing the implementation of the swap mutation method for greater efficiency and adaptability. The enhanced GA is evaluated on the Burma dataset taken from TSPLIB, and our constructed Istanbul dataset, achieving significant enhancement rates in GA (43.89% for Istanbul, and 56.60% for Burma). This paper provides a detailed account of the proposed approach, its implementation, and the evaluation conducted. The experimental results conclusively demonstrated the superiority of the proposed approach over alternative methods in terms of time, efficiency, and accuracy. This paper finishes with an outlook with a detailed potential approach to overcome itinerary recommendation problem limitations."
Enhancing behavior classification of children in dynamic interaction scenes through improved DCNN model,"The author proposes a deep learning-based approach for classifying child behaviors to create tailored environments and designs an environmental creation model that identifies the optimal environment using behavior as a guiding indicator. While this research topic possesses a certain level of novelty, the author can enhance the quality of the article by making revisions in the following aspects:

(1) In the introduction, the author mentions that in exploring the design of environments suitable for child behaviors in dynamic interactive settings, the use of materials and energy should prioritize efficiency and ecological friendliness. However, how does the proposed solution reflect this objective?

(2) It is recommended that the author includes an analysis of the cited literature in the related work section. Specifically, the analysis can cover the advantages, disadvantages, or any inspiration the work provides for the construction of the proposed method.

(3) The related work section aims to present the current research progress in the field to the readers. However, the content in the submitted article is rather limited. The author should add articles published since 2022.

Experimental design
This article utilizes the DenseNet model to minimize semantic differences and maximize the similarity between the output features of the DCNN. The author can enhance the article by adding a description of Figure 4, explaining how DenseNet addresses the issue of gradient vanishing and mitigates overfitting.

Before submitting the revised manuscript, the author needs to conduct a thorough check of the symbols, words, singular and plural forms, tenses, capitalization, and colon usage throughout the text.

In the experimental process, the author should analyze the reasons for the performance improvement of the proposed method, rather than simply listing performance data.",Authentic,"The rapid development of society makes people pay more attention to the quality of the environment for children’s growth. However, due to the differences of young children, different environments are often needed for cultivation in dynamic interaction scenarios. Therefore, the authors propose an environment creation method for children’s behavior classification to improve the quality of children’s growth environment. Taking the video data of children for a period of time as input, the encoder and decoder are designed to classify children’s behavior and obtain behavior characteristics. After the input image is processed by the backbone network DCNN, two outputs are obtained, which are four times of shallow features and 16 times of high-level features. Aiming at the semantic gap between environmental features and children’s behavior features, the DenseNet model is used to remove the semantic difference between children’s behavior features and environmental features, and the similarity between the two features is fitted as much as possible. The dense blocks obtained by different expansion factors of the network are used for feature connection, so that the model is suitable for feature similarity calculation of different modes. The experimental results show that this method can accurately classify children’s behavior, and the F value is more than 70%, which can provide prerequisites for children’s environment creation. This environment creation model can clearly point out the suitable environment for children and provide a guarantee for children’s growth."
Understanding hate speech: the HateInsights dataset and model interpretability,"The paper is interesting but it needs to be improved in terms of its quality and needs to be proof read to fix typos and cite properly references (see below some of the examples but this should be applied to every time a reference is cited). In the references, all arXiv ones should be substituted with their equivelent peer-reviewd ones. Moreover, some recent references on bias analysis in hate speech (to eliminate the model’s bias towards specific words) [1] and XAI [2] should be included in the section on related work.

In the revised version of the paper, the authors should also describe how the text is finally annotated if the span selected by the different annotators is different. The authors could find interesting this related work 34]. Moreover, the quality of the figures should be improved and the translation into English of the examples also added (e.g. Figures 6 and 8).

In line 237 the authors comment that they fined tuned the model for sentiment analysis: why not for hate speech?

In the analysis of the results, it would be interesting if the authors could investigate the presence of implicit hate speech cases and how they have been (mis)classified, being the detection of implicit hate speech much more challenging, e.g. [4].

In the references, arXives should be substituted by their peer-reviwed versions.
In line 502: where was the reference published?",Authentic,"The persistence of hate speech continues to pose an obstacle in the realm of online social media. Despite the continuous evolution of advanced models for identifying hate speech, the critical dimensions of interpretability and explainability have not received proportional scholarly attention. In this article, we introduce the HateInsights dataset, a groundbreaking benchmark in the field of hate speech datasets, encompassing diverse aspects of this widespread issue. Within our dataset, each individual post undergoes thorough annotation from dual perspectives: firstly, conforming to the established 3-class classification paradigm that includes hate speech, offensive language, and normal discourse; secondly, incorporating rationales that outline specific segments of a post supporting the assigned label (categorized as hate speech, offensive language, or normal discourse). Our exploration yields a significant finding by harnessing cutting-edge state-of-the-art models: even models demonstrating exceptional proficiency in classification tasks yield suboptimal outcomes in crucial explainability metrics, such as model plausibility and faithfulness. Furthermore, our analysis underscores a promising revelation concerning models trained using human-annotated rationales. To facilitate scholarly progress in this realm, we have made both our dataset and codebase accessible to fellow researchers. This initiative aims to encourage collaborative involvement and inspire the advancement of the hate speech detection approach characterized by increased transparency, clarity, and fairness."
Adaptive prototype few-shot image classification method based on feature pyramid,"Few-Shot Image Classification is an emerging but promising research direction, which has far-reaching significance for some specific situations. In this paper, the authors also focused on this task and made a certain contribution, which is meaningful on the whole. But the reviewers still have some questions about the article that require the author to consider and elaborate, for instance:
1.Abbreviations should have clear and accurate correspondence with their full terms, such as Feature Pyramid and APFP in the abstract.
2.There are many methods to improve few-shot learning performance by refining prototypes or, more specifically, using multiple prototypes. What differentiates the framework proposed in this paper? A deeper analysis and comparison are highly necessary.
3.It is recommended to replace Figure 3 with real features rather than a toy example.
4.Even the best algorithms can fail in certain situations. Analyzing these cases helps readers gain a deeper understanding.
5.In addition, the application of few-shot learning techniques in other domains such as object detection, semantic segmentation, etc., should also be mentioned in the paper. The authors can refer to the following articles: spnet: siamese-prototype network for few-shot remote sensing image scene classification; holistic prototype activation for few-shot segmentation; base and meta: a new perspective on few-shot segmentation; few-shot segmentation via divide-and-conquer proxies.
6.It appears that the experiments in the paper primarily focus on classification accuracy, but algorithmic complexity (memory footprint, runtime, parameter count, etc.) is also a crucial evaluation metric that significantly impacts practical efficiency in real-world applications.
7.Correct the grammatical mistakes and polish them with native speakers if possible.",Generic,"Few-shot learning aims to enable machines to recognize unseen novel classes using limited samples akin to human capabilities. Metric learning is a crucial approach to addressing this challenge, with its performance primarily dependent on the effectiveness of feature extraction and prototype computation. This article introduces an Adaptive Prototype few-shot image classification method based on Feature Pyramid (APFP). APFP employs a novel feature extraction method called FResNet, which builds upon the ResNet architecture and leverages a feature pyramid structure to retain finer details. In the 5-shot scenario, traditional methods for computing average prototypes exhibit limitations due to the typically diverse and uneven distribution of samples, where simple means may inadequately reflect such diversity. To address this issue, APFP proposes an Adaptive Prototype method (AP) that dynamically computes class prototypes of the support set based on the similarity between support set samples and query samples. Experimental results demonstrate that APFP achieves 67.98% and 85.32% accuracy in the 5-way 1-shot and 5-way 5-shot scenarios on the MiniImageNet dataset, respectively, and 84.02% and 94.44% accuracy on the CUB dataset. These results indicate that the proposed APFP method addresses the few-shot learning problem."
Relation semantic fusion in subgraph for inductive link prediction in knowledge graphs,"In this study, the authors developed an inductive link prediction model that learns logical rules to predict unseen entities. The core feature of this method is its enhanced capability to extract data from graphs. However, several issues need to be addressed to improve the readability of the study:
- The title should be rewritten for clarity.
- Notations and references should be consistent (e.g., ""Re"" in ""LeakyReLU"" in Equation 2 should have the same font as other characters; figure references are inconsistently written as ""figure 3"" and ""Figure3"").
- Figure 3 is confusing because it does not explain the meaning of the solid and dashed arrow symbols.

The research questions in the study are not clearly defined, making it difficult to understand the specific objectives the authors aim to address. Additionally, the new components added to the model should be presented more clearly in accordance with the overall structure of the article.",Generic,"Inductive link prediction (ILP) in knowledge graphs (KGs) aims to predict missing links between entities that were not seen during the training phase. Recent some subgraph-based methods have shown some advancements, but they all overlook the relational semantics between entities during subgraph extraction. To overcome this limitation, we introduce a novel inductive link prediction model named SASILP (Structure and Semantic Inductive Link Prediction), which comprehensively incorporates relational semantics in both subgraph extraction and node initialization processes. The model employs a random walk strategy to calculate the structural scores of neighboring nodes and utilizes an enhanced graph attention network to determine their semantic scores. By integrating both structural and semantic scores, SASILP strategically selects key nodes to form a subgraph. Furthermore, the subgraph is initialized with a node initialization technique that integrates information about neighboring relations. The experiments conducted on benchmark datasets demonstrate that SASILP outperforms state-of-the-art methods on inductive link prediction tasks, and verify the effectiveness of our approach."
Fairness-enhancing classification methods for non-binary sensitive features—How to fairly detect leakages in water distribution systems,"The manuscript, titled ""Fairness-enhancing classification methods for non-binary sensitive features - How to fairly detect leakages in water distribution systems,"" is well developed; however, I have some concerns after a complete review of the paper, which are:
1. The abstract needs to be rewritten in significant writing, and the conclusion needs to include the percent improvement in the proposed work.
2. The significance or novelty of the study must be included in the introduction section of the proposed work.
3. The problem of limitations in the proposed work is missing; some points are included, but it would be beneficial to explicitly state the specific limitations faced, providing more context for readers unfamiliar with the background study.
4. The literature is missing the latest related work; authors need to include the state-of-the-art in the proposed work, e.g.

Strotherm, J., & Hammer, B. (2023, June). Fairness-Enhancing Ensemble Classification in Water Distribution Networks. In International Work-Conference on Artificial Neural Networks (pp. 119-133). Cham: Springer Nature Switzerland.

5. Figure 1 is very simple and having no titles?
6. The graphs are very simple; it would be better to compare your work to the state-of-the-art.
7. Minor typos and grammatical mistakes need correction before revision.",Authentic,"Especially if artificial intelligence (AI)-supported decisions affect the society, the fairness of such AI-based methodologies constitutes an important area of research. In this contribution, we investigate the applications of AI to the socioeconomically relevant infrastructure of water distribution systems (WDSs). We propose an appropriate definition of protected groups in WDSs and generalized definitions of group fairness, applicable even to multiple non-binary sensitive features, that provably coincide with existing definitions for a single binary sensitive feature. We demonstrate that typical methods for the detection of leakages in WDSs are unfair in this sense. Further, we thus propose a general fairness-enhancing framework as an extension of the specific leakage detection pipeline, but also for an arbitrary learning scheme, to increase the fairness of the AI-based algorithm. Finally, we evaluate and compare several specific instantiations of this framework on a toy and on a realistic WDS to show their utility."
Ensemble learning approach for distinguishing human and computer-generated Arabic reviews,"Authors have proposed a study to classify human and computer-generated Arabic reviews, with the aim of detecting fake reviews produced by computers.
This is an interesting study ,however there are some points that need to addressed strictly before decising about its acceptance. May address the following points.

Clarify the methodology section by providing more details on the feature selection process
Provide more context on the limitations of the study
In the results section, provide additional statistical analysis to support the observed differences in lexical and emotional features between human-generated and machine-generated reviews",Generic,"While customer reviews are crucial for businesses to maintain their standing in the marketplace, some may employ humans to create favorable reviews for their benefit. However, advances in artificial intelligence have made it less complex to create these reviews, which now rival real ones written by humans. This poses a significant challenge in distinguishing between genuine and artificially generated reviews, thereby impacting consumer trust and decision-making processes. Research has been conducted to classify whether English reviews were authored by humans or computers. However, there is a notable scarcity of similar studies conducted in Arabic. Moreover, the potential of ensemble learning (EL) techniques, such as soft voting, to enhance model performance remains underexplored. This study conducts a comprehensive empirical analysis using various models, including traditional machine learning, deep learning, and transformers, with an investigation into ensemble techniques, like soft voting, to classify human and computer-generated Arabic reviews. Integrating top logistic regression (LR) and convolutional neural network (CNN) models, it achieves an accuracy of 89.70%, akin to AraBERT’s 90.0%. Additionally, a thorough textual analysis, covering parts of speech (POS), emotions, and linguistics reveals significant linguistic disparities between human and computer-generated reviews. Notably, computer-generated reviews exhibit a substantially higher proportion of adjectives (6.3%) compared to human reviews (0.46%), providing crucial insights for discerning between the two review types. The results not only advance natural language processing (NLP) in Arabic but also have significant implications for businesses combating the influence of fake reviews on consumer trust and decision-making."
Optimal robust configuration in cloud environment based on heuristic optimization algorithm,"This paper addresses the following three questions:
1. How to model the cloud environment with perturbations?
2. How to conduct robustness metrics on the model?
3. How to determine the optimal configuration to ensure robustness?

While the paper appears to be well-researched and includes notable contributions, it is challenging to follow. The detailed technical aspects can obscure the main problem statement and the primary contribution of the paper.

The paper's abstract fails to adequately describe the methodology of the proposed techniques, including how they improve upon existing methods to yield better results. Additionally, the abstract lacks examples of results. Furthermore, the abstract does not explain what ESOA and WOA methods are or why they were selected in particular.

In terms of system modeling, it is not clear whether the model presented is the authors' contribution or taken from existing literature. At certain points, the authors reference that it has been introduced in other publications, such as the waiting time and cost modeling. If these models are novel contributions from the authors, further explanation on the reasoning behind the equations and the authors' specific contributions should be provided.

In the problem description section, it is mentioned that, to the best of the authors' knowledge, perturbation factors have not been previously discussed in the modeling and solving process of the problem presented in the paper. The paper intends to describe the impacts of specific perturbation factors on the profit of cloud service providers and the waiting time of customers. It considers server size and speed as key factors. However, it is unclear why these two factors were chosen specifically, and what is meant by server size. The paper then sets values for some parameters, such as ""a"" and lambda, without explaining the reasons for choosing these values, which could affect the analysis. Additionally, in this section, particularly in Figure 2a, the caption may be incorrect as it indicates waiting time rather than profit.",Authentic,"To analyze performance in cloud computing, some unpredictable perturbations that may lead to performance degradation are essential factors that should not be neglected. To prevent performance degradation in cloud computing systems, it is reasonable to measure the impact of the perturbations and propose a robust configuration strategy to maintain the performance of the system at an acceptable level. In this article, unlike previous research focusing on profit maximization and waiting time minimization, our study starts with the bottom line of expected performance degradation due to perturbation. The bottom line is quantified as the minimum acceptable profit and the maximum acceptable waiting time, and then the corresponding feasible region is defined. By comparing between the system’s actual working performance and the bottom line, the concept of robustness is invoked as a guiding basis for configuring server size and speed in feasible regions, so that the performance of the cloud computing system can be maintained at an acceptable level when perturbed. Subsequently, to improve the robustness of the system as much as possible, discuss the robustness measurement method. A heuristic optimization algorithm is proposed and compared with other heuristic optimization algorithms to verify the performance of the algorithm. Experimental results show that the magnitude error of the solution of our algorithm compared with the most advanced benchmark scheme is on the order of 10 ?? , indicating the accuracy of our solution."
Automatic spread factor and position definition for UAV gateway through computational intelligence approach to maximize signal-to-noise ratio in wooded environments,"1. Within the scope of the study, it was aimed to determine the best position and optimal spreading factor for Unmanned Aerial Vehicles.

2. In the Introduction section, Long-Range technology and the importance of the subject are mentioned in detail and sufficiently.

3. In the Related works section, existing studies in the literature in this field and their main contributions are basically mentioned.

4. The dataset, number of samples and SNR values used in the study are sufficient.

5. Multi-Layer Perceptron Network and General Regression Neural Networks were used in the study to predict signal behavior. Although there are different neural networks that can be used for this process in the literature, it should be explained in more detail why these two models are preferred.

6. For the optimizer, bioinspired Grey-Wolf optimizer was preferred in the study. Although the optimizer chosen to solve the problem is suitable, explain why this optimizer is preferred and the reasons why a different optimizer is not used and/or preferred.

7. The results and RMSE obtained as a result of the study are sufficient.

8. It is recommended that the future work section in the Conclusion section be detailed a little more.

As a result, although the study is sufficient in terms of the problem and originality addressed, it is recommended to examine the parts mentioned above.",Authentic,"The emergence of long-range (LoRa) technology, together with the expansion of uncrewed aerial vehicles (UAVs) use in civil applications have brought significant advances to the Internet of Things (IoT) field. In this way, these technologies are used together in different scenarios, especially when it is necessary to have connectivity in remote and difficult-to-access locations, providing coverage and monitoring of greater areas. In this sense, this article seeks to determine the best positioning for the LoRa gateway coupled to the drone and the optimal spreading factor (SF) for signal transmission in a LoRa network, aiming to improve the connected devices signal-to-noise ratio (SNR), considering a suburban and densely wooded environment. Then, multi-layer perceptron (MLP) networks and generalized regression neural networks (GRNN) were trained to predict the signal behavior and determine the best network to represent this behavior. The MLP network presented the lowest RMSE, 2.41 dB, and was selected for use jointly with the bioinspired Grey-Wolf optimizer (GWO). The optimizer proved its effectiviness being able to adjust the number of UAVs used to obtain 100% coverage and determine the best SF used by the endnodes, guaranteeing a higher transmission rate and lower energy consumption."
Enhancing energy balance in wireless sensor networks through optimized minimum spanning tree,"The paper should discuss the scalability of the proposed solution and its feasibility in real-world deployments. Insights into computational overheads, real-time applicability, and compatibility with existing network protocols would be valuable.

While the paper briefly mentions future work, a more detailed discussion on the limitations of the current study would be useful. This might include potential challenges in implementing the algorithm in highly dynamic environments or under varying environmental conditions.

Additional graphical representations and visualizations of the network operations, energy distributions, and before-and-after scenarios with the application of the MST could enhance the comprehensibility and impact of the results.

Information about the simulation tools and software used, along with the settings for Prim’s algorithm implementation, would aid in replicability and validation of the study by other researchers.

Overall, the research presents a promising approach to addressing energy imbalances in wireless sensor networks, with significant implications for enhancing network efficiency and sustainability. Further detailing in certain aspects of the study could make the article even more robust and impactful.",Generic,"Wireless sensor networks (WSNs) are important for applications like environmental monitoring and industrial automation. However, the limited energy resources of sensor nodes pose a significant challenge to the network’s longevity. Energy imbalances among nodes often result in premature failures and reduced overall network lifespan. Current solutions have not adequately addressed this issue due to network dynamics, varying energy consumption rates, and uneven node distribution. To tackle this, we propose a novel method using Prim’s algorithm to construct minimum spanning trees (MSTs) that enhance energy balance in WSNs. Prim’s algorithm effectively identifies optimal connections among network nodes to minimize energy consumption. Our methodology includes several key steps: network initialization, energy consumption modeling, MST construction using Prim’s algorithm, and optimizing the movement of mobile sink nodes. Extensive experiments with diverse datasets show that our approach significantly improves energy equilibrium, demonstrating high sensitivity and moderate complexity. This research underscores the potential of Prim’s algorithm to extend the lifespan of WSNs and enhance energy efficiency, contributing to sustainable and effective network deployments."
Enhancing machine learning-based forecasting of chronic renal disease with explainable AI,"1. Line 68: The sentence ""After rigorous tests using…"" is incomplete.
2. Line 83: Please spell out the full name of ""DL"" since it's the first time it is mentioned.
3. Lines 108–115: The description of the dataset is insufficient. Please provide more information, such as a statistical summary including the value for each attribute, the count of each attribute's records, and the count of CRD positive/negative for each value. What is the total count (N) of the dataset? In lines 110-111, is 400 the count? What does the class attribute mean? Please provide further explanation. In line 114, what does 158 mean? Does it mean 158 records out of all records have missing values? Or are there only 158 fields in the whole dataset with missing values? Please provide further explanation.
4. Lines 174–185: One of the possible effects of using SMOTE is overfitting. Is an accuracy of 99.07% indicative of overfitting? Please add the number of records used in the model before and after SMOTE.
5. Line 190: The reviewer suggests adding evaluation results before/after parameter tuning or for each potential parameter combination to show the difference and improvement of results using GridSearchCV. Taking accuracy as an example, before parameter tuning, the accuracy is 92%, and after finding the best parameter, the accuracy is 99%. These numbers will intuitively show the effect of parameter tuning.
6. Line 306: Again, please provide the number of records in the dataset. How many for the training set and how many for the testing set?
7. Table 4: What are ""TN, FP, FN, TP""? They are not explained in the manuscript.
8. Figure 8: This figure doesn't provide much meaningful interpretation. The height of each group of bars is similar and hard to differentiate with the naked eye. The values are not shown in the bar chart. If not presenting the same evaluation parameter of each classifier together (for example, putting the precision of the 6 classifiers together), it's really hard to discern the difference and make comparisons.
9. Lines 357–358: Do the four classifiers give the exact same accuracy?
10. Lines 395–402: The meaning of the illustration in this paragraph is obvious. But the description doesn't explain Figure 11 well. Please further explain.
11. Lines 423–433: The description of Figures 14 and 15 is insufficient. In lines 427–428, what does ""orange bar represents a positive prediction"" mean? Why would there be a negative value when predicting a positive value? What do the numbers in the table mean? What's their significance? In the bar chart, what do the numbers on the bars mean (e.g., in Fig.14, 0.12 of specific gravity)? What does the number of the feature threshold mean (e.g., in Fig.14, <=-0.42 of specific gravity)? What do the positive and negative signs in these values mean? What do the numbers in the legend represent in the chart/table? Why is there no color for the legend in Fig. 15? Please further address the above issues.
12. Lines 434–452: The results show certain features are important for CRD prediction. Does this result align with research experience in the medical field? Is there any validation with practical medical research or experience?",Authentic,"Chronic renal disease (CRD) is a significant concern in the field of healthcare, highlighting the crucial need of early and accurate prediction in order to provide prompt treatments and enhance patient outcomes. This article presents an end-to-end predictive model for the binary classification of CRD in healthcare, addressing the crucial need for early and accurate predictions to enhance patient outcomes. Through hyperparameter optimization using GridSearchCV, we significantly improve model performance. Leveraging a range of machine learning (ML) techniques, our approach achieves a high predictive accuracy of 99.07% for random forest, extra trees classifier, logistic regression with L2 penalty, and artificial neural networks (ANN). Through rigorous evaluation, the logistic regression with L2 penalty emerges as the top performer, demonstrating consistent performance. Moreover, integration of Explainable Artificial Intelligence (XAI) techniques, such as Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), enhances interpretability and reveals insights into model decision-making. By emphasizing an end-to-end model development process, from data collection to deployment, our system enables real-time predictions and informed healthcare decisions. This comprehensive approach underscores the potential of predictive modeling in healthcare to optimize clinical decision-making and improve patient care outcomes."
The role of Internet of Things (IoT) technology in modern cultivation for the implementation of greenhouses,"In this paper, the author performs a review of Internet of Things technology in Modern Cultivation for the Implementation of Greenhouses.

The paper presentation needs special attention. Please revise your paper according to the below suggestions.
1) The abstract and the conclusion section should be more polished. The future work should be the part of conclusion section.
2) The paper organization missing at the end of the paper. Please mention this at the end of the introduction section.
3) The contribution should be mentioned in bullets.
4) Section 2 state of the art of related work is missing. Please add this section, and cite the most recent state of the art in this area. In addition, the difference between state-of-the-art and your research should be mentioned in this section.

5) Please reduce section 3 and remove unnecessary subsections, i.e. 2.10.1 Networks and Technologies. It is advised to add the most recent studies in other sections.
6) The section is shortened, please increase by adding more studies in this section.
7) Polish challenges section.
8) Check English and grammar for typos and mistakes.",Authentic,"In recent years, the Internet of Things (IoT) has become one of the most familiar names creating a benchmark and scaling new heights. IoT an indeed future of the communication that has transformed the objects (things) of the real world into smarter devices. With the advent of IoT technology, this decade is witnessing a transformation from traditional agriculture approaches to the most advanced ones. Limited research has been carried out in this direction. Thus, herein we present various technological aspects involved in IoT-based cultivation. The role and the key components of smart farming using IoT were examined, with a focus on network technologies, including layers, protocols, topologies, network architecture, etc. We also delve into the integration of relevant technologies such as cloud computing, big data analytics, and the integration of IoT-based cultivation. We explored various security issues in modern IoT cultivation and also emphasized the importance of safeguarding sensitive agricultural data. Additionally, a comprehensive list of applications based on sensors and mobile devices is provided, offering refined solutions for greenhouse management. The principles and regulations established by different countries for IoT-based cultivation systems are presented, demonstrating the global recognition of these technologies. Furthermore, a selection of successful use cases and real-world scenarios and applications were presented. Finally, the open research challenges and solutions in modern IoT-based cultivation were discussed."
Modelling the effects of perceived system quality and personal innovativeness on the intention to use metaverse: a structural equation modelling approach,"This is a well-written paper that covers a timely and interesting topic. However, there are a few suggestions to improve the quality of this good work:

Introduction Section: The introduction is too short and should be extended. Focus on providing a clearer research problematization and emphasize the main contributions of your work. This will help readers understand the significance and context of your study from the outset.

Paper Structure: It is important to add a paragraph at the end of the introduction section to outline the structure of the paper. This will guide readers through the organization of your manuscript and set clear expectations.

Methodology Section: More details are required regarding the sample population, sampling technique, and the adequacy of the sample size. Providing this information will enhance the transparency and replicability of your research.

Theoretical Foundation and Hypotheses Development: Strengthen the theoretical foundation and hypotheses development by incorporating recent and related research on the metaverse. This will provide a more robust conceptual framework for your study and align it with current scholarly discourse.
Discussion Section: Utilize the incorporated studies mentioned in the theoretical foundation to strengthen the discussion section. This will help in contextualizing your findings within the broader literature and highlight the implications of your research.

By addressing these suggestions, the overall quality and impact of your paper will be significantly improved.",Authentic,"The metaverse, an interactive and immersive 3D virtual environment, has recently become popular and is widely used in several fields, including education. However, the successful use of metaverse relies on the extent to which users intend to adopt and use it. Close examination of this critical issue reveals a lack of research that examines the effects of certain factors on users’ intentions toward using metaverses. Thus, this study extends the technology acceptance model by integrating two constructs—perceived system quality and students’ personal innovativeness. Using a survey to collect data, 164 responses were received from students at the University of Ha’il in Saudi Arabia. Two steps in structural equation modelling (SEM) using the AMOS software were applied to analyse the data and test the research hypotheses. The results revealed that perceived system quality had a significant effect on students’ intentions to use metaverses through perceived ease of use. Furthermore, personal innovativeness had a significant effect on students’ intentions through the perceived usefulness of the metaverse. In addition, perceived usefulness affected students’ intentions to use a metaverse. Surprisingly, perceived ease of use had an insignificant effect on students’ intentions to use the metaverse. Although the proposed model and its findings contribute to the technology acceptance model (TAM) literature, the study’s practical value is significant because it can help educational policymakers and authorities to understand the effect of each factor and plan future strategies. Additionally, the findings of this study can assist practitioners, designers, and developers in designing and promoting the utilisation of metaverses."
Enabling personalized smart tourism with location-based social networks,"The study presents original primary research within the scope of the journal. The research question is well-defined, relevant, and meaningful. The authors clearly state how their research addresses an identified knowledge gap in the literature on smart tourism and LBSNs.

The methodology is described in sufficient detail to allow replication. The authors use a robust experimental design, incorporating both representation learning and contrastive learning techniques. However, the explanation of some methods could be elaborated for better understanding. For example, the process of how the contextual information from LBSNs is embedded could be detailed further.

The investigation is performed to a high technical and ethical standard. There is no indication of ethical concerns in the data collection or analysis procedures.


The data provided are robust, statistically sound, and well-controlled. The authors employ rigorous statistical methods to analyze the data, ensuring that the results are reliable.

The conclusions are well-stated and directly linked to the original research question. They are supported by the results presented, providing a clear answer to the research question. The authors successfully limit their conclusions to what their data can support, avoiding overgeneralization.

Strengths:
- The integration of LBSNs with deep learning techniques for personalized smart tourism is innovative and addresses a significant gap in the literature.
- The study uses a comprehensive and methodologically sound approach, enhancing the reliability of the findings.
- The manuscript is well-structured and clearly written, facilitating understanding.

Weaknesses:
- Minor grammatical errors and awkward phrasings should be addressed to improve clarity.
- The explanation of some methodological aspects could be expanded for better comprehensibility.
- Ensure all figures and tables are correctly cited in the text.",Authentic,"With the rapid advance of mobile internet, communication technology and the Internet of Things (IoT), the tourism industry is undergoing unprecedented transformation. Smart tourism offers users personalized and customized services for travel planning and recommendations. Location-based social networks (LBSNs) play a crucial role in smart tourism industry by providing abundant data sources through their social networking attributes. However, applying LBSNs to smart tourism is a challenge due to the need to deal with complex multi-source information modeling and tourism data sparsity. In this article, to fully harness the potential of LBSNs using deep learning technologies, we propose an knowledge-driven personalized recommendation method for smart tourism. Representation learning techniques can effectively modeling the contextual information ( e.g. , time, space, and semantics) in LBSNs, while the data augmentation strategy of contrastive learning techniques can explore user personalized travel behaviors and alleviate data sparsity. To demonstrate the effectiveness of the proposed approach, we conducted a case study on trip recommendation. Furthermore, the patterns of human mobility are revealed by exploring the effect of contextual data and tourist potential preferences."
Pre-touch reaction is preferred over post-touch reaction in interaction with displayed agent,"The article aims at investigating how different types of human-machine interaction methodologies (pre-touch or post-touch reactions) affect the user's perception of an avatar in the context of display-based (2D) agents.
The article is well structured and the research question is clear. Despite that, several sections result to be poorly written [1-2], confusing [3-4] or trivial [5] and could be simplified.
Moreover, the authors did not mention how they dealt with the issue presented at line 47 [6]. It results unclear whether they decided to consider the distance between user hand and avatar face or between user hand and display physical boundary. In fact, at line 144, the authors argue that the UltraLeap sensor ""calculates the distance between the display surfaces [...] and the detected hands of the participants."" Thus implying that the distance measured is between user hand and display.
However, in Figure 3, it looks like the only difference between the 20cm condition and the 45cm condition is the distance of the agent within the virtual environment, while the user hand is at the same distance with respect to the screen.
For a better understanding and reproducibility of the experiment, it should be clarified:
a) which distance was actually measured,
b) what is the rationale that motivated the authors to measure one distance over the other.

Moreover, the objective of the ""Implications"" section should be stated more clearly, as it currently enumerates a series of state-of-the-art issues (e.g. define a logic to manage pre/post touch behavior according to the position of the agent [7], pre-touch reactions in teleoperated avatars [8], etc.) that are not addressed in the research and that would better fit the ""Related works"" section.

[1] (line 20) ""To design the pre-touch reaction of such an agent, we focused on the display's physical boundary as a criterion for pre-touch by agents on the display.""
[2] (line 89) ""For example, Shiomi et al. modeled the pre-touch reaction distance around the face from a data collection with people and implemented a model to investigate the importance of pre-touch reaction distance toward a social robot (Shiomi et al. 2018b).""
[3] (line 78) ""Moreover, past studies have focused on spatial interaction with such displayed agents, e.g., a scene where people encountered the displayed agent, and reported how their interaction distances differed based on the people's impressions toward the displayed agents (Cafaro et al. 2016).""
[4] (line 278) ""The teleoperated avatar's pre-touch reaction resembles a visualization of remote touch interaction in distant locations.""
[5] (line 46) ""One notable difference between the displayed agents and robots/agents is the existence of a display.""
[6] (line 47) ""Unlike robots in physical environments and agents in VR environments, it is unclear whether the criterion for the pre-touch reaction distance to which the display agents should respond is the distance from the face on the displayed image or to the display's physical boundary.""
[7] (line 268) ""Modifying the reaction behaviors, depending on the places that are actually touched would be useful for refusing excessive or 'bad' touches, as suggested by a past study (Kimoto & Shiomi 2024).""
[8] (line 277) ""From another perspective, a pre-touch reaction is useful for a teleoperated avatar to increase the perceptions of being touched by the operator.""",Authentic,"A pre-touch reaction, which is a response before a physical contact, is an essential factor for natural human-agent interaction. Although numerous studies have investigated the effectiveness of pre-touch reaction design for virtual agents in virtual reality (VR) environments and robots in physical environments, one area remains underexplored: displayed agents, i.e., on-screen computer graphics agents. To design an appropriate pre-touch reaction for such a displayed agent, this article focused on the display’s physical boundary as a criterion for the pre-touch reaction of the agent. This article developed a displayed agent system that can detect both the touch events on the screen and the pre-touch behaviors of the interacting people around the display. This study examined the effectiveness of the pre-touch reactions of the displayed agent by the developed system in experiments with human participants. The findings revealed that people significantly preferred pre-touch reactions over post-touch reactions in the context of perceived feelings."
Developing a tablet-based brain-computer interface and robotic prototype for upper limb rehabilitation,"The writing is clear and easy to follow, with professional English used throughout. The literature references and field background/context are sufficient. The article structure is professional, but it would be beneficial to share raw data. The results are self-contained and relevant to the hypothesis.

1. The research question is well-defined and relevant to the journal's Aims and Scope, exploring the integration of BCI technology with a virtual end-effector robot in a game setting for upper limb dysfunctions. The research fills an identified knowledge gap in the field.
2. The investigation is original primary research, but the sample size of 12 subjects seems a bit small. Consider using open datasets as a secondary dataset to further validate the results.
3. The methods are described with sufficient detail, but it would be beneficial to include more information to replicate the experiment.

1. The data provided is robust, but the use of LDA for classification without exploring other baseline models may limit the validity of the findings.
2. The results are statistically sound, but the conclusions may be overstated given the moderate accuracy of around 55-70%.",Generic,"Background
The current study explores the integration of a motor imagery (MI)-based BCI system with robotic rehabilitation designed for upper limb function recovery in stroke patients.

Methods
We developed a tablet deployable BCI control of the virtual iTbot for ease of use. Twelve right-handed healthy adults participated in this study, which involved a novel BCI training approach incorporating tactile vibration stimulation during MI tasks. The experiment utilized EEG signals captured via a gel-free cap, processed through various stages including signal verification, training, and testing. The training involved MI tasks with concurrent vibrotactile stimulation, utilizing common spatial pattern (CSP) training and linear discriminant analysis (LDA) for signal classification. The testing stage introduced a real-time feedback system and a virtual game environment where participants controlled a virtual iTbot robot.

Results
Results showed varying accuracies in motor intention detection across participants, with an average true positive rate of 63.33% in classifying MI signals.

Discussion
The study highlights the potential of MI-based BCI in robotic rehabilitation, particularly in terms of engagement and personalization. The findings underscore the feasibility of BCI technology in rehabilitation and its potential use for stroke survivors with upper limb dysfunctions."
Optimized virtual reality design through user immersion level detection with novel feature fusion and explainable artificial intelligence,"This paper aims to detect user immersion levels in VR using an efficient machine-learning model. The authors utilized a benchmark dataset based on user experiences in VR environments to conduct our experiments. a novel technique called Polynomial Random Forest (PRF) was proposed for feature engineering mechanisms. Extensive research experiments show that random forest outperformed state-of-the-art studies, achieving a high-performance score of 98%, using the proposed PRF technique. The paper is well organized and written. However, there are some issues to be clarified.
1. How to evaluate immersion, apart from subjective feelings, is there an objective evaluation criteria.
2. Are there any similar research data sets in the world? The data set in this paper includes 1000 samples, which is a small number and whether it covers enough usage cases and scenarios.
3. It is recommended to add references to the method of comparison.
4. For the innovative method PRF proposed in this paper, it is suggested to add pictures to represent the innovation of the algorithm.",Generic,"Virtual reality (VR) and immersive technology have emerged as powerful tools with numerous applications. VR technology creates a computer-generated simulation that immerses users in a virtual environment, providing a highly realistic and interactive experience. This technology finds applications in various fields, including gaming, healthcare, education, architecture, and training simulations. Understanding user immersion levels in VR is crucial and challenging for optimizing the design of VR applications. Immersion refers to the extent to which users feel absorbed and engrossed in the virtual environment. This research primarily aims to detect user immersion levels in VR using an efficient machine-learning model. We utilized a benchmark dataset based on user experiences in VR environments to conduct our experiments. Advanced deep and machine learning approaches are applied in comparison. We proposed a novel technique called Polynomial Random Forest (PRF) for feature generation mechanisms. The proposed PRF approach extracts polynomial and class prediction probability features to generate a new feature set. Extensive research experiments show that random forest outperformed state-of-the-art approaches, achieving a high immersion level detection rate of 98%, using the proposed PRF technique. We applied hyperparameter optimization and cross-validation approaches to validate the performance scores. Additionally, we utilized explainable artificial intelligence (XAI) to interpret the reasoning behind the decisions made by the proposed model for user immersion level detection in VR. Our research has the potential to revolutionize user immersion level detection in VR, enhancing the design process."
Pedagogical sentiment analysis based on the BERT-CNN-BiGRU-attention model in the context of intercultural communication barriers,"This model utilizes BERT models to extract semantic and syntactic features from text, and also incorporates convolutional neural networks (CNN) and bidirectional gated loop unit (GRU) networks to study text features more deeply, thereby enhancing the model's proficiency in recognizing nuanced emotions. However, there are still several points that need to be improved:
1. The introduction should weave the topic together with the research background, strengthen the connection between the introduction and the research background;
2. The basic principle and procedure of how CNN and BiGRU compose dual channel need to be explained in more detail;
3. The formula in Chapter 3 lacks a detailed description of its parameters;
4. In the experimental chapter, P,R and F values are introduced in detail, but other evaluation indicators, such as confusion matrix, seem to be missing;
5. In the article, there are many abbreviated names for the model, pay attention to check the completeness;
6. Add descriptive statistics for the data set used in this study. Also provide details about the availability and origin of the data set.",Generic,"Amid the wave of globalization, the phenomenon of cultural amalgamation has surged in frequency, bringing to the fore the heightened prominence of challenges inherent in cross-cultural communication. To address these challenges, contemporary research has shifted its focus to human-computer dialogue. Especially in the educational paradigm of human-computer dialogue, analysing emotion recognition in user dialogues is particularly important. Accurately identify and understand users' emotional tendencies and the efficiency and experience of human-computer interaction and play. This study aims to improve the capability of language emotion recognition in human-computer dialogue. It proposes a hybrid model (BCBA) based on bidirectional encoder representations from transformers (BERT), convolutional neural networks (CNN), bidirectional gated recurrent units (BiGRU), and the attention mechanism. This model leverages the BERT model to extract semantic and syntactic features from the text. Simultaneously, it integrates CNN and BiGRU networks to delve deeper into textual features, enhancing the model's proficiency in nuanced sentiment recognition. Furthermore, by introducing the attention mechanism, the model can assign different weights to words based on their emotional tendencies. This enables it to prioritize words with discernible emotional inclinations for more precise sentiment analysis. The BCBA model has achieved remarkable results in emotion recognition and classification tasks through experimental validation on two datasets. The model has significantly improved both accuracy and F1 scores, with an average accuracy of 0.84 and an average F1 score of 0.8. The confusion matrix analysis reveals a minimal classification error rate for this model. Additionally, as the number of iterations increases, the model's recall rate stabilizes at approximately 0.7. This accomplishment demonstrates the model's robust capabilities in semantic understanding and sentiment analysis and showcases its advantages in handling emotional characteristics in language expressions within a cross-cultural context. The BCBA model proposed in this study provides effective technical support for emotion recognition in human-computer dialogue, which is of great significance for building more intelligent and user-friendly human-computer interaction systems. In the future, we will continue to optimize the model's structure, improve its capability in handling complex emotions and cross-lingual emotion recognition, and explore applying the model to more practical scenarios to further promote the development and application of human-computer dialogue technology."
A model based LSTM and graph convolutional network for stock trend prediction,"It is known that changes in stock prices in the same sector can follow a parallel course. However, although their dependencies cannot be observed directly and they are not in the same sector, some stocks can be quite synchronous in the short term. Research on how to use these relationships between stocks has been an interesting research topic in recent years.

In this article, LSTM was chosen to extract stock time series information and GCN was chosen to extract stock relationship information. Therefore, what is done in this article is actually to integrate stock relationships into stock price prediction. Thus, this integration is intended to enable the prediction of future stock prices and support investors in making investment decisions.

Although the integrated GCN-LSTM method is a method used in different fields, it has not been used much in stock price prediction. Zhao et al. (2023), which the authors also refer to in their study, is probably an exemplary study in this field in recent years.

In my opinion, the purpose, methodology and results of the authors' study should be compared with this literature in question. It should differ from them at some points and make itself unique. If necessary, the study should be revised again with new and original contributions for separation.

I expect convincing answers and satisfactory methodological revisions from the authors on these issues.

Although the integrated GCN-LSTM method is a method used in different fields, it has not been used much in stock price prediction. Zhao et al. (2023), which the authors also refer to in their study, is probably an exemplary study in this field in recent years.",Generic,"Stock market is a complex system characterized by collective activity, where interdependencies between stocks have a significant influence on stock price trends. It is widely believed that modeling these dependencies can improve the accuracy of stock trend prediction and enable investors to earn more stable profits. However, these dependencies are not directly observable and need to be analyzed from stock data. In this paper, we propose a model based on Long short-term memory (LSTM) and graph convolutional network to capture these dependencies for stock trend prediction. Specifically, an LSTM is employed to extract the stock features, with all hidden state outputs utilized to construct the graph nodes. Subsequently, Pearson correlation coefficient is used to organize the stock features into a graph structure. Finally, a graph convolutional network is applied to extract the relevant features for accurate stock trend prediction. Experiments based on China A50 stocks demonstrate that our proposed model outperforms baseline methods in terms of prediction performance and trading backtest returns. In trading backtest, we have identified a set of effective trading strategies as part of the trading plan. Based on China A50 stocks, our proposed model shows promising results in generating desirable returns during both upward and downward channels of the stock market. The proposed model has proven beneficial for investors to seeking optimal timing and pricing when dealing with shares."
Linguacodus: a synergistic framework for transformative code generation in machine learning pipelines,"The authors should improve to improve overall organization of the manuscript.
Please arrange the figures in more professional way and not just copy/paste presentation format. Resolution of the figures should be improved. Try to use linker for Tables/Figures/References in the text to enhance readability. Try to use less paragraphs if not actually required, like say in ""Conclusion"" section. It impedes readability.
The authors discussed limitations of prior works in ""Related work"" section and again in a separate section as ""Discussion"" as a comparative analysis. While this is relevant but appears as dis-joint and repetitive. It will be more appropriate to present in the ""Discussion"" section only as a comparative analysis against cited previous research works [for each feature/points] in a tabular format (not mandatory), how the proposed approach outperforms or performing as per.
""Limitations"" should be discussed as a separate section. What will be the future directives?

Research motivation, Proposed methodology and training criterion are well discussed and well presented.
Figures helped better understandability of each stages in ""instruction creation"" section.

The proof-of-validation is satisfactory to support proposed claims. However, authors need to provide additional details as the ""score"" metrics for proposed ""Linguacodus"" and that of GPT3.5, is the score ""higher"" is more accurate or the ""lower"", as from the percentile also, it appears GPT3.5 is outperforming the proposed approach, say for competition C3, ""Linguacodus"" percentile 0 against GPT3.5 21, for C4, the former 58 against the later 81 and so on. Please justify ""Linguacodus consistently produces compilable code, outperforming vanilla 298 GPT-3.5 solutions across specified machine learning metrics"".

Similarly, for the time comparison as ""This approach minimizes the time (it takes less than 1 minute to generate a solution)"".... did you compare with GPT 3.5? Please include time comparison metric as well.

If possible, please modify the Appendix C, and listings C1-C4. for at least 2 competitions, a side-by-side tabular format comparison, for --> ""examples of code inferred by GPT-3.5 with two variations of task-describing prompts: one with and one without the automatically chosen best instruction."" ….It will improve readability of the manuscript and experimental findings as well as help extended research community to better understand the ""differentiation"". Please make a table as [Left column] without and [Right column] with...........feel free to use any other presentable format ..",Authentic,"In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across diverse domains. Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction. In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus. The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.
Subjects:	Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL); Software Engineering (cs.SE)
Cite as:	arXiv:2403.11585 [cs.LG]
 	(or arXiv:2403.11585v2 [cs.LG] for this version)
 
https://doi.org/10.48550/arXiv.2403.11585
Focus to learn more"
Improving synthetic media generation and detection using generative adversarial networks,"The authors present an improved GAN model for human faces. The data processing pipeline consists of image preprocessing, dataset splitting, image augmentation and data processing. A publicly available dataset, specifically Flickr-Faces, was used for the experiments and validation of the improved model. The title of the article does not fit the content of the paper. Whereas the title is holistic and rather leads to the assumption that it portrays a systematic review, the article itself is very specific by dealing with synthetic data for human face features. We would recommend specifying the title.
The structure of the article is coherent. The introduction gives an understandable motivation for the need for action. However, the introduction is repetitive as the functionality and structure of a GAN is explained multiple times (ll.40, ll.45 and ll.51) and the state of the art is anticipated superficially (ll.58-ll.60).
The reviewers highly recommend the authors improve the language as well as formal aspects. The introduction of abbreviations is conducted multiple times in an unstructured way (e.g. ll. 40, ll.45, ll.13, ll.51, ll.67, ll83). Sentences to not finish sensibly (ll.56, ll.178), upper and lower case are incorrect (ll. 66). Sentences are grammatically incorrect. Lastly, the use of space between words, references and remarks in brackets is not consistent (e.g. ll.44, ll.88, ll.128, ll. 156).
References in the state of the art should be improved by current research. Works from 2023 and 2022 relating to GAN are missing. The reference style is completely wrong. Please refer to the guidelines for authors.
The graphs fit the text sections and underline the findings. Nevertheless, they need some improvements. Specifically coherent writing and naming, colours, and layout. Graphs should not possess the same name, e.g. Fig. 4 Receiver Operating Characteristic (ROC) Curve or Fig. 8 generator Loss and Disciminator Loss. The quality of images is also to be improved as some parts of the images are cut off. For the visualization of architecture in Fig. 2, we propose known techniques as opposed to a flow chart.",Generic,"Synthetic images ar---e created using computer graphics modeling and artificial intelligence techniques, referred to as deepfakes. They modify human features by using generative models and deep learning algorithms, posing risks violations of social media regulations and spread false information. To address these concerns, the study proposed an improved generative adversarial network (GAN) model which improves accuracy while differentiating between real and fake images focusing on data augmentation and label smoothing strategies for GAN training. The study utilizes a dataset containing human faces and employs DCGAN (deep convolutional generative adversarial network) as the base model. In comparison with the traditional GANs, the proposed GAN outperform in terms of frequently used metrics i.e., Fréchet Inception Distance (FID) and accuracy. The model effectiveness is demonstrated through evaluation on the Flickr-Faces Nvidia dataset and Fakefaces d--ataset, achieving an FID score of 55.67, an accuracy of 98.82%, and an F1-score of 0.99 in detection. This study optimizes the model parameters to achieve optimal parameter settings. This study fine-tune the model parameters to reach optimal settings, thereby reducing risks in synthetic image generation. The article introduces an effective framework for both image manipulation and detection."
Terrorism group prediction using feature combination and BiGRU with self-attention mechanism,"The paper entitled “Terrorism group prediction using combined features and BiGRU with self-attention mechanism” proposed a framework for classifying and predicting terrorist groups using a Bi-directional Gated Recurrent Unit (BiGRU) enhanced with a self-attention mechanism. It leverages combined features from the Global Terrorism Database, employing advanced Machine Learning and Deep Learning techniques for improved prediction accuracy. The framework, termed BiGRU-SA, demonstrates exceptional efficacy in classifying 36 terrorist organizations and achieved promising results; however, my comments are below:

Introduction:
1. Peerj computer science do not support section numbering, please address the sections numbering.
2. Please check the appropriate citation style for this journal. As far as I know, it is author-year. The manuscript uses only “author (year)” citations throughout, and no parenthetical citations “(author, year).” This format should only be used when the cited work forms part of the text, e.g., “author (year) proposes method X” and the latter otherwise, e.g., “we method differs from method X (author, year).”
3. Figure 1 has no scientific value; I would suggest removing it.
4. I would suggest summarizing the introduction in 3 pages maximum and avoiding the use of subsections.
5. Is better to introduce the “Problem formulation” in a separate section.
6. All the techniques and methods that are not proposed in this work such as GRU, EDA, PCC, etc. it should be cited.
7. The list of contribution should be rewritten, to the best of your understanding, “Pre-processing Policies for Data Integrity, Combination of Relevant Features, and Focused Analysis on High-Impact Groups” are not contributions.
Background and related work:

8. There are citation issues in lines 195, 200, 209, 228, 230, etc.
9. The authors repeatedly introduce the abbreviation, for example, GRU in lines 272 and 1095.",Authentic,"The world faces the ongoing challenge of terrorism and extremism, which threaten the stability of nations, the security of their citizens, and the integrity of political, economic, and social systems. Given the complexity and multifaceted nature of this phenomenon, combating it requires a collective effort, with tailored methods to address its various aspects. Identifying the terrorist organization responsible for an attack is a critical step in combating terrorism. Historical data plays a pivotal role in this process, providing insights that can inform prevention and response strategies. With advancements in technology and artificial intelligence (AI), particularly in military applications, there is growing interest in utilizing these developments to enhance national and regional security against terrorism. Central to this effort are terrorism databases, which serve as rich resources for data on armed organizations, extremist entities, and terrorist incidents. The Global Terrorism Database (GTD) stands out as one of the most widely used and accessible resources for researchers. Recent progress in machine learning (ML), deep learning (DL), and natural language processing (NLP) offers promising avenues for improving the identification and classification of terrorist organizations. This study introduces a framework designed to classify and predict terrorist groups using bidirectional recurrent units and self-attention mechanisms, referred to as BiGRU-SA. This approach utilizes the comprehensive data in the GTD by integrating textual features extracted by DistilBERT with features that show a high correlation with terrorist organizations. Additionally, the Synthetic Minority Over-sampling Technique with Tomek links (SMOTE-T) was employed to address data imbalance and enhance the robustness of our predictions. The BiGRU-SA model captures temporal dependencies and contextual information within the data. By processing data sequences in both forward and reverse directions, BiGRU-SA offers a comprehensive view of the temporal dynamics, significantly enhancing classification accuracy. To evaluate the effectiveness of our framework, we compared ten models, including six traditional ML models and four DL algorithms. The proposed BiGRU-SA framework demonstrated outstanding performance in classifying 36 terrorist organizations responsible for terrorist attacks, achieving an accuracy of 98.68%, precision of 96.06%, sensitivity of 96.83%, specificity of 99.50%, and a Matthews correlation coefficient of 97.50%. Compared to state-of-the-art methods, the proposed model outperformed others, confirming its effectiveness and accuracy in the classification and prediction of terrorist organizations."
Harnessing AI and analytics to enhance cybersecurity and privacy for collective intelligence systems,"Despite the authors' efforts to improve the quality of the English used in the paper, there is still significant room for improvement to ensure the text can be considered professional. The structure of the sections ""Literature Review"" and ""Results and Discussion"" remains confusing.

The authors enhanced their previous work by incorporating essential metrics such as Precision, Recall, and F1-score, which are crucial for evaluating the overall performance of the proposed method. Additionally, they compared their approach with existing methods in the field, presenting detailed comparative results. The paper also includes comprehensive details about the experimental laboratory setup, providing a thorough context for their findings.

The authors improved the previous text and provided evidence to support the previously mentioned claims.",Generic,"Collective intelligence systems like Chat Generative Pre-Trained Transformer (ChatGPT) have emerged. They have brought both promise and peril to cybersecurity and privacy protection. This study introduces novel approaches to harness the power of artificial intelligence (AI) and big data analytics to enhance security and privacy in this new era. Contributions could explore topics such as: leveraging natural language processing (NLP) in ChatGPT-like systems to strengthen information security; evaluating privacy-enhancing technologies to maximize data utility while minimizing personal data exposure; modeling human behavior and agency to build secure and ethical human-centric systems; applying machine learning to detect threats and vulnerabilities in a data-driven manner; using analytics to preserve privacy in large datasets while enabling value creation; crafting AI techniques that operate in a trustworthy and explainable manner. This article advances the state-of-the-art at the intersection of cybersecurity, privacy, human factors, ethics, and cutting-edge AI, providing impactful solutions to emerging challenges. Our research presents a revolutionary approach to malware detection that leverages deep learning (DL) based methodologies to automatically learn features from raw data. Our approach involves constructing a grayscale image from a malware file and extracting features to minimize its size. This process affords us the ability to discern patterns that might remain hidden from other techniques, enabling us to utilize convolutional neural networks (CNNs) to learn from these grayscale images and a stacking ensemble to classify malware. The goal is to model a highly complex nonlinear function with parameters that can be optimized to achieve superior performance. To test our approach, we ran it on over 6,414 malware variants and 2,050 benign files from the MalImg collection, resulting in an impressive 99.86 percent validation accuracy for malware detection. Furthermore, we conducted a classification experiment on 15 malware families and 13 tests with varying parameters to compare our model to other comparable research. Our model outperformed most of the similar research with detection accuracy ranging from 47.07% to 99.81% and a significant increase in detection performance. Our results demonstrate the efficacy of our approach, which unlocks the hidden patterns that underlie complex systems, advancing the frontiers of computational security."
Intelligent accounting optimization method based on meta-heuristic algorithm and CNN,"The manuscript introduces an interesting approach to intelligent accounting, however, the contribution to the field is somewhat overstated. The integration of meta-heuristic algorithms and CNN is not entirely novel and requires a more critical examination of its distinct advantages over existing methods. The authors are suggested to address the following comments while revising the paper.

1.? ?The abstract is informative, but the introduction lacks depth. It does not sufficiently articulate the gaps in the current literature that this research aims to fill. A more thorough justification of the research problem is needed to establish the significance of the study.

2.? ?The literature review is superficial and misses key recent works in the field of intelligent accounting and meta-heuristic algorithms. A deeper engagement with contemporary research would provide a stronger foundation for the proposed approach.

3.? ?The description of the multi-modal feature extraction mechanism is vague. Detailed technical specifications and a clearer explanation of how document and voucher information are integrated into the CNN framework are necessary for reproducibility.

4.? ?The performance metrics reported are promising, but the manuscript lacks a rigorous comparative analysis with existing methodologies. Including benchmarks and statistical significance tests would provide a more robust evaluation of the proposed method’s efficacy.

5.? ?The manuscript claims practical relevance, yet it fails to provide concrete examples or scenarios where the proposed method could be applied. Specific case studies or pilot implementations in real-world settings would substantiate these claims.",Generic,"The evolution of social intelligence has led to the adoption of intelligent accounting practices in enterprises. To enhance the efficiency of enterprise accounting operations and improve the capabilities of accountants, we propose an intelligent accounting optimization approach that integrates meta-heuristic algorithms with convolutional neural networks (CNN). First, we enhance the CNN framework by incorporating document and voucher information into accounting audits, creating a multi-modal feature extraction mechanism. Utilizing these multi-modal accounting features, we then introduce a method for assessing accounting quality, which objectively evaluates financial performance. Finally, we propose an optimization technique based on meta-heuristic principles, combining genetic algorithms with annealing models to improve the accounting system. Experimental results validate our approach, demonstrating an accuracy of 0.943 and a mean average precision (mAP) score of 0.812. This method provides technological support for refining accounting audit mechanisms."
A model integrating attention mechanism and generative adversarial network for image style transfer,"In summary, this article constructs a generative model for style transfer using a recurrent consistency network as the backbone framework and integrating attention mechanisms. At the same time, the ResNet module is used in the converter module to improve the stability of the model, avoid gradient vanishing, and make the training process more stable.
(1) The experimental results show that the model can effectively transfer image styles and has a certain degree of innovation.
(2) The format of the period punctuation in line 89 is incorrect. Please make the necessary corrections.
(3) Please reorganize the long sentence into several shorter ones to make it easier to read. Including the following:
Lines 31 to 36: ""Gatys [1] et al., for the first time, reproduced the image style of famous paintings on natural images through convolutional neural network (CNN), they first pre-trained the content of photos through CNN to perform feature modeling, and further made statistics on the style characteristics of famous paintings, and used CNN to extract the content of photos and the style of famous paintings to match the style to the target photo, then the stylized images with given artistic features are successfully generated for the first time.""
Lines 227 to 232: ""Previous studies have shown that because they do not have accurate labels to help the identification of experimental results, the design and selection of evaluation indicators has always been a problem, many unsupervised learning tasks will choose FCN-score for evaluation tasks, the core idea is to use fully convolutional neural network (FCN) [34] to learn its source domain for a large number of original pictures, so that the FCN network remembers the real picture features as much as possible, and then the real picture and the generated picture are mixed into the FCN network.""
(4) For formulas, there are some inconsistent symbols before and after. Please check if the formulas in the entire text are correct and if the symbols are consistent throughout the text.
(5) There are some language and grammar errors, the authors should go through the paper and check the errors. Please re-organize the long sentences, a very long sentence should be splitted to a couple of shorter ones to make it more readable.
(6) For the references, there are some are not consistent, also for some references,the format is not suitable for the paper, please check it and make it more clearly.",Generic,"Image style transfer is an important way to combine different styles and contents to generate new images, which plays an important role in computer vision tasks such as image reconstruction and image texture synthesis. In style transfer tasks, there are often long-distance dependencies between pixels of different styles and contents, and existing neural network-based work cannot handle this problem well. This paper constructs a generation model for style transfer based on the cycle-consistent network and the attention mechanism. The forward and backward learning process of the cycle-consistent mechanism could make the network complete the mismatch conversion between the input and output of the image. The attention mechanism enhances the model's ability to perceive the long-distance dependencies between pixels in process of learning feature representation from the target content and the target styles, and at the same time suppresses the style feature information of the non-target area. Finally, a large number of experiments were carried out in the monet2photo dataset, and the results show that the misjudgment rate of Amazon Mechanical Turk (AMT) perceptual studies achieves 45%, which verified that the cycle-consistent network model with attention mechanism has certain advantages in image style transfer."
Detection and diagnosis of diabetic eye diseases using two phase transfer learning approach,"The content is clear, however, the grammatical mistakes should be avoided. The Literature provided is not the latest one and needs to be improved. The authors must include and discuss recent articles. The organization of the manuscript can be improved further. The authors can refer to the journals suggested in the review report. The methodology and results sections must be improved by highlighting the contribution and including the standard performance metrics for segmentation. Most of the Figure's resolution should be improved as mentioned in the review report.

The work is quite exciting and falls within the scope of the journal. The research gaps and solutions are not well-defined. More investigation is required in terms of literature and methodology and results section. The Literature, Methodology, and Results sections can be improved.

The contribution and novelty should be highlighted in the Methodology section with proper explanations and citing equations. Must address the data imbalance issue in the current approach.",Generic,"Background: Early diagnosis and treatment of diabetic eye disease (DED) improve prognosis and lessen the possibility of permanent vision loss. Screening of retinal fundus images is a significant process widely employed for diagnosing patients with DED or other eye problems. However, considerable time and effort are required to detect these images manually.

Methods: Deep learning approaches in machine learning have attained superior performance for the binary classification of healthy and pathological retinal fundus images. In contrast, multi-class retinal eye disease classification is still a difficult task. Therefore, a two-phase transfer learning approach is developed in this research for automated classification and segmentation of multi-class DED pathologies.

Results: In the first step, a Modified ResNet-50 model pre-trained on the ImageNet dataset was transferred and learned to classify normal diabetic macular edema (DME), diabetic retinopathy, glaucoma, and cataracts. In the second step, the defective region of multiple eye diseases is segmented using the transfer learning-based DenseUNet model. From the publicly accessible dataset, the suggested model is assessed using several retinal fundus images. Our proposed model for multi-class classification achieves a maximum specificity of 99.73%, a sensitivity of 99.54%, and an accuracy of 99.67%."
Gamify4LexAmb: a gamification-based approach to address lexical ambiguity in natural language requirements,"This paper is based on an interesting concept and presents an approach to address ambiguities in Natural Language Requirements. The section 1 and 2 clearly states the purpose of the study.

Gamification in any system includes inclusion of game elements and game mechanics. In background authors have covered game elements used in RE but game mechanics or game rules are not covered in detail. The coverage of game e mechanics/rules in previous studies must be covered in the background.

Can you please tell me how did you decided on the game elements? What is the reason of selecting only points, levels, and leaderboard? I didn’t find any justification in the paper.

The tool was preliminary validated by 3 experts from the industry which I think are very few to validate a tool specifically requirements ambiguity. How this proposed approach Gamify4LexAmb is adequate for addressing ambiguity, detection, and reduction precisely for any software?

It would be nice to show the requirements after ambiguity resolution to see the difference in Table 8

The authors have used two words consequently ‘addressing’ and ‘reduction’ ambiguities. The paper title refers towards addressing the ambiguity but the body of the paper completely states ambiguity detection and reduction. This leads to confusion.",Generic,"Ambiguity is a common challenge in specifying natural language (NL) requirements. One of the reasons for the occurrence of ambiguity in software requirements is the lack of user involvement in requirements elicitation and inspection phases. Even if they get involved, it is hard for them to understand the context of the system, and ultimately unable to provide requirements correctly due to a lack of interest. Previously, the researchers have worked on ambiguity avoidance, detection, and removal techniques in requirements. Still, less work is reported in the literature to actively engage users in the system to reduce ambiguity at the early stages of requirements engineering. Traditionally, ambiguity is addressed during inspection when requirements are initially specified in the SRS document. Resolving or removing ambiguity during the inspection is time-consuming, costly, and laborious. Also, traditional elicitation techniques have limitations like lack of user involvement, inactive user participation, biases, incomplete requirements, etc. Therefore, in this study, we have designed a framework, Gamif ication for Lex ical Amb iguity (Gamify4LexAmb), for detecting and reducing ambiguity using gamification. Gamify4LexAmb engages users and identifies lexical ambiguity in requirements, which occurs in polysemy words where a single word can have several different meanings. We have also validated Gamify4LexAmb by developing an initial prototype. The results show that Gamify4LexAmb successfully identifies lexical ambiguities in given requirements by engaging users in requirements elicitation. In the next part of our research, an industrial case study will be performed to understand the effects of gamification on real-time data for detecting and reducing NL ambiguity."
A flexible perception method of thin smoke based on patch total bounded variation for buildings,"This article proposes the application of ""the total bounded variation"" for detecting thin smoke and conducts experimental verification. The paper has a certain degree of innovation.The English description should be checked, as some descriptions are not rigorous enough.There are deficiencies in the analysis of the current situation in this field in the Introduction, and the latest relevant literature should be added and analyzed.

The SSD and YOLO algorithms selected in the comparative experiment are object detection algorithms, and the author should choose relevant algorithms in this field for comparison to highlight the effectiveness of this paper.
The author should add ablation experiments to demonstrate the effectiveness of applying this algorithm in this article.",Generic,"Early fire warning is critical to the safety and stability of power systems. However, current methods encounter challenges in capturing subtle features, limiting their effectiveness in providing timely alerts for potential fire hazards. To overcome this drawback, a novel detection algorithm for thin smoke was proposed to enhance early fire detection capabilities. The core is that the Patch-TBV feature was proposed first, and the total bounded variation (TBV) was computed at the patch level. This approach is rooted in the understanding that traditional methods struggle to detect minute variations in image characteristics, particularly in scenarios where the features are dispersed or subtle. By computing TBV at a more localized level, the algorithm proposed gains a finer granularity in assessing image quality, enabling it to capture subtle variations that might indicate the presence of smoke or early signs of a fire. Another key aspect that sets our algorithm apart is the incorporation of subtle variation magnification. This technique serves to magnify subtle features within the image, leveraging the computed TBV values. This magnification strategy is pivotal for improving the algorithm's precision in detecting subtle variations, especially in environments where smoke concentrations may be minimal or dispersed. To evaluate the algorithm's performance in real-world scenarios, a comprehensive dataset, named TIP, comprising 3,120 images was constructed. The dataset covers diverse conditions and potential challenges that might be encountered in practical applications. Experimental results confirm the robustness and effectiveness of the proposed algorithm, showcasing its ability to provide accurate and timely fire warnings in various contexts. In conclusion, our research not only identifies the limitations of existing methods in capturing subtle features for early fire detection but also proposes a sophisticated algorithm, integrating Patch-TBV and micro-variation amplification, to address these challenges. The algorithm's effectiveness and robustness are substantiated through extensive testing, demonstrating its potential as a valuable tool for enhancing fire safety in power systems and similar environments."
"TechMark: a framework for the development, engagement, and motivation of software teams in IT organizations based on gamification","The paper is not consistent; it seems to me that it tries to address different aspects without going deep into any one of them.
(1) They propose a framework to design serious games for IT, but it is not clear how the framework is specific of IT and the evaluation of the framework is very weak.
(2) They do an empirical study on the use of a serious game in IT organizations. This is more interesting, but a more detailed description is required.

See more detailed comments in the attached commented article.

The design of the empirical study for part (2) should be better done, or at least better explained. For part (1) there is a single paragraph in the model evaluation.

See more comments in the attached doc.

Validity of the findings
Due to the lack of detail I am not convinced in the validity of results for (1) and (2), though I think that (2) is more interesting and I suggest that the authors rewrite the paper focusing on (2) only.",Generic,"In today's fast-moving world of information technology (IT), software professionals are crucial for a company's success. However, they frequently experience low motivation as a result of competitive pressures, unclear incentives, and communication gaps. This underscores the critical need to handle these internal marketing challenges such as employee motivation, development, and engagement in IT organizations. Internal marketing practices aiming at attracting, engaging, and inspiring employees to use excellent services have become increasingly important. Internal marketing is attracting, engaging, and motivating employees as internal customers to utilize their quality services. Gamification has emerged as a significant trend over recent years. Despite the expanding use of gamification in the workplace, there is still a lack of focus on internal marketing tactics that incorporate gamification approaches. Thus, addressing the challenges related to employee motivation, development, and engagement is crucial. Therefore, as a principal contribution, this research presents a comprehensive framework designed to implement gamified solutions for software teams of IT organizations. This framework has been tailored to effectively address the challenges posed by internal marketing by optimizing motivation, development, and engagement. Moreover, the framework is applied to design and implement a gamified work portal (GWP) through a systematic process, including the design of low-fidelity and high-fidelity prototypes. Additionally, the GWP is validated through a quasi-experiment involving IT professionals from different IT organizations to authenticate the effectiveness of framework. Finally, the outclass results obtained by the gamification-based GWP highlight the effectiveness of the proposed gamification approach in enhancing development, motivation, and engagement while fostering ongoing knowledge of the employees."
A variant-informed decision support system for tackling COVID-19: a transfer learning and multi-attribute decision-making approach,"The manuscript is written in clear, professional English and is generally easy to follow. The introduction provides a comprehensive overview of the COVID-19 pandemic's impact, the emergence of Variants of Concern (VOCs), and the necessity for an adaptive decision support system. The background and context are well-established, demonstrating the relevance and urgency of the research. The literature is well-referenced, citing recent and relevant studies, which strengthens the manuscript's foundation.

Request for correction:
(1) Raw data is mentioned, although its accessibility and detail should be explicitly confirmed. Can you cite and reference the source of your dataset?

(2) In line 189, Hasell et al., 2020, and Mathieu et al., 2021 are not referenced.

The research presents an original primary investigation within the scope of the journal. The research question is well-defined, relevant, and meaningful, addressing a significant gap in knowledge regarding decision support systems for managing COVID-19 variants. The use of Multi-Attribute Decision-Making (MADM) techniques and transfer learning to enhance neural network models is innovative and appropriate for the study's objectives.

The methods are described in sufficient detail to allow replication, covering data sources, preprocessing steps, model development, and evaluation criteria. The study adheres to high technical and ethical standards, including a comprehensive discussion of the data and models used.",Generic,"The global impact of the COVID-19 pandemic, characterized by its extensive societal, economic, and environmental challenges, escalated with the emergence of variants of concern (VOCs) in 2020. Governments, grappling with the unpredictable evolution of VOCs, faced the need for agile decision support systems to safeguard nations effectively. This article introduces the Variant-Informed Decision Support System (VIDSS), designed to dynamically adapt to each variant of concern's unique characteristics. Utilizing multi-attribute decision-making (MADM) techniques, VIDSS assesses a country's performance by considering improvements relative to its past state and comparing it with others. The study incorporates transfer learning, leveraging insights from forecast models of previous VOCs to enhance predictions for future variants. This proactive approach harnesses historical data, contributing to more accurate forecasting amid evolving COVID-19 challenges. Results reveal that the VIDSS framework, through rigorous K-fold cross-validation, achieves robust predictive accuracy, with neural network models significantly benefiting from transfer learning. The proposed hybrid MADM approach integrated approaches yield insightful scores for each country, highlighting positive and negative criteria influencing COVID-19 spread. Additionally, feature importance, illustrated through SHAP plots, varies across variants, underscoring the evolving nature of the pandemic. Notably, vaccination rates, intensive care unit (ICU) patient numbers, and weekly hospital admissions consistently emerge as critical features, guiding effective pandemic responses. These findings demonstrate that leveraging past VOC data significantly improves future variant predictions, offering valuable insights for policymakers to optimize strategies and allocate resources effectively. VIDSS thus stands as a pivotal tool in navigating the complexities of COVID-19, providing dynamic, data-driven decision support in a continually evolving landscape."
Anonymous group structure algorithm based on community structure,"(1) Some formula symbols are not very clear and accurate. In line 146, r1 may have been used incorrectly. In line 152-155, n does not appear in formula 2, and di, dj ?ij, ? and so on are also not explained. In Algorithm 1, ?2 is not set in advance.
(2) Several sentences have unclear meanings. In line 250, what is 'fou dataset' ? In Figure 7, 'Modify' should be replaced with ‘Modified'. In Figure 8, what is 'C1' or 'C2'? In line 281, ’ Secondly‘ should be 'Thirdly'?
(3) In Figure 7 and 8, the horizontal axis should be named. A line of the average degree can be drawn and prominently marked ""modified degree"" of each nodes in Figure 7.

(1) Algorithm 4 is not used in the experiment. And, there is no explanation on how to select a level value in the article.
(2) In 'Compare original K-anonymization', simulation result analysis focuses on the comparison of 'degree'. Considering the purpose of this article is to develop anonymous group structure Algorithm, suggest to discuss some certain characteristic indicators of ’anonymous group structure‘ with different network sizes.",Authentic,"A social network is a platform that users can share data through the internet. With the ever-increasing intertwining of social networks and daily existence, the accumulation of personal privacy information is steadily mounting. However, the exposure of such data could lead to disastrous consequences. To mitigate this problem, an anonymous group structure algorithm based on community structure is proposed in this article. At first, a privacy protection scheme model is designed, which can be adjusted dynamically according to the network size and user demand. Secondly, based on the community characteristics, the concept of fuzzy subordinate degree is introduced, then three kinds of community structure mining algorithms are designed: the fuzzy subordinate degree-based algorithm, the improved Kernighan-Lin algorithm, and the enhanced label propagation algorithm. At last, according to the level of privacy, different anonymous graph construction algorithms based on community structure are designed. Furthermore, the simulation experiments show that the three methods of community division can divide the network community effectively. They can be utilized at different privacy levels. In addition, the scheme can satisfy the privacy requirement with minor changes."
SPCANet: congested crowd counting via strip pooling combined attention network,"The paper is clear and precise, using professional English consistently. It includes relevant literature to provide context for the discussion. Additionally, the introduction effectively sets up the background and importance of the research in the broader academic conversation. A new blend of existing methods has led to incremental contributions to the experimental outcomes.

There are minor formatting issues, that should be fixed. It is recommended that the whole paper be carefully reviewed again for minor formatting errors. Examples:
The same sentence is repeated in lines 43-48.
Line 60: The space before reference is missing.
Line 219: space missing after “.”.
Repeating in line 142: „In 2018, Li et al. Li et al. (2018) proposed...“

All appropriate raw data must be available in accordance with PeerJ Data Sharing policy (https://peerj.com/about/policies-and-procedures/#data-materials-sharing). The source code is not provided and should be included before acceptance!",Generic,"Crowd counting aims to estimate the number and distribution of the population in crowded places, which is an important research direction in object counting. It is widely used in public place management, crowd behavior analysis, and other scenarios, showing its robust practicality. In recent years, crowd-counting technology has been developing rapidly. However, in highly crowded and noisy scenes, the counting effect of most models is still seriously affected by the distortion of view angle, dense occlusion, and inconsistent crowd distribution. Perspective distortion causes crowds to appear in different sizes and shapes in the image, and dense occlusion and inconsistent crowd distributions result in parts of the crowd not being captured completely. This ultimately results in the imperfect capture of spatial information in the model. To solve such problems, we propose a strip pooling combined attention (SPCANet) network model based on normed-deformable convolution (NDConv). We model long-distance dependencies more efficiently by introducing strip pooling. In contrast to traditional square kernel pooling, strip pooling uses long and narrow kernels (1×N or N×1) to deal with dense crowds, mutual occlusion, and overlap. Efficient channel attention (ECA), a mechanism for learning channel attention using a local cross-channel interaction strategy, is also introduced in SPCANet. This module generates channel attention through a fast 1D convolution to reduce model complexity while improving performance as much as possible. Four mainstream datasets, Shanghai Tech Part A, Shanghai Tech Part B, UCF-QNRF, and UCF CC 50, were utilized in extensive experiments, and mean absolute error (MAE) exceeds the baseline, which is 60.9, 7.3, 90.8, and 161.1, validating the effectiveness of SPCANet. Meanwhile, mean squared error (MSE) decreases by 5.7% on average over the four datasets, and the robustness is greatly improved."
Effective sentence-level relation extraction model using entity-centric dependency tree,"The authors show promise in their work, yet certain issues need addressing. The manuscript requires meticulous polishing of its English and writing, including extensive editing of vocabulary, language, syntax, phrasing, and punctuation. Simplify the abstract by using only the simple present tense.",Generic,"The syntactic information of a dependency tree is an essential feature in relation extraction studies. Traditional dependency-based relation extraction methods can be categorized into hard pruning methods, which aim to remove unnecessary information, and soft pruning methods, which aim to utilize all lexical information. However, hard pruning has the potential to overlook important lexical information, while soft pruning can weaken the syntactic information between entities. As a result, recent studies in relation extraction have been shifting from dependency-based methods to pre-trained language model (LM) based methods. Nonetheless, LM-based methods increasingly demand larger language models and additional data. This trend leads to higher resource consumption, longer training times, and increased computational costs, yet often results in only marginal performance improvements. To address this problem, we propose a relation extraction model based on an entity-centric dependency tree: a dependency tree that is reconstructed by considering entities as root nodes. Using the entity-centric dependency tree, the proposed method can capture the syntactic information of an input sentence without losing lexical information. Additionally, we propose a novel model that utilizes entity-centric dependency trees in conjunction with language models, enabling efficient relation extraction without the need for additional data or larger models. In experiments with representative sentence-level relation extraction datasets such as TACRED, Re-TACRED, and SemEval 2010 Task 8, the proposed method achieves F1-scores of 74.9%, 91.2%, and 90.5%, respectively, which are state-of-the-art performances."
Decoding Bitcoin: leveraging macro- and micro-factors in time series analysis for price prediction,"The paper shows promise, but it requires revisions to meet the journal's standards.

Provide a thorough description of data collection methods.

Modify Figure 1, Creating an effective experimental flowchart for predicting Bitcoin prices involves outlining the entire process clearly and logically.

The experimental design is given in Figure 1, but the flow of work is not clear for that reason I
also raise comment to modify Figure 1 in the Experimental design section.

Validity of the findings
The preprocessing steps are vague or incomplete.

Elaborate on each preprocessing step, including specific methods for handling missing values (e.g., imputation techniques), outlier detection methods, and normalization techniques. Provide examples of feature engineering.

The authors of the manuscript incorporated a comprehensive set of macroeconomic indicators, on-
chain data revealing transactional details.

In the manuscript interestingly the Bi-LSTM outperformed the SOTA models; and in terms of
performance.

Bi-LSTM processes the input sequence in both forward and backward directions, allowing the
model to understand the full context of the data. This is particularly useful for time series data
where future events can be influenced by past trends and vice versa.

This methodology provides a varied examination of the factors influencing price statistics, helping
investors make informed decisions regarding Bitcoin-related investments.",Authentic,"Predicting Bitcoin prices is crucial because they reflect trends in the overall cryptocurrency market. Owing to the market’s short history and high price volatility, previous research has focused on the factors influencing Bitcoin price fluctuations. Although previous studies used sentiment analysis or diversified input features, this study’s novelty lies in its utilization of data classified into more than five major categories. Moreover, the use of data spanning more than 2,000 days adds novelty to this study. With this extensive dataset, the authors aimed to predict Bitcoin prices across various timeframes using time series analysis. The authors incorporated a broad spectrum of inputs, including technical indicators, sentiment analysis from social media, news sources, and Google Trends. In addition, this study integrated macroeconomic indicators, on-chain Bitcoin transaction details, and traditional financial asset data. The primary objective was to evaluate extensive machine learning and deep learning frameworks for time series prediction, determine optimal window sizes, and enhance Bitcoin price prediction accuracy by leveraging diverse input features. Consequently, employing the bidirectional long short-term memory (Bi-LSTM) yielded significant results even without excluding the COVID-19 outbreak as a black swan outlier. Specifically, using a window size of 3, Bi-LSTM achieved a root mean squared error of 0.01824, mean absolute error of 0.01213, mean absolute percentage error of 2.97%, and an R-squared value of 0.98791. Additionally, to ascertain the importance of input features, gradient importance was examined to identify which variables specifically influenced prediction results. Ablation test was also conducted to validate the effectiveness and validity of input features. The proposed methodology provides a varied examination of the factors influencing price formation, helping investors make informed decisions regarding Bitcoin-related investments, and enabling policymakers to legislate considering these factors."
CSQUiD: an index and non-probability framework for constrained skyline query processing over uncertain data,"1. The figures are not clear. High-resolution figures should be included.
2. In Table 2, the authors must include the limitations of existing schemes. Which motivates the authors to do this research.
3. In Algorithm 2, too many else-if statements are used. Which can lead the algorithm to high complexity. It seems the algorithm is with high complexity or should derive the complexity of the algorithms.",Generic,"Uncertainty of data, the degree to which data are inaccurate, imprecise, untrusted, and undetermined, is inherent in many contemporary database applications, and numerous research endeavours have been devoted to efficiently answer skyline queries over uncertain data. The literature discussed two different methods that could be used to handle the data uncertainty in which objects having continuous range values. The first method employs a probability-based approach, while the second assumes that the uncertain values are represented by their median values. Nevertheless, neither of these methods seem to be suitable for the modern high-dimensional uncertain databases due to the following reasons. The first method requires an intensive probability calculations while the second is impractical. Therefore, this work introduces an index, non-probability framework named Constrained Skyline Query processing on Uncertain Data (CSQUiD) aiming at reducing the computational time in processing constrained skyline queries over uncertain high-dimensional data. Given a collection of objects with uncertain data, the CSQUiD framework constructs the minimum bounding rectangles (MBRs) by employing the X-tree indexing structure. Instead of scanning the whole collection of objects, only objects within the dominant MBRs are analyzed in determining the final skylines. In addition, CSQUiD makes use of the Fuzzification approach where the exact value of each continuous range value of those dominant MBRs’ objects is identified. The proposed CSQUiD framework is validated using real and synthetic data sets through extensive experimentations. Based on the performance analysis conducted, by varying the sizes of the constrained query, the CSQUiD framework outperformed the most recent methods (CIS algorithm and SkyQUD-T framework) with an average improvement of 44.07% and 57.15% with regards to the number of pairwise comparisons, while the average improvement of CPU processing time over CIS and SkyQUD-T stood at 27.17% and 18.62%, respectively."
YOLOv8-Coal: a coal-rock image recognition method based on improved YOLOv8,"In this paper, the authors have used a new method introduced to enhance recognition accuracy and processing speed in odrder to address issues such as misdetection and omission due to low light, image defocus, and worker occlusion in coal-rock image recognition. Extensive experimental results demonstrate the efficiency of the proposed method. Overall, the issue of this study is of practical significance. However, there are a few points that need further improvement.
1. In the Introduction, more relevant references could be cited to analyse the relevant work.
2. In the Materials & Methods, the authors should highlight the innovative work in this paper, instead of simply listing the modular frameworks of other reference.
3. The figure 3 of Deformable Convolutional Networks Version 3 is the same as Deformable Convolutional Networks Version 1, please give a more detailed explanation.
4. Parameters in the equations need to be specifically given an explanation, for example, in Eq(4), ech and esp should be explaned.
5. Image resolution is too low to see specific information, which needs to improve image clarity.
6. Suggested modifications to Table 2 for ablation experiments for which a specific table template exists.
7. Further experiments are needed to explain why the module introduced in the paper must be used onYOLOv8 and how it works on other YOLOversions.",Generic,"To address issues such as misdetection and omission due to low light, image defocus, and worker occlusion in coal-rock image recognition, a new method called YOLOv8-Coal, based on YOLOv8, is introduced to enhance recognition accuracy and processing speed. The Deformable Convolution Network version 3 enhances object feature extraction by adjusting sampling positions with offsets and aligning them closely with the object's shape. The Polarized Self-Attention module in the feature fusion network emphasizes crucial features and suppresses unnecessary information to minimize irrelevant factors. Additionally, the lightweight C2fGhost module combines the strengths of GhostNet and the C2f module, further decreasing model parameters and computational load. The empirical findings indicate that YOLOv8-Coal has achieved substantial enhancements in all metrics on the coal rock image dataset. More precisely, the values for AP50, AP50:95, and AR50:95 were improved to 77.7%, 62.8%, and 75.0% respectively. In addition, optimal localization recall precision (oLRP) were decreased to 45.6%. In addition, the model parameters were decreased to 2.59M and the FLOPs were reduced to 6.9G. Finally, the size of the model weight file is a mere 5.2 MB. The enhanced algorithm's advantage is further demonstrated when compared to other commonly used algorithms."
Coalitional Privacy-Utility Trade-off for Network-Wide Mobility Data Release and Reconstruction with Hidden Confounders,"This paper “Coalitional Privacy-Utility Trade-off for Network-Wide
Mobility Data Release and Reconstruction with Hidden
Confounders” presents a technique to implement reconfigurable networks. The following points are the constructive criticism for the authors.

Concern # 1: The technical writing should be improved a lot. The aim should be towards readability for novice reader. The first sentence of the abstract is so long that it is difficult to understand. 
Concern # 2: Why a service provider will utilize user mobility data to compromise the privacy? Are their historical examples of such events?
Concern # 3: What is the point of view on privacy utility trade-off? Are the authors considering it from the PoV of the user? The SP?
Concern # 4: Can blockchain play a role in the privacy preservation studied in this article?
Concern # 5: Figure 1 is too small for visibility. Figure 3 and 4 are better but need more height adjustment. 
Concern # 6: The supplementary document should not have same references. 
Concern # 7: More recent references from 2020 onwards should be included.",Authentic,"We study an information theoretic privacy mechanism design problem for two scenarios where the private data is either observable or hidden. In each scenario, we first consider bounded mutual information as privacy leakage criterion, then we use two different per-letter privacy constraints. In the first scenario, an agent observes useful data Y that is correlated with private data X, and wishes to disclose the useful information to a user. A privacy mechanism is designed to generate disclosed data U which maximizes the revealed information about Y while satisfying a bounded privacy leakage constraint. In the second scenario, the agent has additionally access to the private data. To this end, we first extend the Functional Representation Lemma and Strong Functional Representation Lemma by relaxing the independence condition and thereby allowing a certain leakage to find lower bounds for the second scenario with different privacy leakage constraints. Furthermore, upper and lower bounds are derived in the first scenario considering different privacy constraints. In particular, for the case where no leakage is allowed, our upper and lower bounds improve previous bounds. Moreover, considering bounded mutual information as privacy constraint we show that if the common information and mutual information between X and Y are equal, then the attained upper bound in the second scenario is tight. Finally, the privacy-utility trade-off with prioritized private data is studied where part of X, i.e., X1, is more private than the remaining part, i.e., X2, and we provide lower and upper bounds."
HLEM-VMP: A Virtual Machine Placement Model for Effectively Reducing SLA Violations,"In the paper titled “HLEM-VMP: A Virtual Machine Placement Model for
Effectively Reducing SLA Violations” authors provide a VM placement framework to reduce SLA violations. Causes of SLA breaches include unreasonable virtual machine placement, hardware failures, and network issues. This paper introduces HLEM-VMP, a novel Virtual Machine Placement strategy aiming to reduce SLA violations by addressing host overload risks and minimizing virtual machine migrations through a multi-step process involving host filtering, load evaluation, and power consumption prediction. The article needs major organization/presentation changes with the following points. 
•	There are several issues in technical writing. The abstract does not provide any insights to the heuristic/algorithm/research method used for placement/allocation strategy. Same is the issue in the introduction. The related work already highlighted that there are some methods such as greedy. What are the proposed methods of this work?
•	The absence of the research methods puts questions on the novelty of the article. 
•	Line 40, 41, the author name should not be in all capitals. 
•	The contributions at the end of intro should be itemized. 
•	Section IV.E lists the benchmarks that have been used for comparison. However, some are basic and they do not have any reference to the previously proposed article. Only one has and more should be included from recent articles such as “Sonia Bashir, Saad Mustafa, Junaid Shuja, Raja Wasim Ahmad, Tahir Maqsood, Abdullah Alourani, “Multi-Factor Nature Inspired SLA-Aware Energy Efficient Resource Management for Cloud Environments” Cluster Computing, 26(2), 1643 – 1658, 2023”
•	The organization of the article is also absent. This puts into question the motivation of section V after the results have been discussed.",Authentic,"Abstract
Cloud service providers deliver computing services on demand using the Infrastructure as a Service (IaaS) model. In a cloud data center, several virtual machines (VMs) can be hosted on a single physical machine (PM) with the help of virtualization. The virtual machine placement (VMP) involves assigning VMs across various physical machines, which is a crucial process impacting energy draw and resource usage in the cloud data center. Nonetheless, finding an effective settlement is challenging owing to factors like hardware heterogeneity and the scalability of cloud data centers. This paper proposes an efficient algorithm named VMP-ER aimed at optimizing power consumption and reducing resource wastage. Our algorithm achieves this by decreasing the number of running physical machines, and it gives priority to energy-efficient servers. Additionally, it improves resource utilization across physical machines, thus minimizing wastage and ensuring balanced resource allocation."
A Blockchain-based Electric Vehicle Charging Cooperation Model,"The author proposed an A Blockchain-based Electric Vehicle Charging Cooperation Model. The work is worthy but needs extensive amendments. The following comments are necessary to address in the manuscript:
1.	Strong motivation is required to be added at the end of the introduction section please.  
2.	There is lack of literature about the topic, so the authors need to add more work from practical projects of the EVs charging system. At the end of the related work the authors need to add the exact problem which is not considered by the researchers. 
3.	Figure 1 caption is not clear, “overall platform”, one cannot get idea from the caption. Fig 1 needs to change as it does not give any meaningful information. 
4.	The authors need to establish why the EVs charging stations information require security, can we share the info without security, or the security is must, please prove it.  
5.	In table 1 every column is somehow yes value then where you are fitting your solution, please compare your proposed model where the available work was No but you are making it YES.
6.	In Fig 2 proposed protocol is provided while there is no proposed solution diagram. In fig 2 D (2) is repeated what it means?
7.	Equations are not properly explained, and there is no numbering with equations.  
8.	Algorithms are not properly explained.
9.	Need to test the proposed solution on real system to check the efficiency of the proposed model.  
10.	Communication cost results are necessary to check, I think the proposed system will increase communication. The author did not explain how the EV will be added to the block chain.
11.	Conclusions need modifications and validate your results in the abstract and conclusion with the help of statistical values or percentages.
12.	Some of the recent references need to be added such as
L. U. Khan et al., ""Federated Learning for Digital Twin-Based Vehicular Networks: Architecture and Challenges,"" in IEEE Wireless Communications, doi: 10.1109/MWC.012.2200373.",Authentic,"The increase in development of electric vehicle(EV) will have a strong impact on the power distribution grid if adequate care is not taken on the high power demand required for charging EV. Consequently, there is need to create a platform to enable charging point operators to effectively manage the EV user’s charging requests and ensure that there charging needs are satisfied while not exceeding the distribution grid capacity. If this is not done, there is no doubt that in few a years, EV owners will be faced with the problems of unavailability of charging stations and congestion in grid sequel to simultaneous charging of many EV’s. This work proposes a smart EV charging infrastructure based on a blockchain platform. With the charging demand (kWh) and maximum duration of the charging event provided by the EV user, the EV load flexibility is determined and utilized through smart charging to achieve a stable grid. EV owners and charging stations are linked through the platform thereby reducing the actors in EV charging ecosystem from six to four. Flexibility (power and time) in charging of EV is traded within the blockchain platform. By this, additional investors will be attracted into the business of EV charging station and through flexible offers, EV loads are shifted from the peak load hours. Consequently, the simulations shows that the acceptance rate of EV users increased by more than 50% when our smart charging system was adopted compared to the normal charging scenario."
Modeling the Performance of HPC Applications on Heterogeneous cloud storage,"The paper titled “Modeling the Performance of HPC Applications on Heterogeneous cloud storage” presents a model for performance evaluation of HPc application. The focus is heterogeneous cloud platforms. The paper is not well presented and organized. The contributions are not defined due. The article requires some major improvements. Some issues are listed below.
•	The abstract misses some important points. Why are cloud storage platforms heterogeneous? What research issues arise as a result? 
•	Introduction is very brief. It does not address many points. What are requirements of HPC applications? Why and why not are cloud suitable for those requirements? 
•	The references are not in correct format. () has been used instead of []. 
•	The introduction does not sell the motivation behind this research. What were the issues in previous works that are focused in this work? Why is storage performance modeling necessary? Why is the proposed EVT method suitable for this problem? 
•	Table 1 formatting is very poor. The text size needs to be small. 
•	An overall illustration of the proposed research methodology is required.
•	The related works section needs to be extended and studies focusing on cloud storage performance need to be added. Currently the focus is only HPC based works. 
•	More HPC focused benchmarks need to be tested as the aim is to migrate HPC application to cloud.
•	Where is heterogeneity discussed and addressed in the results? Specifying the type of heterogeneity (OS, hardware, application, etc) might be a good idea.
•	Figure captions are misplaced. 
•	Isnt it better to present result of figure 4 in a table? 
•	There are many problems in the references section. Few are very old limiting the contributions of the study in terms of recent state of the art research. Some do not have year listed. 
•	The following references can be accommodated based on their high relevance. 
Analysis of Vector Code Offloading Framework in Heterogeneous Cloud and Edge Architectures
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment Models",Authentic,"Moving high-performance computing (HPC) applications from HPC clusters to cloud computing clusters, also known as the HPC cloud, has recently been proposed by the HPC research community. Migrating these applications from the former environment to the latter can have an important impact on their performance, due to the different technologies used and the suboptimal use and configuration of cloud resources such as heterogeneous storage. Probabilistic models can be applied to predict the performance of these applications and to optimise them for the new system. Modelling the performance in the HPC cloud of applications that use heterogeneous storage is a difficult task, due to the variations in performance. This paper presents a novel model based on Extreme Value Theory (EVT) for the analysis, characterisation and prediction of the performance of HPC applications that use heterogeneous storage technologies in the cloud and high-performance distributed parallel file systems. Unlike standard approaches, our model focuses on extreme values, capturing the true variability and potential bottlenecks in storage performance. Our model is validated using return level analysis to study the performance of representative scientific benchmarks running on heterogeneous cloud storage at a large scale and gives prediction errors of less than 7%."
Proactive Publish/Subscribe Broker Clustering with Topic Similarities and Proximity Geolocations,"The paper titled “Proactive Publish/Subscribe Broker Clustering with Topic Similarities and Proximity Geolocations” presents a broker model of Edge-cloud performance. The paper is well presented and organized. The contributions are very defined and essential. The article requires some minor improvements. Some issues are listed below.
•	When the authors state, “…to address these issues” in abstract, they have not clearly mentioned the existing research issues.
•	Cloud data centers and CDNs are responsible for data publishing/subscribing. How the generic and centralized model of publish/subscribe can be brought to a decentralized and geo dispersed edge network.
•	Paragraphs should not start with moreover. It means this concept can be merged in previous paragraph. 
•	The related works section needs to be partitioned into subsections as the related works are easily categorized. At the end of the section, works more related to this article should be discussed. 
•	What is the need to clustering based on geo locations? Simple filters are not enough! 
•	The authors state in section V that
We employed the brokers’ coordinates to calculate the geographic distance between two edge brokers to randomly generate the round-trip time (RTT) between them on the assumption that the RTT
Why RTTs are generated randomly when geolocations and edge coordinates are known? 
•	Pixel quality of figure 6 is very low. 
•	Can the value of alpha in zipf model be more than 1 as stated by authors?
•	An overall illustration of the proposed research methodology is required.
•	The following references can be accommodated based on their high relevance. 
Sensor Cloud Frameworks: State-of-the-Art, Taxonomy, and Research Issues (IEEE Sensors)
Resource Efficient Geo-Textual Hierarchical Clustering Framework for Social IoT Applications (IEEE Sensors)
Hybrid DBSCAN based Community Detection for Edge Caching in Social Media Applications (IEEE Conference)",Authentic,"Computing services for the Internet-of-Things (IoT) play a vital role for widespread IoT deployment. A hierarchy of Edge-Cloud publish/subscribe (pub/sub) broker overlay networks that support latency-sensitive IoT applications in a scalable manner is introduced. In addition, we design algorithms to cluster edge pub/sub brokers based on topic similarities and geolocations to enhance data dissemination among end-to-end IoT devices. The proposed model is designed to provide low delay data dissemination and effectively save network traffic among brokers. In the proposed model, IoT devices running pub/sub client applications periodically send collected data, organized as a hierarchy of topics, to their closest edge pub/sub brokers. Then, the data are processed/analyzed at edge nodes to make controlling decisions promptly replying to the IoT devices and/or aggregated for further delivery to other interested edge brokers or to cloud brokers for long-term processing, analysis, and storage. Extensive simulation results demonstrate that our proposal achieves the best data delivery latency compared to two baseline schemes, a classical Cloud-based pub/sub scheme and an Edge-Cloud pub/sub scheme. Considering the similar Edge-Cloud technique, the proposed scheme outperforms PubSubCoord-alike in terms of relay traffic ratio among brokers. Therefore, our proposal can adapt well to support wide-scale latency-sensitive IoT applications."
Application of Improved Sparrow Search Algorithm in Edge Computing,"The paper titled “Application of Improved Sparrow Search Algorithm in Edge Computing” presents a CMSSA algorithm for edge computing. The paper is not well presented and organized an very poorly written. The article requires major improvements in terms of organization and figurative explanation. Some issues are listed below.
•	The article is very poorly written as can be seen from the abstract and first few paragraphs of the introduction. The authors use both past and present tense for their proposed work in the abstract. The first sentence of the introduction is combination of multiple sentences making little sense and readability. Look into this sentence
With the rapid development of Internet, web of Things
big data, producing all kinds of new business based on
Internet, in today's fault diagnosis field, at the side of the
network edge of mechanical equipment to produce a large
amount of data, but due to the edge of the end of the
heterogeneous computing and storage capacity, most of the
fault equipment data analysis is done in the cloud. 
Another such discussion can be found as
In order to reduce the edge-cloud task processing delay and
power consumption, and the redundancy of one-dimensional
data and the dependence of cloud processing, reduce the
network traffic pressure, and ultimately improve the quality
and efficiency of fault information analysis.
The article needs thorough technical writing review.  
•	There are formatting mistakes too. VOLUME XX, 2017scale is embedded in the last line of first page. 
•	The introduction should first present an overall view of edge computing and resource management within it. Then specific articles addressing the issues should be presented. 
•	Related work should be presented in separate section. 
•	Algorithm should be presented in the style and format of algorithm. 
•	The abbreviated references are not according to the rule. 
•	The figure qualities are not good.
•	An overall illustration of the proposed framework is required as diagrams are very few and essential figurative detail is missing.
•	The following references can be accommodated based on their high relevance. 
Mobility-aware computational offloading in mobile edge networks: a survey (Cluster computing)
Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks (Complexity)",Authentic,"With the network changes brought by 5G, Mobile Edge Computing (MEC) has been deeply concerned as a prospective computing pattern. In MEC network, offloading the tasks to edge servers can address the problems of 5G mobile users' delay sensitivity and insufficient energy. To further decrease the delay and energy consumption, the offloaded task data can be reduced by compressing part of the task data before computing offloading. This paper investigates the problem of collectively optimizing computation offloading, data compression, and resource distribution aiming at minimizing the total system cost with limited MEC computational capacity. To solve the problem, an improved sparrow search algorithm (ISSA) is developed, which integrates circle chaotic mapping strategy, dynamic step factor strategy and Levy flight strategy, and lots of experiments verified the excellent performance of ISSA-based offloading scheme. Experimental results show the developed offloading scheme outperforms the sparrow search algorithm (SSA) based offloading scheme and particle swarm optimization (PSO) based offloading scheme in reducing the total system cost."
Fall Detection System With Artificial Intelligence-Based Edge Computing,"The paper titled “Fall Detection System With Artificial Intelligence-Based Edge Computing” mainly addresses mobile jamming attacks in smart cities. The paper is very well presented and organized. The idea is interesting. The article requires some minor improvements. Some issues are listed below.
a-	What is the relation of detection speed with frames per second as stated in the abstract as
the detection speed of the overall system was 11.5 frames per second
b-	The authors should include a section on related work to highlight state of the art research in this problem. Some of th related works listed in the Introduction should also be moved to this new section. The section should also discuss the research gap in existing works.
c-	A diagram is required (system framework) that explains where the inputs come from, what part of intelligence is deployed in Edge server, what is the role of cloud server, and what is the output. An overall block diagram is necessary for the general understanding. 
d-	The details of 8bit CNN model need to be added. Does in compromise accuracy? 
e-	How can this solution reduce fall incidents? 
f-	Some of the equations of Accuracy etc are un necessary. The should only be discussed and not equated. 
g-	System evaluation should be divided into subsections. Evaluation of accuracy, time, etc should be presented in each subsection.
h-	Are there any results on the processing time of the proposed work? As it is a realtime problem, the processing time should be compared with previous works. 
i-	The results of power consumption and detection speed are listed in abstract but I don’t find them in results section. 
j-	Separate discussion on limitations and future research directions is required in last section
k-	The following references can be accommodated based on their relevance
Sensor Cloud Frameworks: State-of-the-Art, Taxonomy, and Research Issues
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?",Authentic,"Falls are the second leading cause of death from unintentional injuries in older adults. Although many systems have been used to detect falls, they are limited by the computational complexity of the algorithm. The images taken by the camera must be transmitted through a network to the back-end server for calculation. As the demand for Internet of Things increases, this architecture faces such problems as high bandwidth costs and server computing overload. Emerging methods reduce the workload of servers by transferring certain computing tasks from cloud servers to edge computing platforms. To this end, this study developed a fall detection system based on neuromorphic computing hardware, which streamlines and transplants the neural network model of the back-end computer to the edge computing platform. Through the neural network model with integer 8 bit precision deployed on the edge computing platform, the object photos obtained by the camera are converted into human motion features, and a support vector machine is then used for classification. After experimental evaluation, an accuracy of 96% was reached, the detection speed of the overall system was 11.5 frames per second, and the power consumption was 0.3 W. This system can monitor the fall events of older adults in real time and over a long period. All data were calculated on the edge computing platform. The system only reports fall events via Wi-Fi, thereby protecting the privacy of the user."
Task Offloading and Resource Allocation Mechanism of Moving Edge Computing in Mining Environment,"The paper titled “Task Offloading and Resource Allocation Mechanism of Moving Edge Computing in Mining Environment” mainly focuses on task offloading in MEC. The paper is not well presented and organized. The article requires some major improvements in terms of organization and technical details. Some issues are listed below.
Comment # 1: What specific requirements does a mine bring to task offloading in MEC. How these requirements are different from the general scenario?
Comment # 2: The introduction is very short and in sufficient. References are missing at many places. The MEC, task offloading and mining + IoT requirements and motivation should be highlighted in the introduction. 
Comment # 3: The technical writing is not good and requires significant improvement. 
Comment # 4: The introduction does not highlight the problem statement from the existing literature not the contributions of the article nor the organization of the article. 
Comment # 5: The article formatting is not good and can be improved along with the quality of figures.
Comment # 6: Figure 6 does not label the unit of the floating values. 
Comment # 7: how is the movement of the devices resolved and formulated in the problem? 
Comment # 7: How are the SERVICE ACHIEVEMENT RATES FOR EACH MECHANISM measured?
Comment # 8: The authors state  a PSO based algorithm abstract. However, the nature of the algorithm is not defined in the section IV. Why is this algo suitable for the problem? 
Comment # 9: The number of references is very low. The following references can be accommodated based on their high relevance. The technical article can be compared in the related works sections while the surveys provide information related to the introduction section. 
Mobility-aware computational offloading in mobile edge networks: a survey (cluster computing)
Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks (Complexity)
Joint wireless power transfer and task offloading in mobile edge computing: a survey (cluster computing)",Authentic,"The mine improves the response speed of the IoT control system by sinking computing services into the MEC Server. In this case, computing resources are scattered and it is difficult to cope with the changing computing requirements of mobile terminal devices. To solve this problem, we optimize from the supply side by build P.M.R (problem of maximum revenue) problem. In P.M.R, we comprehensively consider the air rate of wireless communication, network delay, computing resource demand and other factors that affect the offloading task of terminals. To obtain the relative optimal solution of P.M.R, we construct a task unloading algorithm based on particle swarm optimization. And we set up a simulation experiment environment according to the mine network parameters. The experimental results show that this task unloading algorithm can effectively improve the efficiency of computing facilities in the system and achieve load balancing."
IoT Serverless Computing at the Edge: Open Issues and Research Direction,"The paper titled “IoT Serverless Computing at the Edge: Open Issues and Research Direction” mainly focuses on the survey of serverless computing at the edge of networks (MEC). The paper is not well presented and organized. The article requires major improvements in terms of organization and figurative explanation. Overall it is an interesting article and has defined contributions. Some issues are listed below.
•	The abstract need to be more detailed. 
•	References are very scarcely placed in the introduction.
•	What is the logic for the time period selection of the articles?
•	The introduction needs several discussions. What are the contributions of this article? How is this survey different from previous efforts in the same direction?
•	Section II para one has no reference complementing the discussion. This is a common mistake in some parts of the article. 
•	Diagrams are very few and essential figurative detail is missing. Diagrams can show a serverless IoT architecture at the edge of the network among multiple other concepts.
•	Taxonomy is also required to relate and categorize different concepts of this research.
•	There are too many subsections in each section. A better overview of the article organization is also needed.
•	The figure pixel quality is very low.
•	How is this article different from the published article of same authors in Computers? Are there new contributions?
•	The following references can be accommodated based on their high relevance. The surveys provide information related to the introduction section. 
Sensor Cloud Frameworks: State-of-the-Art, Taxonomy, and Research Issues (IEEE Sensors Journal)
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment Models (IEEE Access)
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution? (IEEE Comm mag)",Authentic,"Novel computing paradigms aim to enable better hardware utilization, allowing a greater number of applications to be executed on the same physical resources. Serverless computing is one example of such an emerging paradigm, enabling faster development, more efficient resource usage, as well as no requirements for infrastructure management by end users. Recently, efforts have been made to utilize serverless computing at the network edge, primarily focusing on supporting Internet of Things (IoT) workloads. This study explores open issues, outlines current progress, and summarizes existing research findings about serverless edge computing for IoT by analyzing 67 relevant papers published between 01.01.2015 and 01.09.2021. We discuss the state-of-the-art research in 8 subject areas relevant to the use of serverless at the network edge, derived through the analysis of the selected articles. Results show that even though there is a noticeable interest for this topic, further work is needed to adapt serverless to the resource constrained environment of the edge."
A Review of Data Centers Energy Consumption And Reliability Modeling,"The paper titled “A Review of Data Centers Energy Consumption And Reliability Modeling” mainly addresses the issue of data centers energy and reliability. The paper is a detailed review. The research gap is well defined and the review is on a targeted subject. The paper is not well presented and organized. The article requires some major improvements in terms of organization, taxonomy, and the tabular summarization of concepts. Some issues are listed below.
a-	The authors mention carbon emissions due to data centers. Are data center directly responsible for it or indirectly? Which electricity production technique is more responsible for CO2?
b-	The author are based in a Nordic country. Are renewable energy resources enough to solely power data centers in some regions? 
a-	Please provide some metadata for the 188 reviewed articles in the form of a table. 
b-	Figure 2 needs higher resolution and width. 
c-	The tables 3 and 4 need to summarize discussion. Similarly for table 6, I would suggest more fields to summarize the presented data. 
d-	What are regression models? What is their role in DC load models? The detail is missing. 
e-	I feel that the taxonomy should be included to present a high level organization of the article. 
f-	What is the difference between INTERNAL POWER CONDITIONING SYSTEM and cooling section models? This division is not clear from the start of the article. 
g-	There are multiple terms used interchangeably for reliability such as SLA aware etc. The authors should state if the term reliability is inclusive or not.  
h-	Separate discussion on limitations and future research directions is required before the last section as it is a detailed review article.
i-	Renewable energy models should also be discussed and their effect on reliability as renewable energy resources is often not 24/7.
j-	The following references can be accommodated based on their high relevance
Sustainable Cloud Data Centers: A Survey of Enabling Techniques and Technologies
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment Models
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds",Authentic,"Enhancing the efficiency and the reliability of the data center are the technical challenges for maintaining the quality of services for the end-users in the data center operation. The energy consumption models of the data center components are pivotal for ensuring the optimal design of the internal facilities and limiting the energy consumption of the data center. The reliability modeling of the data center is also important since the end-user’s satisfaction depends on the availability of the data center services. In this review, the state-of-the-art and the research gaps of data center energy consumption and reliability modeling are identified, which could be beneficial for future research on data center design, planning, and operation. The energy consumption models of the data center components in major load sections i.e., information technology (IT), internal power conditioning system (IPCS), and cooling load section are systematically reviewed and classified, which reveals the advantages and disadvantages of the models for different applications. Based on this analysis and related findings it is concluded that the availability of the model parameters and variables are more important than the accuracy, and the energy consumption models are often necessary for data center reliability studies. Additionally, the lack of research on the IPCS consumption modeling is identified, while the IPCS power losses could cause reliability issues and should be considered with importance for designing the data center. The absence of a review on data center reliability analysis is identified that leads this paper to review the data center reliability assessment aspects, which is needed for ensuring the adaptation of new technologies and equipment in the data center. The state-of-the-art of the reliability indices, reliability models, and methodologies are systematically reviewed in this paper for the first time, where the methodologies are divided into two groups i.e., analytical and simulation-based approaches. There is a lack of research on the data center cooling section reliability analysis and the data center components’ failure data, which are identified as research gaps. In addition, the dependency of different load sections for reliability analysis of the data center is also included that shows the service reliability of the data center is impacted by the IPCS and the cooling section."
Contribution of Deep-Learning Techniques towards fighting COVID-19: A Bibliometric Analysis of Scholarly Production during 2020,"The paper titled “Contribution of Deep-Learning Techniques towards fighting COVID-19: A Bibliometric Analysis of Scholarly Production during 2020” mainly focuses on the bibliometric analysis of deep learning articles aimed at COVID-19. The paper is well presented and organized but not well formated. The article requires minor improvements in terms of organization and technical details. Overall it is an interesting article and has defined contributions. Some issues are listed below.
•	What is the significance of the article? How bibliometric analysis can contribute towards a better scientific insight on covid and deep learning? 
•	The discussion in section II.A should be reduced. 
•	The formatting of the article is not upto the mark. There are empty spaces.
•	Table 1 should summarize the previous works and not put much detail as in current form. The summarization should highlight the contributions of this article from previous efforts. 
How? 
•	Many of the references are not in correct form and miss some of the information. Ref 14, 17, 19 are few of the examples.
•	The following references can be accommodated based on their high relevance. The technical article can be compared in the related works sections while the surveys provide information related to the introduction section. 
Evaluating the Dynamics of Bluetooth Low Energy Based COVID-19 Risk Estimation for Educational Institutes",Authentic,"COVID-19 has dramatically affected various aspects of human society with worldwide repercussions. Firstly, a serious public health issue has been generated, resulting in millions of deaths. Also, the global economy, social coexistence, psychological status, mental health, and the human-environment relationship/dynamics have been seriously affected. Indeed, abrupt changes in our daily lives have been enforced, starting with a mandatory quarantine and the application of biosafety measures. Due to the magnitude of these effects, research efforts from different fields were rapidly concentrated around the current pandemic to mitigate its impact. Among these fields, Artificial Intelligence (AI) and Deep Learning (DL) have supported many research papers to help combat COVID-19. The present work addresses a bibliometric analysis of this scholarly production during 2020. Specifically, we analyse quantitative and qualitative indicators that give us insights into the factors that have allowed papers to reach a significant impact on traditional metrics and alternative ones registered in social networks, digital mainstream media, and public policy documents. In this regard, we study the correlations between these different metrics and attributes. Finally, we analyze how the last DL advances have been exploited in the context of the COVID-19 situation."
Cooperative Federated Learning based Task Offloading Scheme for Tactical Edge Networks,"The paper titled “Cooperative federated learning based task offloading scheme for tactical edge networks” mainly focuses on federated learning based task offloading in 5G/6G. The paper is not well presented and organized. The article requires some major improvements in terms of organization, taxonomy, and the tabular summarization of concepts. Some issues are listed below.
Comment # 1: The article is well written and achieves optimal offloading scheme in edge networks, but at times the reading is a bit confusing due to difficult sentence structure and vocabulary. Author should use simple vocabulary and sentence structure.
Comment # 2: The author should briefly discuss the tactical edges in the introduction section.
Comment # 3: Some good quality figures are needed to support the federated learning and tactical edge networks.
Comment # 4: Multiple references are used at the end of each para containing multiple discussions and techniques. The author should support the discussion in the introduction and specifically technical concepts through proper referencing at the right places. 
Comment # 5: The mathematical equations are very complex and need simplier representation if possible.
Comment # 6: The algorithm does not have proper format. 
Comment # 7: The number of references is very low. The following references can be accommodated based on their high relevance. The technical article can be compared in the related works sections while the surveys provide information related to the introduction section. 
Mobility-aware computational offloading in mobile edge networks: a survey (cluster computing)
Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks (Complexity)
Joint wireless power transfer and task offloading in mobile edge computing: a survey (cluster computing)",Authentic,"In this study, we focus on the federated learning (FL) based tactical edge network platform to cooperatively operate computation-hungry tasks as efficiently as possible. Based on the incentive of FL model training, each individual device makes offloading decisions for their tactical tasks in resource-constrained network environments. According to the ideas of two different bargaining solutions - weighted average solution and constant elasticity substitution solution - edge server and IoT devices work together in a coordinated manner to approximate a well-balanced system performance. Therefore, we can reach an agreement while exploring the mutual benefits to leverage a reciprocal consensus between different viewpoints. The main novelty of our approach is to investigate the dual-interactive bargaining process based on the interdependent relationship between IoT devices and the tactical edge server. To the best of our knowledge, this is the first work that jointly considers different bargaining solutions to handle tactical edge-assisted task offloading services. Based on the numerical simulation, it is demonstrated that the proposed approach can increase the system throughput, device payoff and device fairness up to 10%, 15% and 20%, respectively, in comparison with existing protocols."
HCE: A Runtime System for Efficiently Supporting Heterogeneous Cooperative Execution,"The paper titled “HCE: A Runtime System for Efficiently Supporting Heterogeneous Cooperative Execution” proposes a heterogeneous execution platform fitness and analysis hardware/software platform connected with the Cloud. The paper is very poorly presented and organized and has limited contributions. The following major issues need to be addressed in first revisions.
a-	What kind of heterogeneity the authors are focusing on? Is it instruction set architecture heterogeneity? Please specify it in the abstract. 
b-	Specify the full form of HCE in abstract. 
c-	The work seems to be an improvement on previous work by same authors. If so, the previous solution and the proposed extensions should be detailed in the introduction. 
d-	The contributions/dynamics of the HCE runtime are not clear from figure 1 and 2. 
e-	The algorithms are too lengthy and difficult to understand/review in current format. Try to present concise information. 
f-	The related works section should be added to highlight contributions from the existing state of the art. 
g-	The authors need to refer a very relevant article on heterogeneous platform execution. The SIMDOM framework reduces the execution overhead of migrated vectorized multimedia application by using vector-to-vector instruction mappings. The framework maps and translates ARM SIMD intrinsic instructions to x86 SIMD intrinsic instructions such that an application programmed for the mobile platform can be executed on the cloud server without any modification. Please refer to 
SIMDOM: A Framework for SIMD Instruction Translation and Offloading in Heterogeneous Mobile Architectures
AND
Analysis of Vector Code Offloading Framework in Heterogeneous Cloud and Edge Architectures",Authentic,"Heterogeneous systems with multiple different compute devices have come into common use recently, and the heterogeneity of the compute device is mainly reflected in three aspects: hardware architecture, instruction set architecture, and processing capability. Heterogeneous CPU-accelerator systems have attracted increasing attention especially. To make full use of multiple CPUs and accelerators to execute data-parallel applications, programmers may need to manually map computation and data to all available compute devices, which is tedious, error-prone, and difficult. Especially for some data-parallel applications, the inter-device communication could easily become the performance bottleneck of multi-device co-execution. Therefore, firstly, a runtime system is designed for supporting heterogeneous cooperative execution (HCE) of data-parallel applications, which can help programmers to automatically and efficiently map computation and data to multiple compute devices. Secondly, an incremental data transfer method is designed to avoid redundant data transfers between devices, and a three-way overlapping communication optimization method based on software pipelining is designed to effectively hide the inter-device communication overhead. Based on our previously proposed feedback-based dynamic and elastic task scheduling (FDETS) scheme and asynchronous-based dynamic and elastic task scheduling (ADETS) scheme, the modified FDETS that supports incremental data transfer and the modified ADETS that supports three-way overlapping communication optimization are proposed, which not only can effectively partition and balance the workload among multiple compute devices but also can significantly reduce data transfer overhead between devices. Thirdly, a prototype of the proposed runtime system is implemented, which provides a set of runtime APIs for task scheduling, device management, memory management, and transfer optimization. Our experimental results show that the communication overhead between devices is greatly reduced using the proposed inter-device communication optimization methods and the multi-device co-execution significantly outperforms the best single-device execution."
A Survey on Reinforcement Learning-Aided Caching in Heterogeneous Mobile Edge Networks,"The paper titled “A Survey on Reinforcement Learning-Aided Caching in Heterogeneous Mobile Edge Networks” mainly focuses on RL based caching in MEC 5G/6G. The paper is a detailed review. and the review is on a targeted subject. The paper is not well presented and organized without the taxonomy. The article requires some major improvements in terms of organization, taxonomy, and the tabular summarization of concepts. Some issues are listed below.
a-	There are a lot of recent surveys focusing on the same topic. The authors should include a table to highlight their contributions from previous articles. One of them has not been included in discussion at all and listed below
Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey JNCA
b-	The taxonomy is an important part of such survey. It is missing and the structure of the article without it looks in bits and pieces that are not connected to each other. 
c-	In the definition of unsupervised clustering, the authors state that
In edge caching users can be clustered based on, for example, their desired contents, mobility, desire to cooperate with each other. 
It is not clear from the beginning if the article focuses only on RL. It seems from the above statement that unsupervised learning and other ML techniques are also applicable to caching. Are such studies also included or discussed on sidelines? 
a-	What is the importance of “lessons learned” heading at the end of each section? If the authors are following IEEE COM ST template, the section should be justified while not including redundant information. 
b-	What can be the role of ML in predicting the user requests and content popularity? Highlight this discussion.
c-	The following references can be accommodated based on their high relevance
Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks
Hybrid DBSCAN based Community Detection for Edge Caching in Social Media Applications",Authentic,"Mobile networks experience a tremendous increase in data volume and user density due to the massive number of coexisting users and devices. An efficient technique to alleviate this issue is to bring the data closer to the users by exploiting cache-aided edge nodes, such as fixed and mobile access points, and even user devices. Meanwhile, the fusion of machine learning and wireless networks offers new opportunities for network optimization when traditional optimization approaches fail or incur high complexity. Among the various machine learning categories, reinforcement learning provides autonomous operation without relying on large sets of historical data for training. In this survey, reinforcement learning-aided mobile edge caching solutions are presented and classified, based on the networking architecture and optimization target. As sixth generation (6G) networks will be characterized by high heterogeneity, fixed cellular, fog, cooperative, vehicular, and aerial networks are studied. The discussion of these works reveals that there exist reinforcement learning-aided caching schemes with varying complexity that can surpass the performance of conventional policy-based approaches. Finally, several open issues are presented, stimulating further interest in this important research field."
Implementing practical DNN-based object detection offloading decision for maximizing detection performance of mobile edge devices,"The paper titled “Implementing Practical DNN-based Object Detection Offloading Decision for Maximizing Detection Performance of Mobile Edge Devices” proposes a edge offloading framework for object detection in mobile devices. The paper is well presented and organized and has defined contributions. However, the following major issues need to be addressed in first revisions.
a-	In the abstract the authors point to poor resource allocation in the edge as a problem. While the solution is offloading. Shouldn’t the solution be resource/edge server allocation? 
b-	In the following sentence, are authors missing a not!
Note that the binary optimization problem is NP-hard, indicating that it is able to find a globally optimal solution in non-polynomial time.
c-	Figure 2 does not show the offload decision module. Where it is running? What may be its runtime complexity given the environment variables? 
d-	Does figure 4b show that the proposed work is not scalable with number of devices? 
e-	How is bandwidth adjusted for figure 5a analysis?
f-	The number of references is low where authors cite mostly articles from their Country with low author diversity. 
g-	The authors need to refer several relevant articles such as the Edge computing paper.
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution? 
AND edge offloading article
Analysis of Vector Code Offloading Framework in Heterogeneous Cloud and Edge Architectures
And ML based offloading strategy in edge computing
Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks",Authentic,"In the last decade, deep neural network (DNN)-based object detection technologies have received significant attention as a promising solution to implement a variety of image understanding and video analysis applications on mobile edge devices. However, the execution of computationally intensive DNN-based object detection workloads in mobile edge devices is insufficient in fulfilling the object detection requirements with high accuracy and low latency, owing to the limited computation capacity. In this paper, we implement and evaluate a DNN-based object detection offloading framework to improve the object detection performance of mobile edge devices by offloading computation-intensive workloads to a remote edge server. However, preliminary experimental results have shown that offloading all object detection workloads of mobile edge devices may lead to worse performance than executing the workloads locally. This degradation is obtained from the inefficient resource utilization in the edge computing architectures, both for the edge server and mobile edge devices. To resolve the aforementioned problem with degradation, we devise a device-aware DNN offloading decision algorithm that is aimed to maximize resource utilization in the edge computing architecture. The proposed algorithm decides whether or not to offload the object detection workloads of edge devices by considering their computing power and network bandwidth, and therefore maximizing their average object detection processing frames per second. Through various experiments conducted in a real-life wireless local area network (WLAN) environment, we verified the effectiveness of the proposed DNN-based object detection offloading framework."
Smart City: The Role of Internet of Things and Machine Learning in Realizing a Data Centric Smart Environment,"The paper titled “Smart City: The Role of Internet of Things and Machine Learning in Realizing a Data Centric Smart Environment” mainly addresses the role of IoT and ML in smart cities. The paper is not very well presented and organized. The idea is interesting. The article requires major improvements in organization of the concepts. Some issues are listed below.
a-	The technical writing of the article can be improved.
b-	The term data-centric in the title does not reflect in the abstract. The abstract needs more discussion in general relating different concepts and fields of the survey with each other
c-	The role of edge computing in smart cities is necessary to debate. Moreover, ML techniques have been applied to edge network. Refer to the article,
Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey
d-	A taxonomy of the related concept would be beneficial and can also be added to the contributions of the article. 
e-	Section III requires references at many places to validate the concepts. 
f-	A caption is misplaced on page 3. 
g-	The pixel quality of some of the figures such as 9 needs improvement. 
h-	In table 8, the authors list related works for ML. The references are very old. The application of ML algorithms in smart city and the corresponding references should be added. 
i-	At many points of the article, the connection of smart cities with ML/DL is not made while there are several articles published in the same direction that need a mention.
j-	Separate discussion on  future research directions is required at the end of the article. The section CHALLENGES IN SMART CITY does not address challenges specific to application of ML/DL/data centric IoTs. The general challenges should be removed and specific challenges should be added. 
k-	The following references should be accommodated based on their relevance
Sensor Cloud Frameworks: State-of-the-Art, Taxonomy, and Research Issues
Resource Efficient Geo-Textual Hierarchical Clustering Framework for Social IoT Applications
Resource Based Direct Manipulation: A User-Centric Visual Interface for Operational Customization of Future Smart Appliances",Authentic,"This paper explores the concept of smart cities and the role of the Internet of Things (IoT) and machine learning (ML) in realizing a data-centric smart environment. Smart cities leverage technology and data to improve the quality of life for citizens and enhance the efficiency of urban services. IoT and machine learning have emerged as key technologies for enabling smart city solutions that rely on large-scale data collection, analysis, and decision-making. This paper presents an overview of smart cities' various applications and discusses the challenges associated with implementing IoT and machine learning in urban environments. The paper also compares different case studies of successful smart city implementations utilizing IoT and machine learning technologies. The findings suggest that these technologies have the potential to transform urban environments and enable the creation of more livable, sustainable, and efficient cities. However, significant challenges remain regarding data privacy, security, and ethical considerations, which must be addressed to realize the full potential of smart cities."
Hybrid Workflow Scheduling on Edge Cloud Computing Systems,"The paper titled “Hybrid Workflow Scheduling on Edge Cloud Computing Systems” addresses an important topic, which is workload scheduling in Edge cloud systems. The paper is well-presented and has defined contributions. However, the following minor issues need to be addressed in first revisions.
a-	The abstract mentions performance efficiency compared to other nominated techniques. But fails to mention those techniques and their flaws. 
b-	Page 2 line 12 needs a reference. 
c-	The introduction flow is logical and readable. However, it does not mention previous state of the art. It should also highlight the issues in hybrid workflow scheduling in existing literature. 
d-	The authors have divided the related work precisely into 3 parts. However, and article that encompasses the three dimension, hence the base article, needs to be highlighted. 
e-	More details of the functional block “predict behavior” figure 3 are required in text. 
f-	Why is the execution time in hours for delay sensitive application (figure 4 and 5)? Any realtime application such as traffic monitoring can bear such execution time? 
g-	Many important references are missing such as
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?
Resource Efficient Geo-Textual Hierarchical Clustering Framework for Social IoT Applications
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds",Authentic,"Internet of Things applications can be represented as workflows in which stream and batch processing are combined to accomplish data analytics objectives in many application domains such as smart home, health care, bioinformatics, astronomy, and education. The main challenge of this combination is the differentiation of service quality constraints between batch and stream computations. Stream processing is highly latency-sensitive while batch processing is more likely resource-intensive. In this work, we propose an end-to-end hybrid workflow scheduling on an edge cloud system as a two-stage framework. In the first stage, we propose a resource estimation algorithm based on a linear optimization approach, gradient descent search (GDS), and in the second stage, we propose a cluster-based provisioning and scheduling technique for hybrid workflows on heterogeneous edge cloud resources. We provide a multi-objective optimization model for execution time and monetary cost under constraints of deadline and throughput. Results demonstrate the framework performance in controlling the execution of hybrid workflows by efficiently tuning several parameters including stream arrival rate, processing throughput, and workflow complexity. In comparison to a meta-heuristics technique using Particle Swarm Optimization (PSO), the proposed scheduler provides significant improvement for large-scale hybrid workflows in terms of execution time and cost with an average of 8% and 35%, respectively."
Privacy Preservation of User Identity in Contact Tracing for COVID-19-like Pandemics Using Edge Computing,"The paper titled “Privacy Preservation of User Identity in Contact Tracing for COVID-19-like Pandemics Using Edge Computing” addresses an important topic, which is privacy preservation in contact tracing applications.  The paper is not well-presented and has the following major issues to be addressed.
a-	It is unclear in the abstract what benefits does edge servers bring to contact tracing. 
b-	The authors state that the contact tracing is based on wireless technologies such as Global Positioning System, Bluetooth, and WiFi enabled communication. Please briefly elaborate contact tracing with each and the pros and cons.
c-	The authors do not mention the centralized and decentralized contact tracing applications and the privacy issues in both. This needs to be established before going to the contributions and should be mentioned what type of contact tracing application the authors are targeting. 
d-	The authors abruptly come to the end of introduction while skipping many important details. What are previous solutions? Why edge computing is necessary? What is the proposed solution? 
e-	There is no detail of the organization of the article at the end of intro.  
f-	The number of references is very short. The authors miss many relevant references. What benefits does edge bring, refer to the article 
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?
g-	Similarly, an important survey regarding research on covid19 is missing. Refer to 
Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey 
h-	The related work fails to mention existing privacy preserving solutions in contact tracing and the research gap in them. 
i-	There is no reference to a blockchain based solution in the abstract and suddenly authors jump to blockchain in section 3. 
j-	Figure 1 is very generic. Where is contact tracing taking place? Where is the blockchain?
k-	The authors do not provide background knowledge of the working of contact tracing application.
l-	There is no discussion on the limitations of this research and the future direction.",Authentic,"Pandemic and infectious disease outbreaks put pressure on health authorities and require lockdowns. These outbreaks, which strain limited healthcare resources, must be swiftly controlled and monitored. A large number of healthcare authorities are currently investigating automated systems to support outbreak monitoring and control. However, current contact tracing systems face many privacy, participation, and power constraints. Furthermore, elderly or less financially able individuals often cannot participate in automated contact tracing due to not owning a smartphone. This paper proposes a new system that enables health authorities to track exposure among individuals participating in the automated system, aid health authorities in interviewing non-participating individuals, and minimize the processing required by offloading to nearby edge computing devices. The proposed system utilizes edge servers to assist health authorities in tracking users who withdraw from or are unable to use contact tracing. Edge computing devices have access to more contextual information, resulting in minimal data collection and thus enabling businesses, houses, and offices to participate in contact tracing as locations. Edge computing devices enable location-based data collection of contact tracing data using proximity-based sensors for offices, homes, and shops, thereby assisting health authorities to notify users of exposure without disclosing the identities of businesses or individuals. Moreover, the proposed system reduces the overall power for end users up to 97% by delegating contact tracing to nearby edge computing devices. In addition, the system mitigates data poisoning attacks that target individuals' smartphones, edge devices, or cloud servers by utilizing blockchain to store contacts, delegations, and identities."
A novel Local and Global Cooperative Approach for Distributed Mobile Cloud Computing,"The paper titled “A novel Local and Global Cooperative Approach for Distributed Mobile Cloud Computing” addresses an important topic, which is service provisioning in mobile cloud computing. The paper is well-presented and has defined contributions. However, the mathematical formulation reduces the technological implications of the article. The following major issues need to be addressed in first revisions.
a-	The title mentions “novel”. However the author does not debate existing solutions in introduction so that the novelty of proposed work can be established.
b-	The authors mention the publication year with the reference in the following case,
A pioneering study that addresses this problem is that of L. Shapley in 1953.
The year should not be mentioned.  
c-	The number of references in the article are very less. Moreover, few of the references are very old. Recent references need to be added. Consider the following relevant articles to be added 
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds
Computation Offloading Cost Estimation in Mobile Cloud Application Models
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?
d-	The author discusses resource heterogeneity in the introduction. Has it been considered in problem formulation? 
e-	Each performance evaluation metric should be presented in separate sub section
f-	Why fairness is evaluated while it is not discussed in abstract and introduction?",Authentic,"In the cloud service paradigm, service providers aim to cost-efficiently serve Internet of Things (IoT) devices’ applications that often request computation-intensive services. In this study, we focus on the cloud resource sharing problem in the distributed mobile cloud computing (DMCC) platform. By considering different IoT applications and distributed cloud infrastructure, we design a novel DMCC resource sharing scheme for data offloading services. Based on the cooperative game theory, five different value solutions are adopted, and they are implemented for each case cooperative process to provide the fair-efficient solution. According to the service characteristics, our proposed scheme explores the mutual benefits of local and global MCC providers’ interactions, and effectively shares the distributed cloud resources. To dynamically adapt the current DMCC system conditions, our approach can provide an appropriate guidance to improve the efficiency of cloud resource usage while enhancing the service quality. The main novelty of our proposed scheme is the ability to leverage a reciprocal consensus between different control viewpoints. Therefore, we can ensure a relevant tradeoff between efficiency and fairness among multiple service providers."
Deep Learning Based Resource Availability Prediction for Local Mobile Crowd Computing,"The paper titled “Deep Learning Based Resource Availability Prediction for Local Mobile Crowd Computing” addresses an important topic, which is application of ML techniques in MCC/mobile computing. The paper is very well-presented and has significant contributions. However, the following minor issues need to be addressed and the article can be accepted after revisions.
a-	A sentence should not start with “but” as in the abstract.
b-	What does ecological cost mean in abstract? 
c-	Abbreviations should be first defined in full form in abstract such as QoS 
d-	In Section IV (B) the authors expressed C1 as the time complexity of the prediction algorithm, but C1 is nowhere calculated in the article.
e-	Further, the authors also used C1 as variable in another place (figure 13) defined as Channels
f-	for intime and out-time. It is confusing! same varibale for two different things??
g-	The authors used LSTM for predicting/forecasting since the input data is a time series problem. That is fine as LSTM has great accuracy result. But how they handle the tremendous time and space complexity thats a natural problem of LSTM models? This is very important aspect needed to be addressed in revision. It is better to add a subsection for that.
h-	In page 15 section 3, the authors mentioned that they used the CONVOLUTIONAL variant of LSTM. The nature of their input data is sequential and using convolution mechanism as an addition to straight LSTM seems not necessary. The authors should provide some justification for it in introduction, where they claimed to compare the prediction performances of convolutional LSTM and GRU with the basic LSTM and GRU.
i-	Equations and variables are not rigorous and may be revised.
j-	On page 15 figure 17, in the second layer Pixel-wise data transformation is occurred. Why is it so? Have the authors converted the input data that is textual mobility traces into an image/pixles? It is confusing, please clarify.
k-	On page 16 they used and defined a term Perplexity. Provide a proper reference to it.
l-	Many important references are missing such as
Zhou, Shuchen, Waqas Jadoon, and Junaid Shuja. ""Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks."" Complexity 2021 (2021).
Shuja, J., Bilal, K., Alasmary, W., Sinky, H., & Alanazi, E. (2021). Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey. Journal of Network and Computer Applications, 103005.",Authentic,"Mobile crowd computing (MCC) that utilizes public-owned (crowd’s) smart mobile devices (SMDs) collectively can give adequate computing power without any additional financial and ecological cost. However, the major challenge is to cope with the mobility (or availability) issue of SMDs. User’s unpredicted mobility makes the SMDs really unstable resources. Selecting such erratic resources for job schedule would result in frequent job offloading and, in the worst case, job loss, which would affect the overall performance and the quality of service of MCC. In a Local MCC, generally, a set of users are available for a certain period regularly. Based on this information, the chances of a user being available for a certain duration from a given point of time can be predicted. In this paper, we provide an effective model to predict the availability of the users (i.e., their SMDs) in such an MCC environment. We argue that before submitting a job to an SMD, the stability of it is to be assessed for the duration of execution of the job to be assigned. If the predicted availability period is greater than the job size, then only the job should be assigned to the SMD. An accurate prediction will minimize the unnecessary job offloading or job loss due to the early departure of the designated SMD. We propose an advanced convolutional feature extraction mechanism that is applied to LSTM and GRU-based time-series prediction models for predicting SMD availability. To collect user mobility data, we considered a research lab scenario, where real mobility traces were recorded with respect to a Wi-Fi access point. We compared the prediction performances of convolutional LSTM and GRU with the basic LSTM and GRU and ARIMA in terms of MAE, RMSE, R 2 , accuracy, and perplexity. In all the measurements, the proposed convolutional LSTM exhibited considerably better prediction performance."
The 3-Axis Scalable Service-Cloud Resource Modeling for Burst Prediction under Smart Campus Scenario,"The authors have proposed a 3-axis scalability model to predict service-burst at the smart campus level. The article is revised and the authors have made appropriate changes according to reviewer comments. 
The area of research is quite interesting, and the manuscript presents a well-defined flow of problems, methodology and simulation, however, I have the following suggestions for further improvements.
1.The authors should proofread the whole manuscript for any grammatical and technical mistakes.
2.The study should highlight the number of nodes/users employed for this smart campus case study.
3.Authors should also include one paragraph about the significance of detection of service-burst in the context of smart campus study.
4.The future directions of this work should also be elaborated in the conclusion of the manuscript.",Authentic,"Internet of Things (IoT) enables smart campuses more convenient for cloud services. The availability of cloud resources to its users appears as a fundamental challenge. The existing research presents several auto-scaling techniques to scale the resources with the increase in users’ demands. However, still, the cloud users of auto-scaled servers experience service disruption, delayed responses, and the occurrence of service bursts. The prevailing burst management framework exhibits limitations in the context of burdening the existing auto-scaled machines for cost estimation and resource allocation. This research presents a 3-axis auto-scaling framework for load balancing and resource allocation by incorporating a dedicated cost estimator and allocator (on the z-axis). The cost estimation server develops a log of existing load estimates of vertical and horizontal servers and scales the new users’ requests in case the vertical threshold is breached with new requests. The cost estimator, in its data structure, keeps track of the current resources available at both vertical and horizontal servers. The historical information of available resources and the new resources’ requests is decided by the cost estimator as per demand and supply scenario. The general characteristics of servers are resources pooling, requests queue development, burst identification, automatic scaling, and load balancing. The cost estimator also prioritizes vertical servers for resource allocations, and switches to the horizontal server when the vertical server reaches its 75% quota of resources. The study simulates 1000 users’ requests of smart campus, adopts state-of-the-art ensemble with bagging strategy and handles an effective class imbalance situation."
"Design, Development and Evaluation of an Intelligent Animal Repelling System for Crop Protection based on Embedded Edge-AI","The paper titled “Design, Development and Evaluation of an Intelligent Animal Repelling System for Crop Protection based on Embedded Edge-AI” presents a very interesting study on Edge-AI for smart agriculture. The idea is beneficial for the society and it presentation and evaluation are also technically sound.  The paper is very well-presented and has significant contributions. Abstract is to the point and consice. However, the following minor issues need to be addressed.
a-	It is not advised to start a paragraph with howver, such as on p2.
b-	The introduction has sufficient discussion on the research. However, itemized contributions are required at the end.
c-	Ultrasounds are normally used in medical imaging. How can ultrasound be animal repellent? This needs to be discussed in the introduction. 
d-	Ultrasound use in animal repellent should also be discussed in related works. 
e-	What image augmentation and segmentation techniques were utilized? 
f-	The result section is poorly organized and presented. The authors should compare the accuracy with multiple indicators. In one sunsection, the accuracy should be discussed. While in the other, the resource efficiency (time, power, etc) should be discussed. 
g-	There are many relevant references missing from the study such as
Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?",Authentic,"In recent years, edge computing has become an essential technology for real-time application development by moving processing and storage capabilities close to end devices, thereby reducing latency, improving response time and ensuring secure data exchange. In this work, we focus on a Smart Agriculture application that aims to protect crops from ungulate attacks, and therefore to significantly reduce production losses, through the creation of virtual fences that take advantage of computer vision and ultrasound emission. Starting with an innovative device capable of generating ultrasound to drive away ungulates and thus protect crops from their attack, this work provides a comprehensive description of the design, development and assessment of an intelligent animal repulsion system that allows to detect and recognize the ungulates as well as generate ultrasonic signals tailored to each species of the ungulate. Taking into account the constraints coming from the rural environment in terms of energy supply and network connectivity, the proposed system is based on IoT platforms that provide a satisfactory compromise between performance, cost and energy consumption. More specifically, in this work, we deployed and evaluated various edge computing devices (Raspberry Pi, with or without a neural compute stick, and NVIDIA Jetson Nano) running real-time object detector (YOLO and Tiny-YOLO) with custom-trained models to identify the most suitable animal recognition HW/SW platform to be integrated with the ultrasound generator. Experimental results show the feasibility of the intelligent animal repelling system through the deployment of the animal detectors on power efficient edge computing devices without compromising the mean average precision and also satisfying real-time requirements. In addition, for each HW/SW platform, the experimental study provides a cost/performance analysis, as well as measurements of the average and peak CPU temperature. Best practices are also discussed and lastly, this article discusses how the combined technology used can help farmers and agronomists in their decision making and management process."
LwTE: Light-weight Transcoding at the Edge,"The paper titled “LwTE: Light-weight Transcoding at the Edge” addresses an important topic, which is transcoding at the edge.  The paper is very well-presented and has significant contributions. However, the following minor issues need to be addressed.
a-	The authors need to differentiate between cloud and edge in the introduction for readers. 
b-	The article has significant contributions if authors can distinguish it more clearly from their previous works. Therefore, the 2nd last paragraph of the related work requires more elaboration.
c-	Please distinguish CDN from edge which is used in the title in section III.B.
d-	Some elements of the framework need more detail, such as, what video popularity distribution is used? What video access distribution is used? What video dataset is used and why? Why movielens has not been used? 
e-	The overall transcoding process needs details for a reader who is not familiar to it. 
f-	If video metadata is used, can ML based feature extraction be applied on them for useful insights?
g-	There are many relevant references missing from the study such as
Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?
Mobility-aware computational offloading in mobile edge networks: a survey",Authentic,"Due to the growing demand for video streaming services, providers have to deal with increasing resource requirements for increasingly heterogeneous environments. To mitigate this problem, many works have been proposed which aim to ( i) improve cloud/edge caching efficiency, (ii) use computation power available in the cloud/edge for on-the-fly transcoding, and (iii) optimize the trade-off among various cost parameters, e.g., storage, computation, and bandwidth. In this paper, we propose LwTE, a novel Light- weight Transcoding approach at the Edge, in the context of HTTP Adaptive Streaming (HAS). During the encoding process of a video segment at the origin side, computationally intense search processes are going on. The main idea of LwTE is to store the optimal results of these search processes as metadata for each video bitrate and reuse them at the edge servers to reduce the required time and computational resources for on-the-fly transcoding. LwTE enables us to store only the highest bitrate plus corresponding metadata (of very small size) for unpopular video segments/bitrates. In this way, in addition to the significant reduction in bandwidth and storage consumption, the required time for on-the-fly transcoding of a requested segment is remarkably decreased by utilizing its corresponding metadata; unnecessary search processes are avoided. Popular video segments/bitrates are being stored. We investigate our approach for Video-on-Demand (VoD) streaming services by optimizing storage and computation (transcoding) costs at the edge servers and then compare it to conventional methods (store all bitrates, partial transcoding). The results indicate that our approach reduces the transcoding time by at least 80% and decreases the aforementioned costs by 12% to 70% compared to the state-of-the-art approaches."
ACOA: Application-Centric Orchestration Architecture,"The paper titled “ACOA: Application-Centric Orchestration Architecture” addresses an important topic, which is application orchestration in cloud-edge computing. The paper is very well-presented and has significant contributions. However, the following minor issues need to be addressed and the article can be accepted after revisions.
a-	Both title and abstract are short and not good representative of the article.
b-	The references are very few matching a magazine article. Most of the discussion in Introduction has no reference. The article should have atleast 20 references and each paragraph in introduction should end with one. 
c-	The authors state that 
Being closer to the physical world, they also may be found in worse temperature, energy and network supply conditions.
Why are edge devices in worse network conditions? They are closer to users with lower latencies and higher data rates.
d-	The contributions need to be defined in pointwise manner at the end of intro
e-	The related works/state of the art should be mentioned in intro and the research issues in it should be elaborated which this article addresses. 
f-	Previous work in II.C should be expanded to more studies.
g-	The diagrams 2-8 should have higher pixel quality.
h-	Why are previous work and related work presented in two sections?
i-	Many important references are missing such as
Zhou, Shuchen, Waqas Jadoon, and Junaid Shuja. ""Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks."" Complexity 2021 (2021).
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment Models
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?",Authentic,"The fast growth in the amount of connected devices with computing capabilities in the past years has enabled the emergence of a new computing layer at the Edge. Despite being resource-constrained if compared with cloud servers, they offer lower latencies than those achievable by Cloud computing. The combination of both Cloud and Edge computing paradigms can provide a suitable infrastructure for complex applications’ quality of service requirements that cannot easily be achieved with either of these paradigms alone. These requirements can be very different for each application, from achieving time sensitivity or assuring data privacy to storing and processing large amounts of data. Therefore, orchestrating these applications in the Cloud–Edge computing raises new challenges that need to be solved in order to fully take advantage of this layered infrastructure. This paper proposes an architecture that enables the dynamic orchestration of applications in the Cloud–Edge continuum. It focuses on the application’s quality of service by providing the scheduler with input that is commonly used by modern scheduling algorithms. The architecture uses a distributed scheduling approach that can be customized in a per-application basis, which ensures that it can scale properly even in setups with high number of nodes and complex scheduling algorithms. This architecture has been implemented on top of Kubernetes and evaluated in order to asses its viability to enable more complex scheduling algorithms that take into account the quality of service of applications."
Intelligent Technology of Energy Saving Service Management in Selecting Mobile Edge Computing of Internet of Things Devices,"The article technical writing is very poor and this is evident from the abstract.
The article title is ambiguous and can be made concise.  
Several important references are missing such as
Machine Learning-Based Offloading Strategy for Lightweight User Mobile Edge Computing Tasks",Authentic,"The purpose is to solve the problems of high transmission rate and low delay in the deployment of mobile edge computing network, ensure the security and effectiveness of the Internet of things (IoT), and save resources. Dynamic power management is adopted to control the working state transition of Edge Data Center (EDC) servers. A load prediction model based on long-short term memory (LSTM) is creatively proposed. The innovation of the model is to shut down the server in idle state or low utilization in EDC, consider user mobility and EDC location information, learn the global optimal dynamic timeout threshold strategy and N-policy through trial and error reinforcement learning method, reasonably control the working state switching of the server, and realize load prediction and analysis. The results show that the performance of AdaGrad optimization solver is the best when the feature dimension is 3, the number of LSTM network layers is 6, the time series length is 30–45, the batch size is 128, the training time is 788 s, the number of units is 250, and the number of times is 350. Compared with the traditional methods, the proposed load prediction model and power management mechanism improve the prediction accuracy by 4.21%. Compared with autoregressive integrated moving average (ARIMA) load prediction, the dynamic power management method of LSTM load prediction can reduce energy consumption by 12.5% and realize the balance between EDC system performance and energy consumption. The system can effectively meet the requirements of multi-access edge computing (MEC) for low delay, high bandwidth and high reliability, reduce unnecessary energy consumption and waste, and reduce the cost of MEC service providers in actual operation. This exploration has important reference value for promoting the energy-saving development of Internet-related industries."
Enhanced Service Framework based on Microservice Management and Client Support Provider for Efficiency User Experiment in Edge Computing Environment,"The paper titled “Enhanced Service Framework based on Microservice Management and Client Support Provider for Efficiency User Experiment in Edge Computing Environment” addresses an important topic, which is microserver management edge computing.  The paper is very well-presented and has significant contributions. However, the following minor issues need to be addressed and the article can be accepted after revisions.
a-	A sentence should not start with “also” as in the abstract. 
b-	In the title, the word efficiency looks ambiguous and may be replaced by efficient. 
c-	The authors discuss and compare edge and cloud briefly in the introduction. The article providing the use case and benefit of edge can be referenced here, 
Mobility-aware computational offloading in mobile edge networks: a survey
d-	Similarly, the augmentation of sensors and clod services in the introduction needs to refer to a very relevant article listed below
R. M. A. Haseeb-ur-rehman et al., ""Sensor Cloud Frameworks: State-of-the-Art, Taxonomy, and Research Issues,"" in IEEE Sensors Journal, doi: 10.1109/JSEN.2021.3090967.
e-	The contributions should be listed in itemized form at the end of introduction. 
f-	Most of the figures have very low font size that is not good for readability. Figure sizes, font sizes and pixel quality can be enhanced for better readability. Specially for figures 6, 7, 8. 
g-	Section 3 should be better organized in subsections
h-	Figure 1 should be 2 column and explained in detail. 
i-	The number of references is low. There are many relevant references missing from the study such as
Resource Based Direct Manipulation: A User-Centric Visual Interface for Operational Customization of Future Smart Appliances",Authentic,"Leveraging the edge computing paradigm, computing resources are deployed in the network edge to provide heterogeneous services. Edge computing delivers sensing and actuating services to the Internet from the constrained Internet of Things (IoT) devices. Meanwhile, management of various elements is provided by offloading sufficient computing and storage to the edge of the networks for the IoT environments such as home, factory, and private spaces without cloud servers. In this paper, we propose an enhanced service framework based on microservice management and client support provider for efficient user experiments in the edge computing environment. For providing the edge computing service and management in the network edge, this paper presents an edge-computing architecture that provides various functions through microservice modules on the edge platform engine. Through the microservices, the interfaces are provided to the client to access the device, data, and additional services. Using Docker, the microservice modules are deployed in the edge platform to provide the services. However, the services and management functions need to be presented to the clients based on the friendly user interfaces. For providing the user interfaces of the services and Docker engine to the clients, the client support service provider is developed and deployed in the network edge based on the edge platform. Therefore, the proposed edge platform provides the services and management to the users for accessing the resources and functions through visualized interfaces in the IoT environment based on edge computing. The performance of our proposed system can be checked through the test result screen and delay time. Compared to controlling edge computing by using a command-line tool for users, we made it easy for general users who are not computer savvy to access edge services through a graphic user interface. And by measuring the delay time and comparing the execution time, it can be seen that the proposed system operates faster."
Toward Automated Security Analysis and Enforcement for Cloud Computing using Graphical Models for Security,"The paper titled “Toward Automated Security Analysis and Enforcement for Cloud Computing using Graphical Models for Security” addresses an interesting topic, which is graphical security models for cloud computing.  The paper is very well-presented and has significant contributions. However, the following minor issues need to be addressed.
1.	The authors state in the abstract that “We present additional techniques to collect security information from the cloud, which is then stored in a database.” What are these additional techniques? ,
2.	The concept of graphical security framework needs explanation. What would be an example of non-graphical security framework? 
3.	Why is related work divided into 3 sub sections? Which one is most related to the proposed work? It is hard to understand the state of the art from the current organization of section 2. It be made concise with fewer studies while focusing on highly related works. 
4.	There are several built-in security tools in Kali Linux which can be used for security assessment. How does this framework distinguish from it? What similar tools have been utilized?  
5.	What are the limitations of this work? The authors should state future research directions to conclude. 
6.	The number of references is low. There are many relevant references missing from the study such as
Secure?CamFlow: A device?oriented security model to assist information flow control systems in cloud environments for IoTs
A lightweight and compromise?resilient authentication scheme for IoTs",Authentic,"Cloud computing has become widely adopted by businesses for hosting applications with improved performance at a fraction of the operational costs and complexity. The rise of cloud applications has been coupled with an increase in security threat vectors and vulnerabilities. In this paper, we propose a new security assessment and enforcement tool for the cloud named CloudSafe, which provides an automated security assessment and enforce best security control for the cloud by collating various security tools. To demonstrate the applicability and usability of CloudSafe, we implemented CloudSafe and conducted security assessment in Amazon AWS. Also, we analyzed four different security countermeasure options in depth; Vulnerability Patching, Virtual Patching, Network Hardening and Moving Target Defence. Virtual Patching, Network Hardening and Moving Target Defence were determined to be feasible with regards to deployment implementation for the project. Proof of concepts were developed demonstrating the effectiveness of each feasible countermeasure option. These results indicate that the proposed tool CloudSafe is effective and efficient in helping security administrators to select optimal countermeasures to secure their cloud by conducting an in-depth security assessment."
Server Placement and Job Allocation for Load Balancing in Edge Computing Networks,"The paper titled “Server Placement and Job Allocation for Load Balancing in Edge Computing Networks” addresses an important topic, which is server placement in 5G edge networks.  The paper is very well-presented and has significant contributions. However, the following minor issues need to be addressed.
a-	It is evident from the abstract that language needs editing and proofing. For example the following sentence can be broken into two or rephrased for better readability,
In this paper, by solving these problems separately, the Lagrangian duality theory with the subgradient method is adopted to solve job allocation, and the feasible locations of edge servers are found with simulated annealing. 
b-	The authors need to discuss what benefits edge computing brings while referencing to the article,
Mobility-aware computational offloading in mobile edge networks: a survey
c-	The authors discuss the problem in the introduction, but do not discuss the already detailed solutions in the literature. This is necessary for any article
d-	The contributions should be listed in itemized form at the end of introduction. 
e-	A study on SLA aware job placement should be added to the related work. Do authors address SLAs in problem formulation? Refer to the following study
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds
f-	What is the criteria of user assignment to an edge server? I feel this discussion is missing. 
g-	In the section of comparative algorithms, the third one should not be named CPLEX as it is an IBM tool for optimization problems. 
h-	The number of references is low. There are many relevant references missing from the study such as
Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey",Authentic,"Offloading tasks to cloud servers has increasingly been used to provide terminal users with powerful computation capabilities for a variety of services. Recently, edge computing, which offloads tasks from user devices to nearby edge servers, has been exploited to avoid the long latency associated with cloud computing. However, edge server placement and task allocation strongly affect the offloading process and the quality of a user’s experience. Therefore, appropriately deploying the edge servers within a network and evenly allocating the workload to the servers are vital. This paper thus considers both the workload of edge servers and the distances involved in offloading tasks to these servers. To improve the user experience, edge server locations are carefully selected and the workload for the servers are allocated in a balanced manner. This scenario is formulated as a mixed-integer linear programming problem, and a novel solution that searches for the best server placement using simulated annealing while integrating task allocation using the Lagrangian duality theory with the sub-gradient method is proposed. Numerical simulations verify that the proposed algorithm can achieve better results than conventional heuristics."
Provisioning computational resources for cloud-based e-learning platforms using deep learning techniques,"The paper titled “Provisioning computational resources for cloud-based e learning platforms using deep learning techniques” addresses an important topic, which is cloud based e learning platforms.  The paper is not well-presented and has the following major issues to be addressed.
a-	The cloud based systems are not a good solution for e learning scenario depicted in this article. Edge based systems are better providing near user resources. Research and industry are rapidly migrating towards the edge. The authors should atleast provide a comparison of both with advantages of each. Also refer to the article
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?
b-	While the authors discuss the SLA aware resource provisioning techniques, several important references from IEEE access are missing such as,
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment Models
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds
c-	The authors abruptly come to the end of introduction while skipping many important details. What are previous solutions? What problems does this study focus on and how? These should be detailed more in the introduction. 
d-	There is no detail of the organization of the article at the end of intro.  
e-	What parameters from the table 2 are used in the NN models? 
f-	There is no discussion on the limitations of this research and the future direction. 
g-	Energy efficiency should also be considered in the cloud resource provisioning, Refer to the article
Sustainable Cloud Data Centers: A Survey of Enabling Techniques and Technologies",Authentic,"The use of e-learning technologies is growing even faster due to the existing conditions where virtual setups temporarily replace traditional classroom environments. Service infrastructure support for e-learning has moved to the cloud. For this reason, the efficient provisioning of resources for such platforms, which is achieved through prediction, is very relevant. The existing techniques for predicting the use of resources in the cloud are not designed with e-learning’s specific requirements. This paper presents a neural network-based model for predicting the usage of computational resources for e-learning platforms. This model consists of a series of interconnected neural networks used to predict values for variables of interest, such as Random Access Memory (RAM) usage and Central Processing Unit (CPU) usage. Using data collected from a high school real scenario, we analyzed and used it to train and validate our neural network-based model. This scenario consisted of a Moodle server deployed in a Google Virtual Machine with a configured course and its contents. Each student performed a series of activities while connected to it. Our proposed model achieves high accuracy. The obtained results are promising, paving the way towards constructing software tools for provisioning computational resources on demand for e-learning platforms."
ContextDrive: Towards a functional scenario-based testing framework for context-aware applications,"The article, ""ContextDrive: Towards a functional scenario-based testing framework for context-aware applications"" presents a testing framework for context aware applications. The article is well written and has defined contributions. There are some issues with respect to presentation and results that should be addressed.
1.	A abstract does not detail why testing context aware applications is different or a challenge. Why we need to look at testing of context aware application in a different way? 
2.	The authors state in the abstract that their framework will reduce testing time. Have authors not experimented? If no, why a statement is added that has no supporting data?
3.	Keyword based testing is not defined in the introduction.
4.	The authors need to list their contributions at the end of introduction in a bullet form
5.	The related work is very lengthy. It should be shortened to only very relevant articles. Moreover, the paragraphs are long. Two related works can be merged into one paragraph. Or scenario based testing works can be organized in one para. 
6.	Figure 1 text is and aesthetics should be improved. 
7.	Same is the case for other figures such as 2 and 3. 
8.	The following article is relevant and can be cited
A survey on Test Suite Reduction frameworks and tools",Authentic,"Context-aware applications are emerging applications in the modern era of computing. These applications can determine and adapt to situational context to provide better user experience. Testing these applications is not straightforward. Constantly changing nature of context makes testing context-aware application is a challenging task. To uncover a defect in context-aware application, a test engineer needs activity (sequence of actions) and context information (context data); this makes test case development a difficult task. Conventional test case development methodologies do not cater for context information. Besides, conventional applications have only one input source, but context-aware application must obtain data from many sources to infer the context. Yet another issue that these applications often face is the noisy data problem as input data collected from physical sensors could be noisy. Test adequacy criteria are used as test stoppage rule and define the quality of testing as well as for generating test suites. Test adequacy criteria is helpful to control the cost of testing as well as determining/establishing confidence in the software product quality. A number of test adequacy criteria exist for testing conventional applications, but the same is not true for context-aware applications. Defining test adequacy criteria and test coverage measures for context-aware applications warrants further research. Several techniques have been developed by researchers to generate and execute test cases for context-aware applications; however, end-to-end testing and result analysis of executed test cases still remains a grueling task for the test engineers. The aim of this study is to automate end-to-end functional testing, analysis of the generated test results as well as functional/requirement coverage assessment. Moreover, we also present a confidence assessment template for result analysis. Test engineers can use our proposed framework to assess the requirement coverage. Our proposed framework will reduce testing time, efforts and cost thus enabling test engineers to execute more testing cycles to attain higher degree of test coverage."
A Minimized Latency Collaborative Computation Offloading Game under Mobile Edge Computing for Indoor Localization,"The paper titled “A Minimized Latency Collaborative Computation Offloading Game under Mobile Edge Computing for Indoor Localization” addresses an important topic, which is MEC offloading for indoor localization tasks. The difference between this work and the existing works is clear because the proposed approach is not only more efficient than the existing works but also achieves new features that are not realized by the existing works. The paper is well-presented but it has the following issues. 

a-	the authors need to describe the technology used for indoor localization. Is it GPS, wifi or BLE? This needs to be done in introduction. 
b-	What are the computation overheads in indoor localization? What task is compute intensive and needs to be offloaded? Indoor localization looks a simple with no explanation in the introduction.  
c-	The introduction does not describe the state of the art existing research, the consequent research gap and the contributions of this article. In short, the intro is too brief.
d-	Did author define the requirements of the indoor localization? Is it realtime? Compute/storage demanding? When requirements are defined, we can define the solution accordingly.
e-	Several relevant works emphasizing the importance of edge computation need to be referenced in the introduction such as
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?
f- The related work section needs extension and more discussion on cloud, edge, local execution mechanisms. One such article is
Analysis of Vector Code Offloading Framework in Heterogeneous Cloud and Edge Architectures
f-	The paper contains some grammatical errors and typos that have to be modified.
g-	It seems from the table 1 that BLE is least accurate and most energy efficient. Have authors considered this in their system design?
h-	What was the simulation tool?
i-	 Please include some future work in the last section of the paper.
j-	In the modeling of the system, following reference can be helpful
Computation Offloading Cost Estimation in Mobile Cloud Application Models",Authentic,"Indoor localization has become one of the fundamental services that is required in a diverse set of applications these days, such as patient monitoring and smart parking. Highly accurate localization techniques impose high latency and high energy consumption on the underlying application system. Thus, for such indoor location-based application, offloading the computation of the localization process to a remote server with high resource capability has been recently introduced as an avenue to address such a challenge. In this paper, a computation offloading problem is formulated to find the optimal decision with regard to the operation of the localization process. This decision includes: a) Where to compute the localization task, either locally on the end device or on the edge server or on the cloud server, b) Which localization technique should be used, and finally, c) Which transmission technology is recommended to be chosen in combination with the localization technique. All these decisions are constrained by the device, and the servers resource capabilities load. They are also constrained by the fact that the localization algorithm has to satisfy a certain application QoS requirement. Within such context, three algorithms are proposed for task offload decision making. First, the Indoor Localization Latency Optimal Offloading algorithm, which finds the optimal offloading decision that minimizes the total latency of the system and is considered a benchmark for the other algorithms. Second, Indoor Localization Latency Centralized Offloading algorithm that finds a sub optimal solution with lower complexity. Third, Indoor Localization Latency Game-Theoretic Offloading decentralized algorithm that converges after finite improvement steps and achieves Nash equilibrium. Altogether, the paper finds the optimum localization strategy for all users with the minimum latency under mobile edge computing environment."
ESCOVE: Energy-SLA-Aware Edge-Cloud Computation Offloading in Vehicular Networks,"The article, "" ESCOVE: Energy-SLA-Aware Edge-Cloud Computation Offloading in Vehicular Networks"" presents a energy and sla aware edge cloud offloading for vehicular networks. The article is well written and has defined contributions. There are some issues with respect to presentation and results that should be addressed.
1.	Explain how offloading can help in efficient fuel utilization and infotainment services as stated in the introduction.
2.	Several important references are missing related to the article. 
3.	While authors introduce the concept of latency reduction with MEC, a very relevant reference is missing provided below,
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?
4.	The main objective of offloading is energy savings and latency reduction. Many research articles focus on this. The authors should not state that energy consumption has not been focus of offloading.  Please refer to the article,
Analysis of Vector Code Offloading Framework in Heterogeneous Cloud and Edge Architectures
5.	The authors do not list state of the art research in the direction of energy-sla aware offloading. Therefore, the research gap in existing literature is poorly defined in the introduction. Please correct this. Maybe, a hybrid cloud MEC offloading framework is the contribution and should be highlighted accordingly. 
6.	The 2-3 contributions should be highlighted at the end of introduction in bulleted format. 
7.	The literature review should also be expanded to detail recent state of the art research in energy sla aware offloading. 
8.	Mobility of vehicular nodes is an important parameter to be considered. However, the system model does not seem to consider it. Refer to the following article for detailed review on mobility based offloading
Mobility-aware computational offloading in mobile edge networks: a survey
9.	Similarly mobility model is not given in table II.",Authentic,"The vehicular network is an emerging technology in the Intelligent Smart Transportation era. The network provides mechanisms for running different applications, such as accident prevention, publishing and consuming services, and traffic flow management. In such scenarios, edge and cloud computing come into the picture to offload computation from vehicles that have limited processing capabilities. Optimizing the energy consumption of the edge and cloud servers becomes crucial. However, existing research efforts focus on either vehicle or edge energy optimization, and do not account for vehicular applications’ quality of services. In this paper, we address this void by proposing a novel offloading algorithm, ESCOVE, which optimizes the energy of the edge–cloud computing platform. The proposed algorithm respects the Service level agreement (SLA) in terms of latency, processing and total execution times. The experimental results show that ESCOVE is a promising approach in energy savings while preserving SLAs compared to the state-of-the-art approach."
Energy Efficiency Analysis of Post-Quantum Cryptographic Algorithms,"The article, ""Energy Efficiency Analysis of Post-Quantum Cryptographic Algorithms"" presents an analysis of cryptographic algorithms. There are many issues with respect to presentation and results that should be addressed.
1.	The contributions of the article are limited with analysis of existing algorithms. The authors can add contributions while making the scripts of the analysis open-source.
2.	The references are not according to the IEEE standard.
3.	The related work is very limited. If there is no PQC energy analysis, the authors can include conventional cryptography energy analysis. The fact that the related work is limited also does not highlight the contributions of the author. 
4.	The results are too extensive and therefore hard to analyze. Multiple variants of one algorithm have been selected which does not help the problem. Such variants can be included in supplementary material if supported by the journal of a technical report sent to arxiv like repositories. 
5.	The algorithm is not according to IEEE publication format.  

6.	Several important references during the discussion of energy-efficient cryptographic algorithms are missing such as, 
A multi?attack resilient lightweight IoT authentication scheme",Authentic,"Classical cryptographic schemes in use today are based on the difficulty of certain number theoretic problems. Security is guaranteed by the fact that the computational work required to break the core mechanisms of these schemes on a conventional computer is infeasible; however, the difficulty of these problems would not withstand the computational power of a large-scale quantum computer. To this end, the post-quantum cryptography (PQC) standardization process initiated by the National Institute of Standards and Technology (NIST) is well underway. In addition to the evaluation criteria provided by NIST, the energy consumption of these candidate algorithms is also an important criterion to consider due to the use of battery-operated devices, high-performance computing environments where energy costs are critical, as well as in the interest of green computing. In this paper, the energy consumption of PQC candidates is evaluated on an Intel Core i7-6700 CPU using PAPI, the Performance API. The energy measurements are categorized based on their proposed security level and cryptographic functionality. The results are then further subdivided based on the underlying mechanism used in order to identify the most energy-efficient schemes. Lastly, IgProf is used to identify the most energy-consuming subroutines within a select number of submissions to highlight potential areas for optimization."
Containers in edge computing: concept and scheduling models,"The article, ""Containers in edge computing: concept and scheduling models"" presents a survey on use cases of containers in edge computing. The article is well written and has defined contributions. There are some issues with respect to presentation and results that should be addressed.
1.	A survey requires references uniformly placed to compliment discussed topics. The article does not place references at the end of some discussions. For example, at the end of first para of intro where cloud computing is discussed, a reference is missing. You can add the reference, “Survey of Techniques and Architectures for Designing Energy-Efficient Data Centers”  
2.	In the second paragraph, the author discusses the power of edge computing to bring computation nearer to end users. Again, a very important reference debating the same idea can be added. Please refer to “Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?”
3.	The article provides good debate on the use cases of edge computing. However, the research issues arising from placing the computation power near to end user need more attention. For example, the research questions of what services to replicate, where (BS) to replicate, and when to migrate near user are the crux research questions in edge computing. The geo-temporal popularity of services and content is very dynamic and needs to be predicted. Such research challenges need to be highlighted. Please refer to a similar discussion in the article “Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey “
4.	Figure 4 need more detail in technical terms. 
5.	In section IV, ML based solutions should be highlighted as they provide more answers to the edge dynamics. 
6.	Section VI should be divided into two sections. One should be future research challenges. Other should be conclusion. 
7.	The industrial edge frameworks porposed by Microsoft and Google should be discussed in section II to highlight industrial use case of edge.",Authentic,"Containers are a form of software virtualization, rapidly becoming the de facto way of providing edge computing services. Research on container-based edge computing is plentiful, and this has been buoyed by the increasing demand for single digit, milliseconds latency computations. A container scheduler is part of the architecture that is used to manage and orchestrate multiple container-based applications on heterogenous computing nodes. The scheduler decides how incoming computing requests are allocated to containers, which edge nodes the containers are placed on, and where already deployed containers are migrated to. This paper aims to clarify the concept of container placement and migration in edge servers and the scheduling models that have been developed for this purpose. The study illuminates the frameworks and algorithms upon which the scheduling models are built. To convert the problem to one that can be solved using an algorithm, the container placement problem in mostly abstracted using multi-objective optimization models or graph network models. The scheduling algorithms are predominantly heuristic-based algorithms, which are able to arrive at sub-optimal solutions very quickly. There is paucity of container scheduling models that consider distributed edge computing tasks. Research in decentralized scheduling systems is gaining momentum and the future outlook is in scheduling containers for mobile edge nodes."
A Meta-analytical Review on Energy Efficient Computing -- Focus on Mobile Devices and Cloud Infrastructure,"The article, ""A Meta-analytical Review on Energy Efficient Computing --Focus on Mobile Devices and Cloud Infrastructure"" presents a meta analytic review of energy efficient computing with focus on mobile and cloud. The article is well written and has defined contributions. There are some issues with respect to presentation and results that should be addressed.
1.	In the abstract requires more detail. What was the motivation of the study? What was the time period of the literature review? 
2.	The motivation of this study is also missing in the introduction. Existing meta analysis studies are also not described. 
3.	Did the authors consider applications and workloads utilized in the literature? This information seems missing in the introduction. If overlooked, it may have significant relation with energy consumption as the authors have analyzed the case with the choice of programming language.  
4.	Have there been any previous meta analytic study on energy consumption of mobile devices? There is no reference to such study in section 2. The authors should comprehensively search repositories to detail and differentiate from any previous studies. 
5.	While discussing cloud and mobile device energy, the authors miss various important references such as,
A systems overview of commercial data centers: initial energy and cost analysis
Greening emerging IT technologies: techniques and practices",Authentic,"Cloud computing is becoming widely adopted, which increases both its user's efficiency and the provider's profitability. Virtualization is assisting the cloud in increasing resource utilization, but it is also increasing operational expenses due to increased energy usage. This paper studies and sheds light on the contemporary modern day research on cloud energy management techniques such as ballooning, virtual machine migration, resource allocation (RA), EEEHVMC (Energy Efficiency Heuristic with Virtual Machine Consolidation), three-way decision (VMM-3WD), and others that can be implemented in fundamental software level, virtualization and various applications and the operating system itself. It studies various measures to reduce the energy usage and clearly contributes to minimizing existing environmental problems and curbing it. This paper clearly explains each approach, including its benefits, drawbacks, and future prospects, taking into account modern day parameters. This article also shed light on the burdensome parameters and criteria's that cloud organizations and cloud data centers must follow."
A Deep Reinforcement Learning-based Dynamic Computational Offloading Method for Cloud Robotics that Considers Application-Input Data Size,"The article, ""A Deep Reinforcement Learning-based Dynamic Computational Offloading Method for Cloud Robotics that Considers Application-Input Data Size"" presents an DRL based offloading framework. There are many issues with respect to presentation and results that should be addressed.
1.	There is no need to include “that considers input size” in the title. Moreover, the emphasis on the input data size seems missing from the contributions and the research gap highlighted in the related works section.  
2.	The authors state that a robot is equipped with limited computing capabilities. In my opinion, robots have necessary resources to carry out all tasks. Can author provide a comparison of the robots specifications in terms of CPU, GPU and RAM? And compare it with a nearby geo-located cloud specs.
3.	There is no discussion of previous works in the same direction i.e., cloud robotics collaborative frameworks in the intro. Specifically, ML aided decision making in offloading. The contributions can not be focused and emphasized without highlighting the issues in previous studies. 
4.	References are not placed at appropriate places in the introduction. 
5.	While defining Edge computing and the applications of ML in edge, the authors miss various important references such as,
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution? 
Applying Machine Learning Techniques for Caching in Edge Networks: A Comprehensive Survey",Authentic,"Robots come with a variety of computing capabilities, and running computationally-intense applications on robots is sometimes challenging on account of limited onboard computing, storage, and power capabilities. Meanwhile, cloud computing provides on-demand computing capabilities, and thus combining robots with cloud computing can overcome the resource constraints robots face. The key to effectively offloading tasks is an application solution that does not underutilize the robot's own computational capabilities and makes decisions based on crucial cost parameters such as latency and CPU availability. In this paper, we formulate the application offloading problem as a Markovian decision process and propose a deep reinforcement learning-based deep Q-network (DQN) approach. The state-space is formulated with the assumption that input data size directly impacts application execution time. The proposed algorithm is designed as a continuous task problem with discrete action space; i.e., we apply a choice of action at each time step and use the corresponding outcome to train the DQN to acquire the maximum rewards possible. To validate the proposed algorithm, we designed and implemented a robot navigation testbed. The results demonstrated that for the given state-space values, the proposed algorithm learned to take appropriate actions to reduce application latency and also learned a policy that takes actions based on input data size. Finally, we compared the proposed DQN algorithm with a long short-term memory (LSTM) algorithm in terms of accuracy. When trained and validated on the same dataset, the proposed DQN algorithm obtained at least 9 percentage points greater accuracy than the LSTM algorithm."
Cloud-Edge Collaboration Feature Extraction Framework in Satellite Multi-access Edge Computing,"The article, ""Cloud-Edge Collaboration Feature Extraction Framework in Satellite Multi-access Edge Computing"" presents an MEC collaborative framework. There are many issues with respect to presentation and results that should be addressed.
1.	There is no reference to the discussion of SMEC in the 2nd paragraph of the introduction. This makes the whole concept of the article and the contributions ambiguous.
2.	Similarly, third paragraph of the introduction also does not have any reference where several are required. 
3.	Are edge servers and satellite servers being considered the same? Or it is a three-way collaboration framework between cloud edge and satellite? If it is a three-way collab, then the description on p2, line 33 onwards needs to be re-written. 
4.	The contributions are repeated and the proposed framework is presented repeatedly. The authors should list point wise itemized contributions and remove redundancy.
5.	 As it is evident from the proposed solution, there is too much partitioning of the overall tasks into cloud and edge components. What will be the effect on communication overhead? Have authors analyzed it in results?
6.	There is no discussion of previous works in the same direction i.e., cloud edge collaborative frameworks in the intro. The contributions can not be focused without highlighting the issues in previous studies. 
7.	While defining MEC, the authors miss various important references such as,
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution? 
8.	The proposed framework is partial offloading in MEC. The authors also miss references of MEC offloading such as,
Analysis of Vector Code Offloading Framework in Heterogeneous Cloud and Edge Architectures
9.	Figure 2 lists depicts that the extractor is working on both cloud and edge which does not make the concept clear. 
10.	Why does accuracy change when the proposed work is just a collaborator framework of the overall image processing setup?
11.	Considering the SMEC, the traffic is not important, rather latency is. The authors should have measured latency.",Authentic,"Mobile image recognition based on satellite multi-access edge computing can meet the needs of users in the coverage of nonterrestrial networks, and provide a great convenience for people's production and lives. Most existing methods extract features of the image data at the satellite servers, and only upload the extracted features to the cloud server for further processing, so as to reduce bandwidth pressure from satellite servers to cloud servers and response time. However, in most of these methods, the feature extraction on the satellite server is separated from the cloud server. This leads to the lack of effectiveness of the extracted features, which cannot meet the user's requirements for high accuracy of image recognition. To this end, we propose a cloud-edge collaboration feature extraction framework, in which the feature extraction on the low earth orbit satellite server is performed with the assistance of the cloud server. In the proposed framework, we first use the data set stored on the ground cloud server to generate a feature extractor E that learns how to extract effective features by exploring the distribution of the data set. Then, the cloud server sends extractor E to the satellite server to extract features of the data on the satellite server. In this way, extractor E can extract the effective features of the image data on the satellite server. Experimental results show that our method can achieve similar or even better recognition accuracy with uploading raw image data when only uploading a small amount of feature data. In addition, compared with existing methods, our method reduces the network traffic by about 98% in the case of similar recognition accuracy."
Hybrid Auto-Scaled Service-Cloud Based Predictive Workload Modeling and Analysis for Smart Campus System,"The article, ""Hybrid Auto-Scaled Service-Cloud Based Predictive Workload Modeling and Analysis for Smart Campus System"" presents a workload modeling for smart campus system. The article is well written with defined contributions. However, some issues with respect to presentation should be addressed.
1. The difference of horizontal and vertical scaling should be clear in abstract. 
2. The algorithms used for workload prediction and auto scaling should be highlighted in the abstract. 
3. The introduction starts with a paragraph on IoT while there is no mention of IoT in the abstract. 
4. The full form of IoT is used multiple times after the abbreviation has been defined. 
5. The references have been used scarcely in the introduction especially during the discussion of smart campus. 
6. See the following sentence for mistake, 
There are two days to….
7. Several important references are missing during the discussion of SLA and VM management such as, 
Characterizing Dynamic Load Balancing in Cloud Environments Using Virtual Machine Deployment Models
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds
8. The authors need to define the contributions point wise at the end of the introduction. 
9. The related work is very length. It should be to the point where the authors can define the research gap. In current form it looks more suitable for a literature review of survey. 
11. The dataset used for user request should be elaborated. 
12. The figure 3 should be changed to a block diagram instead of flowchart. 
13. Figure 4 for workload (user requests) presents a very simple scenario. There is no time series value to elaborate the arrival of request. OR show workload in different times of day based on campus working hours. 
14. The workload also depicts no “service-bursts” as highlighted in the abstract.
15. No results show the performance of the server handling workload. It can be in terms of MIPS, CPU + memory utilization, etc.
16. SLA violation discussed in intro has not been debated in results. 
17. The F1 score, ROC, recall etc lead to similar analysis for measuring the accuracy of the prediction. The authors have portrayed ony one dimension of analysis in multiple manners. Only one would have sufficed.",Authentic,"The Internet of Things is an emerging technology used in cloud computing and provides many services of the cloud. The cloud services users mostly suffer from service delays and disruptions due to service cloud resource management based on vertical and horizontal scalable systems. Adding more resources to a single cloud server is called vertical scaling, and an increasing number of servers is known as horizontal scaling. The service-bursts significantly impact the vertical scaled environment where the scale-up degrades the service quality and users' trust after reaching the server's maximum capacity. Besides, the horizontally scaled environment, though being resilient, is cost-inefficient. It is also hard to detect and manage bursts online to sustain application efficiency for complex workloads. Burst detection in real-time workloads is a complicated issue because even in the presence of auto-scaling methods, it can dramatically degrade the application's efficiency. This research study presents a new bursts-aware auto-scaling approach that detects bursts in dynamic workloads using resource estimation, decision-making scaling, and workload forecasting while reducing response time. This study proposes a hybrid auto-scaled service cloud model that ensures the best approximation of vertical and horizontal scalable systems to ensure Quality of Service (QoS) for smart campus-based applications. This study carries out the workload prediction and auto-scaling employing an ensemble algorithm. The model pre-scales the scalable vertical system by leveraging the service-load predictive modeling using an ensemble classification of defined workload estimation. The prediction of the upcoming workload helped scale-up the system, and auto-scaling dynamically scaled the assigned resources to many users' service requests. The proposed model efficiently managed service-bursts by addressing load balancing challenges through horizontal auto-scaling to ensure application consistency and service availability. The study simulated the smart campus environment model to monitor the time-stamped diverse service-requests appearing with different workloads."
Intelligent Autoscaling of Microservices in the Cloud for Real-time Applications,"The article, “Intelligent Autoscaling of Microservices in the ….."" details an Edge offloading framework with soft deadline applications. The article has many issues mainly related to the performance analysis as listed below and can not be accepted in current form. 
1.	The authors have not defined the concept of microservice which is necessary.
2.	The references should be put at the end of the sentence rather than the middle. 
3.	The authors do not mention existing ML works focused on autoscaling cloud services. The research gap in existing works should be the motivation for this research. 
4.	What ML tasks are performed for Twitter analystics? What is their complexity?
5.	The section 2 is very short with only one main equation. It should be expended or merged with other sections. 
6.	The article is formatted in a poor manner. The figures should accompany text rather than come at the end.
7.	The figure 1 show that the tweets are classified for disaster management. The authors should lookup to articles and see whether it is classification or clustering. 
8.	It seems that authors are optimization some QoS requirements based on a reinforcement learning technique. The optimization model is missing in that case that explains the system states and I/O along with their mathematical form.  
9.	If the authors are using reinforcement learning to predict some system variables, it should be stated so. The introduction does not shed light on the exact application of reinforcement learning in this work.
10.	The algorithms should not come in the section of experimental setup. 
11.	The number of agents described in the section IV.1 are too many which shows a complex system requiring ML tasks at each step. Normally one or two of these agents is applied in an article. The authors should explain there use cases in  a better manner avoiding just 4-5 liners for each. Moreover the authors should discuss the impact on the performance of the system due to so many ML agents. 
12.	The related works section is too long and should be summarized around articles that are relevant to the article. This section should also highlight the research gap in existing ML based cloud autoscaling works. 
13.	The results do not show comparison with any existing works listed in related works section. The article’s contributions are not valid without comparison. 
14.	The author miss many important references for the cloud scaling and ML based task scheduling in Edge cloud including the following,
Applying machine learning techniques for caching in edge networks: A comprehensive survey
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment 
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds",Authentic,"Cloud applications are becoming more containerized in nature. Developing a cloud application based on a microservice architecture imposes different challenges including scalability at the container level. What adds to the challenge is that cloud applications impose quality of service (QoS) requirements and have various resource demands requiring a customized scaling approach. For example, real-time applications require near real time response time as a QoS. Existing autoscaling technologies such as Kubernetes offer some customization to a set of threshold values for autoscaling. The challenge is identifying the right values for the different autoscaling parameters that will guarantee QoS in a changing dynamic environment. Advancements in machine learning and reinforcement learning (RL) provides a means for autoscaling in cloud applications with no domain knowledge. In this article, we introduce an intelligent autonomous autoscaling system for microservices autoscaling in the cloud with QoS constraints. The system consists of two modules. The first module identifies the microservice resource demand via a generic autoscaling algorithm deployed on the Google Kubernetes Engine (GKE). Our algorithm adapts the Kubernetes autoscaling paradigm based on the application resource requirements. The second module uses reinforcement learning agents to learn and identify the autoscaling threshold values based on the resource demand and QoS. Experimental results show an enhancement in the microservice response time up to 20% compared to the default autoscaling paradigm. In addition, the RL agents can identify the autoscaling threshold values while maintaining a response time below the QoS constraint. Our proposed work provides a customized autoscaling solution for microservices in cloud applications while adhering to QoS constraints with minimum user interaction."
Joint Power and QoE Optimization Scheme for Multi-UAV Assisted Offloading in Mobile Computing,"The article, “Joint Power and QoE Optimization Scheme for Multi-UAV……” details a framework for UAV assisted offloading. The main issues of the article are listed below. 
1.	In the first paragraph of the introduction, the authors describe end to end delay for UE as main concern leading towards MCC and MEC. However, the energy is the main concern that derives MCC/MEC offloading. 
2.	It is not clear what massive UE/UAV means in the abstract. The authors should describe it appropriately. 
3.	The related work is limited. Although it describes the research gap, it should be enhanced to include more studies. 
4.	There is not detail of GA solving such offloading problems previously in related work. Similarly, there is no detail of ML algorithms (DRL specifically) providing solutions for MEC/MCC. Refer to the following detailed study for this, https://arxiv.org/abs/2006.16864 
5.	 Simulation environment matlab etc is not mentioned. 
6.	There are several missing references regarding MEC offloading such as 
Analysis of Vector Code Offloading Framework in Heterogeneous Cloud and Edge Architectures
Bringing Computation Closer toward the User Network: Is Edge Computing the Solution?",Authentic,"Recent years, unmanned aerial vehicles (UAVs) have attracted much attention for providing intermediate relay to ground mobile user equipments (UEs) for their flexible mobility. UEs can offload computing-intensive task to mobile cloud computing (MCC) or mobile edge computing (MEC) for fast processing. However, with multi-UAV and ground mobile UEs in the system, heterogeneous performance requirement as well as fast-changing communication condition make the system more complicated. Meanwhile, both UEs and UAVs are battery-driven. How to optimize the energy efficiency for UEs' transmission and UAVs' position should be carefully considered. Since this is a non-convex and mixed-integer optimization problem, a heuristic joint power and quality of experience (HJPQ) algorithm is proposed in this article, where the UEs' offloading delay, MIMO channel, transmission power, as well as UAVs' placement are jointly optimized. The numeral simulations not only reveal the effectiveness of HJPQ, but also guarantee the great quality of experience (QoE) performance for UEs with different priorities. Furthermore, the comparison experiments with random assignment and deep deterministic policy gradient (DDPG) show the superiority of HJPQ in lower complexity, faster convergence, shorter offloading delay as well as higher energy efficiency."
A Renewable Energy-Aware Power Allocation For Cloud Data Centers: A Game Theory Approach,"The article, “A Renewable Energy-Aware Power……….” details a game theoratic approach for CDC power allocation. The article needs few considerations before consideration for publication. The main issues of the article are listed below. 
1.	The introduction needs frequent references. The second paragraph of the intro should be updated with proper reference.
2.	How can a CDC cause a blackout? This needs explanation 
3.	The authors state that CDC will be responsible for 8% of worldwide electricity. The figure looks unrealistic as most undeveloped countries may not have any CDC at all. The authors should check multiple references to make a balanced statement. 
4.	While stating the main contributions, the authors make an exaggerated statement. Many studies have focused on self generated renewable energy for CDC operations.  
5.	The authors state that they define 5 CDC priorities but list only 3. 
6.	The authors do not explain why the power allocation is a non-cooperative game between smart grid and renewable energy.
7.	The introduction does not fully explain the research gap before jumping to the main contributions. 
8.	The works listed in section II.2 are not properly power allocation studies. Some are VM placement studies with focus on energy efficiency. The authors should revise and include specifically power allocation studies.
9.	The authors do not detail the application workload for simulation setup. 
10.	The renewable energy profile can be based on profiles of some cities with consideration of time of the year. 
11.	Several relevant works and surveys are not referenced in the article such as,
Sustainable Cloud Data Centers: A survey of enabling techniques and technologies
A Systems Overview of Commercial Data Centers: Initial Energy and Cost Analysis
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds",Authentic,"With the rapid emerging of Internet of Things (IoT) devices and the proliferation of cloud-based applications, the cloud computing industry is becoming a vital element for ensuring our daily services. However, cloud computing uses large scale data centers equipped with energy-hungry servers and huge power facilities that massively consume power. This presents a real challenge which can negatively influence the power grid, while exposing the environment to global warming issues. Therefore, minimizing cloud data center power consumption is a challenging problem and has to be addressed. In this paper, we look at renewable energy in the context of a smart grid–cloud architecture and investigate the issue of grid power dispatching to cloud data centers. Since cloud data centers have a non-cooperative nature regarding power demand from the power stations, we model our power allocation problem as a non-cooperative game. Afterwards, we prove the existence and the uniqueness of Nash equilibrium. Moreover, we formulate the payoff function of our game as a non-linear optimization problem before resolving it using Lagrange multipliers and Karush–Kuhn–Tucker (KKT) conditions. Thus, we determine the assigned optimal quantity to each data center based on three main criteria : renewable energy usage, number of critical running applications and workload charge. Extensive simulations are performed by comparing our scheme with an existing work. Results show that our scheme outperforms the comparing approach with a percentage of 31.2% in terms of power load rate and significantly reduces emissions of carbon dioxide."
Actor Critic Based Energy Efficient Compute-Intensive Workload Allocation in Data Centers,"The article, “Actor Critic Based Energy Efficient……….” details a workload allocation technique in data centers. The application of ML in workload allocation techniques is new and appreciated. However, the article needs revision before consideration for publication. The main issues of the article are listed below. 
1.	The first sentence of abstract is not correct. DC are being deployed while energy efficient and renewable energy based logistics are employed for sustainability. Moreover, DC workload is also moving towards distributed Edge computing model. The authors also negate the statement in first sentence of the introduction. Rephrase the first sentence of abstract.   
2.	Correct on p2 line 47--?Augmented Realit/
3.	Again at the start of the introduction, the authors detail a flawed concept. AR/VR, AI etc do not always depend on CC. Edge computing is also gaining considerable amount of workload.  
4.	The authors address a multi-disciplinary topic in terms of application. On one hand they apply ML algorithms for workload allocation in DC. The state-of-the-art research in this direction (deep learning in particular) should be clearly stated. On the other hand they jointly minimize SLA violation and energy consumptions. The state-of-the-art works and the research gap in this domain should also be subjectively stated in the introduction and related work.
5.	The authors often state that the existing works are not scalable. This point of view needs to be elaborated based on specific referenced articles. 
6.	The contributions of the article should be summarized. Currently they are detailed. 
7.	The formatting is wrong at many places. There is no whitespace while starting sentences on multiple occasions. 
8.	There is no discussion on the scalability of the proposed algorithm in results section. If there are no results to support the idea, it should not be discussed in previous sections.
9.	Similarly, SLA violations are not discussed in the results section. What was the impact of the proposed algorithm on SLA violations?
10.	The RR and best fit workload allocation techniques are very basic and many advanced techniques have been proposed in cloud data centers. The authors should compare their proposed work with recent and advanced techniques such as those discussed in section 2. 
11.	 Many important references in this article are missing such as,
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment Models
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds
A systems overview of commercial data centers: initial energy and cost analysis",Authentic,"Recently the huge amount of energy consumption has become a barrier to the widespread deployment of data centers serving various Internet of Things applications. The reasonable allocation of compute-intensive workloads to physical servers is an efficient way to improve the data center's energy efficiency. Though existing works has proposed some algorithms to manage workloads or virtual machines for energy saving, most of them did not comprehensively consider the high dynamics of server states, and lacked in high scalability in their implementation. In this paper, the Actor Critic based Compute-Intensive Workload Allocation Scheme (AC-CIWAS) is proposed, which can both guarantee the Quality of Service (QoS) of workloads and reduce the computational energy consumption of physical servers. To achieve rational workload allocation, AC-CIWAS captures the dynamic feature of server states continuously, and takes the impact of different workloads on energy consumption into consideration. AC-CIWAS employs the Deep Reinforcement Learning (DRL) based Actor Critic (AC) algorithm to evaluate the expected cumulative return over time, while the cumulative return guides to allocate workloads with high energy efficiency. Simulation results have demonstrated that compared to existing baseline allocation methods, the proposed AC-CIWAS can achieve an approximately 20 percent decrease in server power consumption with QoS guarantee."
DECA: a Dynamic Energy cost and Carbon emission-efficient Application placement method for green edge clouds,"The article, “DECA: a Dynamic Energy cost and Carbon emission-efficient…..” details a framework for deep learning based object detection. The article has many issues. The foremost is the presentation including references of the article which is not according to IEEE standard. The main issues of the article are listed below. 
1.	The title is consumed by synonyms such as green cost efficient and carbon efficient. This should be revised.  
2.	There is no reference in the second paragraph of the introduction. 
3.	The problem specified is similar to the workload placement and energy optimizations in geo-distributed clouds. Therefore, the authors should specify why the problem is more complex in EC (refer to the discussion in para 3 of introduction).
4.	The flow of concepts in the introduction can be improved. Moreover, several paragraphs are small and can be merged with other paragraphs containing same concept. 
5.	The following sentences are very poorly written considering technical writing.
The applications are assumed to be made of independently deployable components, for example in the form of containerized microservices. Our task is to determine for each component where it should be placed (in which of the ECs, on which CN).
6.	Figure 1 is very basic. It should be revised or removed. 
7.	The introduction does not discuss what is the use case of A* search and fuzzy set technique. Which problem do they solve in this article?
8.	The intro also does not discuss the research gap in existing work. Infact, no existing work is discussed in introduction.
9.	The introduction does not list the contributions of the article. The details of the proposed work are redundant and not concise. 
10.	There is no need to formulate the 9 point characteristics of the system in introduction.  
11.	The concept of EC is similar to cloudlet. What is the difference if any? And what are some solutions to green cloudlets?
12.	The system model is too complex to comprehend with a long list of mathematical notations as listed in table 2. This should be simplified. The compactness of algorithms depict that the mathematical notations can be simplified. 
13.	The phases in figure 4 should not be illustrated in red colors. It looks like the color of some errors. 
14.	The long list of algorithms shows that the authors are solving multiple problems. For the ease of presentation and readability, these can be divided into separate manuscripts. It will also reduce the complexity of understanding the mathematical notations, scenarios, and results. 
15.	The details of each scenario should be presented in a table. For example cost, energy source, nodes etc.
16.	Several notable works in the field are missing from references such as,
Characterizing Dynamic Load Balancing in Cloud Environments using Virtual Machine Deployment Models
SLA-Aware Best Fit Decreasing Techniques for Workload Consolidation in Clouds
Greening emerging IT technologies: techniques and practices",Authentic,"Asanincreasing amount of data processing is done at the network edge, high energy costs and
 carbon emission of Edge Clouds (ECs) are becoming signi cant challenges. The placement of application
 components (e.g., in the form of containerized microservices) on ECs has an important effect on the energy
 consumption of ECs, impacting both energy costs and carbon emissions. Due to the geographic distribution
 of ECs, there is a variety of resources, energy prices and carbon emission rates to consider, which makes
 optimizing the placement of applications for cost and carbon ef ciency even more challenging than in
 centralized clouds. This paper presents a Dynamic Energy cost and Carbon emission-ef cient Application
 placement method (DECA) for ECs. DECA addresses both the initial placement of applications on ECs
 and the re-optimization of the placement using migrations. DECA considers geographically varying energy
 prices and carbon emission rates as well as optimizing the usage of both network and computing resources
 at the same time. By combining a prediction-based A* algorithm with a Fuzzy Sets technique, DECA makes
 intelligent decisions to optimize energy cost and carbon emissions. Simulation results show the ability of
 DECAin providing a tradeoff and optimizing energy cost and carbon emission at the same time"
A Novel Energy Proficient Computing Framework for Improving Renewable Energy Utilization and Distribution among Cloud Computing Devices,"The article, “A Novel Energy Proficient Computing Framework for…..” details a framework for renewable energy utilization in cloud computing. The article has many issues. The foremost is the presentation including references of the article which is not according to IEEE Access standard. Second is the use of k-means without any explainable logic. The main issues of the article are listed below. 
1.	There is no word as distributionis as written in abstract. 
2.	What does the author mean by non-disposable toxic wastes? how non-renewable energy is responsible for this? 
3.	Cloud should not be written with capital C. 
4.	How is energy drain a fundamental requirement as stated in following sentence,
The fundamental requirements are the uninterrupted power supply and energy drain, the adversary of which results in computation failures and overloading.
5.	Reference 20 and 21 are same. Moreover the references are not according to the format. 
6.	Abbreviations are full form are used inconsistently. 
7.	Every wok in section 2 should not start with a new paragraph. Instead similar articles should be grouped together. 
8.	Section 2 does not define the research gap. There is no detailed discussion on what is the issue in these listed articles.
9.	The intuition and need of using k-means is not clear. Has any other research done clustering for the purpose intended by the author? There is no detail on the input to K-means, the size of k etc. 
10.	What is the objective of self analysis? This should be in section of performance evaluation.    
11.	The details of task model and device model are limited. 
12.	 Simulation environment matlab etc is not mentioned. 
13.	There are several missing references regarding renewable energy for data centers such as 
Greening emerging IT technologies: techniques and practices
Sustainable Cloud Data Centers: A Survey of Enabling Techniques and Technologies",Authentic,"Numerous green computing applications employ sustainable energy sources to abate redundant energy consumption. Renewable energy sources are vital to improving energy efficiency and should be used optimally. This paper introduces the Energy Proficient Computing Framework (EPCF) in the resource-centric cloud environment. The main objective of the EPCF is to improve the shared efficiency of energy distribution in the computing systems. Renewable energy is distributed among computers according to their running status and the number of calculations available. Traditional k-means clustering separates the states and computations when making this determination. This mapping procedure is repeated throughout the computation until the energy is dispersed without waste. Energy is conserved for the later use if the sources of the leak can be located in advance. As a result, we can conserve and use energy more effectively. In addition, it speeds up calculations and decreases service allocation waiting times. The proposed framework achieves 14.69% less energy cost for the different service al-location rates, 6.34% less energy drain, and 14.4% high efficiency."
Applications of dry chain technology to maintain high seed viability in tropical climates,"I commend you on your efforts in addressing the concept of the ""dry chain"" within the context of the seed area.

As I delved into the article, I found the discussion surrounding the dry chain to be intriguing. However, I believe there is an opportunity to enhance clarity regarding the distinction between the drying process and the dry chain concept. Providing a more explicit explanation of how the dry chain builds upon traditional drying methods and its unique attributes would greatly benefit readers in understanding its significance within the broader context of the seed process.

Furthermore, while reading through the article, I found myself curious about the scale of the dry chain technique implementation. It would be beneficial for readers to have a clearer understanding of the practical application of the dry chain, including details such as the capacity or volume it can accommodate. Specifically, providing insights into the number of seeds or seed bags the dry chain can handle would help readers assess its feasibility and potential impact in real-world scenarios.

As I delved into the article, I noticed that the section discussing unpublished results lacked detailed descriptions regarding the methodology employed. Specifically, there is a lack of clarity regarding how certain procedures were conducted, such as the germination test mentioned as an example. The calculus of how the dryer was adjusted is another. There isn't data about the initial humidity and the final one and how many times was used. The seeds are freshly harvested, if not how were stored. What means high-quality seed? It would be beneficial for readers to understand whether the germination test followed established protocols, such as those outlined by the International Seed Testing Association (ISTA), or if alternative methodologies were utilized.

Providing such details not only enhances the transparency and reproducibility of your research but also allows readers to contextualize the findings within established standards and practices within the field. Additionally, clarity regarding the methodology strengthens the credibility of the results and ensures they can be effectively evaluated and compared with other studies.

I kindly suggest revisiting the unpublished results section to incorporate more comprehensive descriptions of the methodologies employed, including specific references to relevant protocols or guidelines followed, such as ISTA rules for germination testing. This adjustment will greatly enhance the clarity and robustness of your findings, further contributing to the overall impact of your research.",Authentic,"Seed storage life in tropical areas is shortened by high humidity and temperature and the general inaccessibility to dehumidifying and refrigeration systems, resulting in rapid decreases in seed viability in storage as well as a high incidence of fungal and insect infestations. The dry chain, based on rapid and deep drying of seeds after harvest followed by packaging in moisture-proof containers, has been proposed as an effective method to maintain seed quality during medium-term storage in humid climates, even without refrigeration. In addition, seed drying with zeolite drying beads can be more effective and economical than sun or heated-air drying under these warm, humid conditions. In this paper, we review recent published literature regarding the dry chain, considering different crop species, storage environments and seed traits. In addition, we provide new original data on the application of dry chain methods and their implementation at larger scales in South Asia, Latin America and Pacific Island Countries. The clear conclusion is that the combination of reusable drying beads and waterproof storage containers enables the implementation of the dry chain in tropical climates, enhancing seed viability and quality in storage of many crop species. The dry chain approach can therefore significantly enhance seed security for farmers in many tropical countries. Finally, we propose actions and strategies that could guide further scaling-up implementation of this technology."
Genotype-specific responses to in vitro drought stress in myrtle (Myrtus communis L.): integrating machine learning techniques,"The manuscript entitled “Genotype-specific responses to in vitro drought-stress in myrtle (Myrtus communis L.): integrating machine learning techniques” provides a detailed account of the experimental findings related to the performance of superior myrtle genotypes under in vitro drought conditions has provided valuable insights into the adaptive capabilities of these plants. However, addressing the below-mentioned points would enhance its clarity, specificity, and impact.
1. The abstract provides an overview of the experimental findings, it lacks a clear structure that guides the reader through the different aspects of the study. Organizing the abstract into distinct sections (e.g., background, methods, results, conclusions) would improve clarity and facilitate understanding.
2. The abstract should conclude with a succinct summary of the main findings and their broader implications. It would be beneficial to provide interpretations or hypotheses in the abstract regarding the implications of these changes on stress tolerance mechanisms. Ensure the concluding sentences highlight the practical implications of your findings for cultivation practices under climate change.
3. The introduction section lacks research questions and a hypothesis. Please formulate a hypothesis. Keep in mind that the research objectives should stem from the research hypothesis. Additionally, ensure that the objectives are stated clearly.
4. In the methodology section, it is mentioned that the selection of the genotypes is vaguely described. Critical visual parameters are mentioned, but there is no mention of specific quantitative criteria or the number of plants initially screened. Provide more detailed information on the selection criteria.
5. The sterilization steps are clearly outlined, employing 70% ethanol and 20% sodium hypochlorite for sterilization. However, it raises a question as to why mercuric chloride, a commonly used and highly effective sterilizing agent, was not included in your protocol. The exclusion of mercuric chloride could potentially result in significant contamination issues. Could you please elaborate on the rationale behind not selecting mercuric chloride for your study?
6. The concentrations of PEG6000 used (1%, 2%, 4%, and 6%) might be too low to effectively simulate a wide range of drought conditions. Typically, higher concentrations are needed to induce more severe drought stress. Methodology does not provide a rationale for selecting these specific concentrations. There is no reference to previous studies or preliminary data that justify why these concentrations are appropriate for inducing drought stress in myrtle. The current range may not cover severe drought stress adequately, which could limit the understanding of the upper limits of myrtle genotypes' drought tolerance. While implied, the methodology does not explicitly mention a control (0% PEG6000) for baseline comparison. Based on standard practices in crop physiology, here is a suggested range of PEG6000 concentrations:
Low stress: 5-10% PEG6000 (For mild drought stress and initial screening).
Moderate stress: 15-20% PEG6000 (To differentiate between drought-tolerant and sensitive genotypes).
High stress: 25-30% PEG6000 (To simulate severe drought conditions and study the extreme responses).",Authentic,"Background: Myrtle (Myrtus communis L.), native to the Mediterranean region of Türkiye, is a valuable plant with applications in traditional medicine, pharmaceuticals, and culinary practices. Understanding how myrtle responds to water stress is essential for sustainable cultivation as climate change exacerbates drought conditions.

Methods: This study investigated the performance of selected myrtle genotypes under in vitro drought stress by employing tissue culture techniques, rooting trials, and acclimatization processes. Genotypes were tested under varying polyethylene glycol (PEG) concentrations (1%, 2%, 4%, and 6%). Machine learning (ML) algorithms, including Gaussian process (GP), support vector machine (SVM), Random Forest (RF), and Extreme Gradient Boosting (XGBoost), were utilized to model and predict micropropagation and rooting efficiency.

Results: The research revealed a genotype-dependent response to drought stress. Black-fruited genotypes exhibited higher micropropagation rates compared to white-fruited ones under stress conditions. The application of ML models successfully predicted micropropagation and rooting efficiency, providing insights into genotype performance.

Conclusions: The findings suggest that selecting drought-tolerant genotypes is crucial for enhancing myrtle cultivation. The results underscore the importance of genotype selection and optimization of cultivation practices to address climate change impacts. Future research should explore the molecular mechanisms of stress responses to refine breeding strategies and improve resilience in myrtle and similar economically important crops."
Licorice-root extract and potassium sorbate spray improved the yield and fruit quality and decreased heat stress of the ‘osteen’ mango cultivar,"Introduction: Commitment to writing the introduction according to what was stated in the first evaluation by placing paragraph (127–132), then paragraphs (142–153), then paragraphs (134–141), then paragraphs (176–184), and finally paragraphs (187-192).Action must be taken
Materials and Methods :The climatic conditions data are fixed in Table 2. Action have been taken
Experimental units for treatments in line have been fixed. Action have been taken
Authors have adhered to maintaining references, changing references that are not suitable for the place, and replacing them with appropriate references. Action have been taken
References: The authors have complied with the requested amendments in the list of references as well as the amendments recorded in the body of the manuscript.",Authentic,"Heat stress, low mango yields and inconsistent fruit quality are main challenges for growers. Recently, licorice-root extract (LRE) has been utilized to enhance vegetative growth, yield, and tolerance to abiotic stresses in fruit trees. Potassium sorbate (PS) also plays a significant role in various physiological and biochemical processes that are essential for mango growth, quality and abiotic stress tolerance. This work aimed to elucidate the effects of foliar sprays containing LRE and PS on the growth, yield, fruit quality, total chlorophyll content, and antioxidant enzymes of 'Osteen' mango trees. The mango trees were sprayed with LRE at 0, 2, 4 and 6 g/L and PS 0, 1, 2, and 3 mM. In mid-May, the mango trees were sprayed with a foliar solution, followed by monthly applications until 1 month before harvest. The results showed that trees with the highest concentration (6 g/L) of LRE exhibited the maximum leaf area, followed by those treated with the highest concentration (3 mM) of PS. Application of LRE and PS to Osteen mango trees significantly enhanced fruit weight, number of fruits per tree, yield (kg/tree), yield increasing%, and reduced number of sun-burned fruits compared to the control. LRE and PS foliar sprays to Osteen mango trees significantly enhanced fruit total soluble solids ?Brix, TSS/acid ratio, and vitamin C content compared to the control. Meanwhile, total acidity percentage in 'Osteen' mango fruits significantly decreased after both LRE and PS foliar sprays. 'Osteen' mango trees showed a significant increase in leaf area, total chlorophyll content, total pigments, and leaf carotenoids. Our results suggest that foliar sprays containing LRE and PS significantly improved growth parameters, yield, fruit quality, antioxidant content, and total pigment concentration in 'Osteen' mango trees. Moreover, the most effective treatments were 3 mM PS and 6 g/L LRE. LRE and PS foliar spray caused a significant increase in yield percentage by 305.77%, and 232.44%, in the first season, and 242.55%, 232.44% in the second season, respectively."
A novel gene silencing strategy based on tobacco rattle virus in Hibiscus mutabilis,"In the introduction of the manuscript, the justification of the research, hypothesis, research question, and purpose of the research are clearly stated. However, the literature regarding the study is sufficient.

However, information about which gap in the literature this study will fill should be clearly stated in the introduction of the manuscript.

Where and when was research conducted in the material methods section of the manuscript? This should be written.

I did not see a section on the statistical analysis of the data in the hand patch. This must be included in the manuscript. The following questions must be answered:.
According to which trial design was the data collected?
How many repetitions were the treatments made, and how many plants were analyzed in each repetition?

Why did you use only a single housekeeping gene, ""HmActin,"" as a control in the study? The use of a second housekeeping gene for validation will be important for the accuracy of the results of the real-time PCR study.",Generic,"Background: Hibiscus mutabilis L. is a popular regional characteristic plant in China, cultivated for its attractive flower colors, extended bloom time, and medicinal properties. To enhance molecular breeding and gene function studies, we conducted transcriptome analysis and identified valuable genes in previous research. Nonetheless, the current inefficient and labor-intensive transformation techniques have hindered their applications. Virus-induced gene silencing (VIGS) provides a precise and effective strategy for post-transcriptional down-regulation of endogenous gene expression.

Methods: We investigated the performance of tobacco rattle virus (TRV) as a tool for targeting and silencing the gene encoding the protein involved in chloroplast development, cloroplastos alterados 1 (altered chloroplast; CLA1), of H. mutabilis through Agrobacterium tumefaciens-mediated infiltration.

Results: By effectively suppressing the CLA1 gene associated with chloroplast development in H. mutabilis via the TRV-VIGS system, we have illustrated the inaugural implementation of VIGS in this species. Quantitative RT-PCR proved that HmCLA1 expression in agro-infiltrated plants was lower than in the mock-infiltrated (mock) and the control (CK) plants. Phenotypic observations corroborated the albino phenotype in leaves following successful HmCLA1 silencing.

Conclusions: Our study showcases TRV-VIGS as a potential gene silencing tool for H. mutabilis, facilitating functional genomics studies and molecular breeding efforts in this species."
Genome-wide association study reveals genetic basis and candidate genes for chlorophyll content of leaves in maize (Zea mays L.),"This research provides valuable insights into the genetic basis of chlorophyll content in maize. The findings may be useful for breeding maize varieties with improved chlorophyll content and potentially higher yield. Chlorophyll content is a physiological trait as it is a result of complex physiological processes within a plant, including photosynthesis, nutrient uptake and utilization, and hormonal regulation. By studying the genes that control CC, breeders can develop maize varieties with improved photosynthetic efficiency and yield.
Comments and Suggestions for Authors
- Abstract
-The abstract is well written.
- Introduction
The introduction could be strengthened by providing more specific examples of how increased chlorophyll content leads to yield improvements.
- The research gap could be more explicitly stated. For example, you could highlight the limited number of identified genes affecting chlorophyll content in maize compared to other crops.
- Materials & Methods
- This section is well-written and provides a good foundation for understanding the study's methodology.
- In line 125, (QTNs) When introducing a term or acronym for the first time in a text, it's essential to write it out fully to provide context for readers. Only after establishing the full term should you use the abbreviation or acronym. Correct to quantitative trait nucleotides (QTNs). Do the same with the whole manuscript.
- Results
- The results section provides a clear and comprehensive overview of the study's findings.
- Consider using more precise or varied vocabulary to enhance the text. For instance, instead of ""large variations,"" you could use ""substantial differences"" or ""significant variability.""
- Discussion
- The discussion section effectively summarizes the key findings of the study and places them in the context of existing research.
- Remove the subtitles in discussion section.
- While the discussion mentions a few candidate genes and their potential roles, it could be expanded to provide more in-depth analysis of the identified genes and their functional implications for chlorophyll content. This would strengthen the overall impact of the study.
- The discussion could be strengthened by outlining potential future research directions based on the study's findings. For example, the authors could discuss how their identified SNPs and candidate genes could be used in marker-assisted selection or gene editing to improve chlorophyll content and yield.",Authentic,"The chlorophyll content (CC) directly affects photosynthesis, growth, and yield. However, the genetic basis of CC is still unclear in maize (Zea mays L.). Here, we conducted a genome-wide association study using mixed linear model for CC of the fifth leaves at seedling stage (CCFSS) and the ear leaves at filling stage (CCEFS) for 334 maize inbred lines. The heritability estimates for CCFSS and CCEFS, obtained via variance components analysis using the lme4 package in R, were 70.84% and 78.99%, respectively, indicating that the CC of leaves is primarily controlled by genetic factors. A total of 15 CC-related SNPs and 177 candidate genes were identified with a p-value < 4.49 × 10-5, which explained 4.98-7.59% of the phenotypic variation. Lines with more favorable gene variants showed higher CC. Meanwhile, Gene Ontology (GO) analysis implied that these candidate genes were probably related to chlorophyll biosynthesis. In addition, gene-based association analyses revealed that six variants in GRMZM2G037152, GRMZM5G816561, GRMZM2G324462, and GRMZM2G064657 genes were significantly (p-value < 0.01) correlated with CC, of which GRMZM2G064657 (encodes a phosphate transporter protein) and GRMZM5G816561 (encodes a cytochrome P450 protein) were specifically highly expressed in leaves tissues. Interestingly, these candidate genes were previously reported to involve in the regulation of the contents of chlorophyll in plants or Chlamydomonas. These results may contribute to the understanding of genetic basis and molecular mechanisms of maize CC and the selection of maize varieties with improved CC."
PotatoG-DKB: a potato gene-disease knowledge base mined from biological literature,"The authors, using NLP and AI (LLM) created “PotatoKB”, a knowledge graph of entities and relationships associated with potato disease. While PotatoKB has potential, given the current state of both the resource webpage and the manuscript, it is not ready to be published. Perhaps PotatoKB can become a valuable resource in potato disease research (and even the approach applied to plant species beyond potato), and I encourage the authors to refine their resource, and manuscript.

The manuscript is not well written, the language need to be significantly improved for flow, clarity, and readability.

The literature review in the background is written very haphazardly and confusingly.

The table and figures are either not described or annotated, or are not particularly effective at communicating any information.

The methods are not sufficient, and especially code, supplemental data and intermediate results are not supplied, for example the dictionaries used in pre-processing and knowledge extraction, and the final ontology (only a screenshot of a subset is provided).

The resulting knowledge graph needs to be provided in a usable and interoperable format.",Generic,"Background: Potato is the fourth largest food crop in the world, but potato cultivation faces serious threats from various diseases and pests. Despite significant advancements in research on potato disease resistance, these findings are scattered across numerous publications. For researchers, obtaining relevant knowledge by reading and organizing a large body of literature is a time-consuming and labor-intensive process. Therefore, systematically extracting and organizing the relationships between potato genes and diseases from the literature to establish a potato gene-disease knowledge base is particularly important. Unfortunately, there is currently no such gene-disease knowledge base available.

Methods: In this study, we constructed a Potato Gene-Disease Knowledge Base (PotatoG-DKB) using natural language processing techniques and large language models. We used PubMed as the data source and obtained 2,906 article abstracts related to potato biology, extracted entities and relationships between potato genes and related disease, and stored them in a Neo4j database. Using web technology, we also constructed the Potato Gene-Disease Knowledge Portal (PotatoG-DKP), an interactive visualization platform.

Results: PotatoG-DKB encompasses 22 entity types (such as genes, diseases, species, etc.) of 5,206 nodes and 9,443 edges between entities (for example, gene-disease, pathogen-disease, etc.). PotatoG-DKP can intuitively display associative relationships extracted from literature and is a powerful assistant for potato biologists and breeders to understand potato pathogenesis and disease resistance. More details about PotatoG-DKP can be obtained at https://www.potatogd.com.cn/."
"Unveiling the impact of organically activated biochar on physiological, biochemical, and yield attributes of maize under varied field moisture conditions","1. Abstract: The abstract only contains some parameters without any process conditions or key values from results, which is insufficient to delineate the whole picture of the contribution and possible application of this study. It is suggested to add key values from the results, and possible applications of this study and highlight the novelty of this work clearly (200-250 words).
2. Revise keywords and add more specific and novel keywords with broader meanings (5-7 words).
3. Page numbers are missing in the manuscript.
4. The introduction and background are weak, no strong information is provided about the different types of the agricultural system and their effects on crops and wastewater and environmental pollution; therefore, the authors are advised to read, broader the literature review and add environmental pollution types, effects and levels from the following studies by extending literature
5. Page 8, line 157: “The surface area of biochar can be enhanced by its activation, which involves the improvement in its surface through organic or chemical treatments.” Which of the two mentioned treatments has proven to be more efficient?
6. Page 8. line 167: “Biochar production was undertaken in 2022 by utilizing raw material sourced from the gum Arabica tree (Acacia nilotica) at the University of Agriculture, Faisalabad.” Why was this specific tree chosen for biochar production? What properties make this tree suitable for this study?
7. Page 9, line 184: “The field experiment was conducted at location 32°38.37’N, 74°9.00’E (Gujrat) for exploring the suitable level of activated biochar under three moisture regimes (100% ETC, 70% ETC, and 50% ETC).” On which basis was this location chosen for conducting a field experiment?
8. Page 9, line 196: “The solution was centrifuged (5 minutes) and the upper layer was transferred to the clean cuvette of the spectrophotometer and absorbance was recorded at 663-nm, 645-nm and 470-nm.” Which RCF force was used during centrifugation?
9. The discussion presented is very weak no strong comparison has been made with the literature to support the authenticity of the obtained results. Therefore, the authors are suggested to discuss their results with the following recent researches about novel irrigation systems, water, nano fertilizers, their effect on production, soil properties and related pollution effects to make the background and discussion more strong
10. Page 14, line 368: “To cope with drought, plants evolved a variety of morphological, biochemical, and physiological adaptations.” Can you mention some of these adaptations?
11. Page 14, line 372: “However, by increasing its surface area and pore capacity, organic activation of the biochar further improves its ability to store water for longer periods of time.” Can you compare the durability of water storage when it comes to organic treatment versus some previously reported chemical treatment?
12. Page 16, line 447: “As a result, the leaves ultimately exhibited an increase in carbohydrate and amide concentrations (Gharred et al., 2022; Khan et al., 2021; Singh et al., 2020).” Avoid an abundance of references do not cite more than 2 references in a single place. Correct all these types of references throughout the manuscript.
13. The equations should be numbered and properly cross-referenced in the manuscript. Please correct this.
14. Revise figures in the manuscript. Draw all figures in high-quality figures should be coloured and attractive.
15. Please revise all the tables and figures captions for a better understanding. Should be comprehensive and meaningful.
16. Conclusion: The conclusions only talk about some studied parameters, which is insufficient to depict the whole picture of the contribution of this study. The authors are advised to write the conclusions in a comprehensive way and should contain key values, suitability of the applied method, the major findings, contributions and possible future outcomes (250-300 words).
17. References: The authors are advised to include the latest references. Please see some suggestions in the specific comments section. Further discussion should be modified with the latest references.
18. The authors are advised to include a list of abbreviations in the manuscript.",Authentic,"Water deficiency in semiarid regions is a limiting factor that affects crop quality and yield. In Punjab, Pakistan, a 27% decline in maize yield was detected over the past two decades just because of water scarcity. Currently, no studies have reported the effects of organically activated biochar (AB) on crop productivity under natural field conditions. For this purpose, a field experiment in a split-split-plot design was conducted with three amendment levels (0, 2, and 4 tonnes ac-1), and three maize hybrids (DK-9108, DK-6321, and Sarhaab) under 100%, 70%, and 50% irrigation water (IW) of crop evapotranspiration (ETc). The AB significantly improved the soil's physical and chemical properties, and maximum improvement was recorded in 4 tonnes ac-1 AB amendment in organic matter (16.6%), total organic carbon (17%), phosphorus (11.43%), and available potassium (29.27%). The 4 tonnes ac-1AB amendment in soil had a significant impact on total chlorophyll content (0.3-1-fold in DK-6321), carotenoid content (3.9-4.4-fold in Sarhaab), and relative water content (30% and 21% in Sarhaab) under 50% irrigation water (IW) of ETc at V14th and R3 stages, respectively. Moreover, a significant decline in stress markers (proline content and sugar content) was detected at both growth stages in all maize hybrids in AB amended soil. The analysis of plant metabolites indicated increased intensities of phenolics, alkyl esters, and carbohydrates by 2.5-7%, 17-80%, and 40-43% in DK-6321 under 50% IW in 2-4 tonnes ac-1 AB amended soil, respectively. The highest improvement in growth and yield attributes among maize hybrids was detected in the order DK-6321¿DK-9108¿Sarhaab in 2-4 tonnes ac-1AB amended soil under 70% and 50% IW of ETc, respectively. Hence, this research might help to develop an effective soil amendment to restore degraded soils and improve maize growth under arid climatic conditions."
Evaluation of rabi season sesame productivity from graded nutrient doses and tillage regimes in rice fallows of southern plateau and hills region of the Indian sub-continent,"Thanks for the invitation to evaluation the article 'Evaluation of post-rainy season sesame with graded nutrient doses and tillage regimes in rice fallows of southern plateau and hills region of the Indian sub-continent' for evaluation and possible publication in 'PeerJ'. I am happy to inform you that I have been able to check the whole manuscript to find out its suitability for publication and I confirm that it may be published after resolving the several issues which are listed below:
1. Abstract: The aim and treatments of the study are not clear and that should be mentioned clearly in the revised article.
2. Introduction: I suggested to incorporated latest information related to the current observation and also must be included aims of the study at the end of the section
3. M & M: All methodologies should be mentioned clearly
4. Results and Discussion: I suggest to authors to check data and related discussion based on all Tables and Figures
5. Conclusion: Authors should also add the challenges/limitations of the study in the section",Authentic,"Background: Only scattered information is available on the tillage and nutrient management information for the sesame crop following rice in the literature. Sesame as an edible oil yielding crop with high levels of unsaturated fatty acids has high international demand due to superior health benefits. Being a small seeded crop, it requires standard tillage and nutrient management to obtain optimum productivity under rice fallow ecologies. As a sequential crop after rice harvest, the tillage and nutrient management practices followed for the preceding rice have astounding effects on the succeeding sesame crop. To better understand and manipulate the agro ecology in the rice fallow culture, it is necessary to study the behaviour of sesame cultivars, in relation to the tillage requirements and macro nutrient factors that have a bearing on the productivity.

Methods: The aim of this work was to evaluate the productivity of rice fallow sesame in the southern plateau and hills regions of the Indian sub-continent (Tamil Nadu) with a hypothesis that tillage and nutrient management would immensely benefit the sesame crop. Field experiments were conducted at TNAU, Tamil Nadu Rice Research Institute, Aduturai, Tamil Nadu during 2019-2020 and 2020-2021 with tillage practices (reduced tillage, conventional tillage and zero tillage) and fertilizer doses (zero percent RDF, 25% RDF, 50% RDF, 75% RDF and 100% RDF) in a split plot design replicated thrice.

Results: The results have clearly indicated that the performance of rice fallow sesame was poor under zero till conditions as the sesame crop is poorly adapted leading to a yield penalty up to 68%. A total of 75% RDF has yielded statistically similar yield to that of 100% RDF to the rice fallow sesame. Further, neither the oil content nor the fatty acid composition was modified by tillage and nutrient management regimes."
"Morphological, histological and transcriptomic mechanisms underlying different fruit shapes in Capsicum spp.","The manuscript by Wang et al. rigorously describes fruit formation in ten Capsicum cultivars, which belong to five clearly distinctive fruit shapes. Additionally, the authors conduct a comprehensive transcriptome analysis on five of these cultivars using tissue collected at the anthesis stage. This study serves as a framework for exploring the diversity in fruit shape and their underlying transcriptome profiles in pepper.

As previously mentioned, the research objectives are clearly defined, the selected methods and plant materials are appropriate, and the investigation was conducted with rigorous procedures and protocols. The study aligns with the aims and scope of the journal and will be of interest to plant biologists in general, especially those studying fruit development processes.

The findings of this study provide descriptive and original insights into the morphological and gene expression variations associated with distinct fruit shapes in Capsicum. The authors conducted a comprehensive and rigorous investigation of fruit development across ten Capsicum cultivars, spanning from anthesis to full size. The results demonstrate originality and constitute a robust scientific study.

The authors have responded and incorporated most of the suggested changes. In summary, I believe that this version should be accepted for publication",Generic,"Pepper (Capsicum spp.) has a long domestication history and has accumulated diverse fruit shape variations. The illustration of the mechanisms underlying different fruit shape is not only important for clarifying the regulation of pepper fruit development but also critical for fully understanding the plant organ morphogenesis. Thus, in this study, morphological, histological and transcriptional investigations have been performed on pepper accessions bearing fruits with five types of shapes. From the results it can be presumed that pepper fruit shape was determined during the developmental processes before and after anthesis, and the anthesis was a critical developmental stage for fruit shape determination. Ovary shape index variations of the studied accessions were mainly due to cell number alterations, while, fruit shape index variations were mainly attributed to the cell division and cell expansion variations. As to the ovary wall thickness and pericarp thickness, they were regulated by both cell division in the abaxial-adaxial direction and cell expansion in the proximal-distal and medio-lateral directions. Transcriptional analysis discovered that the OFP-TRM and IQD-CaM pathways may be involved in the regulation of the slender fruit shape and the largest ovary wall cell number in the blocky-shaped accession can be attributed to the higher expression of CYP735A1, which may lead to an increased cytokinin level. Genes related to development, cell proliferation/division, cytoskeleton, and cell wall may also contribute to the regulation of helical growth in pepper. The insights gained from this study are valuable for further investigations into pepper fruit shape development."
Yield-limiting nutrient response of lowbush blueberry grown in recent and ancient alluvial soils of the Mekong Delta,"Ngo et al., Yield-Limiting Nutrient Response of Lowbush Blueberry Grown in Recent and Ancient Alluvial Soils of the Mekong Delta

I have reviewed the previous version of the manuscript, and am glad to see a revised version. I thank the effort by the authors and think that the manuscript has been improved to some degree. However, the responses are unclear and some of the revisions are not sufficient.

For the response to point 1.1, it mentioned that “The use of Trevett’s range was discussed in line 667-674.” But there are no lines 667-674 in the manuscript. The authors didn’t explain how they revised and discussed the potential issue. Some direct answers (responses) in the response letter will be helpful. In Lines 351-360 of the document with track changes, the authors only edited the writing, and didn’t add anything regarding the potential fitness and/or problems applying Vaccinium angustifolium threshold to Vaccinium tenellum. This needs to be carefully discussed, and the reason why Vaccinium angustifolium threshold could be applied to Vaccinium tenellum needs to be well-explained.

The writing and grammar still need to be improved more. E.g. “no specific threshold range has been found for evaluating leaf nutrient content for Vaccinium tellenum.” It reads like Vaccinium tellenum does not have a threshold range. It does have a threshold range, and it is just that it has not been studied. Here the correct word should be “established”, rather than “found”.

Introduction, the first sentence is exactly the same as the first sentence in Abstract. Please avoid repeating the same sentence. Please rewrite one of the sentences.

Line 42-43: Blueberries have a lot of species and varieties. Some are shade-torelant understory species. So please be specific and precise.

Line 70-72: “and respond differently to crop productivity.” It is that: crop productivity responds to soils with different soil fertility. Not that soil fertility responds to crop productivity.

In the response, the authors state that “Statistical results have been added to the figures of 4 and 6.” However, I still could not see your statistical results in the figures. The figure legends also did not describe the statistical tests used. If there is no significant difference, the authors also need to point it out.",Authentic,"Background: Blueberries are ornamental plants grown in pots in many yards in the Mekong Delta (MD) region. In this region, the recent alluvial (RA) soil is fertile and ancient alluvial (AA) soil is considered degraded because it only has around a quarter of the nutrient content of the RA soil. Both soils have a high clay content, so organic matter is needed to improve their physical condition. This study aimed to identify the nutrients that limit the yield of blueberries in RA and AA soils of the MD.

Methods: The pot experiment was performed using a factorial randomized block design (RBD) with two factors: (a) two soil types (RA and AA) and (b) four omission or treatment conditions (NPK, PK, NK, and NP). The same fertilizer formula was used for all treatments, including 45N-20P2O5-20K2O and mixing CHC (10 tha-1) into the potting soil.

Results: The blueberry yield in AA soil was only 81% of that in RA soil. In both RA and AA soils, N omission caused foliar N content deficiency (10.42 g kg-1), resulting in the content of foliar P (0.84 g kg-1) and K (3.78 g kg-1) to fall below the Trevett threshold. In both RA and AA, N omission resulted in reduced fruit yield (47% and 39%, respectively) as well as reduced weight of the stem (70% and 42%, respectively) and leaf (59% and 46%, respectively). Increased crop yields in soils were mainly related to nitrogen fertilizer. The indigenous nutrient supply (INS) of RA, which is fertile, was high but its apparent nutrient recovery efficiency (ARE) index was low, whereas the INS of AA, or the level of degraded soil, was low but its ARE index was high. In alluvial soils, the higher the INS level, the less positive the impact on the ARE index. In AA soil, the indigenous N and K supplies can be improved through fertilizer investment; however, a balance must be achieved considering economic efficiency."
Preliminary study on the association between lignan metabolites and CT non-destructive testing of coconut fruit at different developmental stages,"The current study aimed at finding out association between lignan metabolites and CT non-destructive testing of coconut fruit at different developmental stages. They have chosen two different varieties of coconut to find differences in metabolites. Figures are of importance but need to improve its pixels for the publication purpose. Authors should supply high quality of pics for publications. A through English proof reading is required.

The experiment is well planned and executed. Recording of data and analysis are proper. They have standard method of liquid chromatography to measure the levels of lignan metabolites in coconut water.

The experiment opens up new insight on the use of non-destructive method (CT-based). However it should correlated with its practical utility for example selection of a particular variety of plant after some breeding experiments.",Generic,"Lignans play a crucial role in maintaining plant growth, development, metabolism and stress resistance. Computed tomography (CT) imaging technology can be used to explore the internal structure and morphology of plants, and understanding the correlation between the two is highly significant. In this study, the content of lignan metabolites in coconut water was determined using liquid chromatography. The internal structure data of coconut fruit was obtained by CT scanning, and the relationship between lignan metabolites and CT image data at different developmental stages was evaluated using partial least square (PLS) regression. The results showed that the total lignan content in coconut water initially decreased, then increased, and gradually decreased after the maturity stage. The Wenye No. 5 variety exhibited higher levels of Epiturinol, Turbinol, Isobarinin-9?-o-glucoside, 5?-methoxy-rohanoside, Rohan rosin-4,4?-di-o-glucoside, turbinol-4-O-glucoside, cycloisoperinolin-4-O-glucoside compared to local coconuts. Coconut meat had the greatest effect on Rohan rosin-4,4?-di-o-glucoside, coconut water on Daphne, and coconut shell and coconut fiber on Larinin-4?-o-glucoside. The data from different parts of coconut fruit’s images showed a significant correlation with the content of lignan metabolites. This study has preliminarily explored the correlation between non-destructive testing of coconut fruit and its development process of coconut fruit, providing a new approach and method for further research on non-destructive testing of coconut fruit development."
Sobralia decora Bateman (Orchidaceae) and its relatives in South America,"The English language used throughout should be improved. It is clear but often poor. Furthermore, the writing is often allusive to taxonomic contentions that are perhaps familiar to a very specialized public but otherwise completely unknown to the general reader.

Considering that the manuscript would be a taxonomic “clarification” of the group, the illustration apparatus is amply insufficient to show the differences between the species, and reduced to floral diagrams, in one case also incomplete (just two parts of the petals’ row). It would be advisable that at least a photograph of a living flower should be included in the manuscript, not to dream about multiple photographs to suggest the range of natural variation in the concerned species.

Raw data are supplied.",Generic,"A taxonomical study of Sobralia decora and allied species recorded in South America is presented. The group is characterized by light pink to purple flowers, often with a white tip of the labellum and yellow to brown area on its throat. If considering the habit, the species can be recognized and distinguished from all other Sobralia species by producing keikis-stems arising from the old inflorescence. The similarity of the species forming the discussed complex caused numerous mistakes noticed in the literature and among the herbarium specimens. The aim of the elaboration is to clarify the differences allowing to determine the living and herbarium specimens with no errors."
Molecular and agro-morphological diversity assessment of some bread wheat genotypes and their crosses for drought tolerance,"The changes you made have improved your manuscript considerably, but you should also correct some minor points that were overlooked. After this, your manuscript will be accepted.
1. You should include the findings on the combining ability obtained from the diallel analysis in the abstract section. Positive and significant combining abilities are important in both parental selection and determining promising hybrids, as they indicate a contribution to the relevant trait. You should synthesize these with other findings and provide a conclusion.
2. Lines 94-100: You should state your purpose clearly here. You have made it complicated by cramming literature in between. Please edit and emphasize the focus of your study. Do not ignore that the combining ability results obtained from the diallel analysis helped you make your choice.
3. Lines 182-184: Clarify which statistical program you performed the Griffing analysis. Is it R?
4. Line 209: If you use the abbreviation ""P vs C"" here and only once, write it as ""parent vs crosses"" without abbreviating it.
5. Lines 410-422: Add literature.
6. The conclusion section needs some development. Reorganize it by paying attention to the points I have indicated in the abstract. Synthesize the results of the combining ability analysis obtained from Griffing analysis molecular and phenotypic results and make recommendations for future breeding studies for drought tolerance. For example, good parents and promising hybrids. In your response letter, briefly explain the importance of the marker-trait relationships you mentioned and how the data you obtained will be used in future studies.
7. Pay attention to the words that should be italicized throughout the text. For example, Triticum urartu",Authentic,"Wheat, a staple cereal crop, faces challenges due to climate change and increasing global population. Maintaining genetic diversity is vital for developing drought-tolerant cultivars. This study evaluated the genetic diversity and drought response of five wheat cultivars and their corresponding F1 hybrids under well-watered and drought stress conditions. Molecular profiling using ISSR and SCoT-PCR markers revealed 28 polymorphic loci out of 76 amplified. A statistically significant impact of parental genotypes and their crosses was observed on all investigated agro-morphological traits, including root length, root weight, shoot length, shoot weight, proline content, spikelet number/spike, spike length, grain number/spike, and grain weight/spike. The parental genotypes P1 and P3 had desirable positive and significant general combining ability (GCA) effects for shoot fresh weight, shoot dry weight, root fresh weight, root dry weight, shoot length, and root length under well-watered conditions, while P3 and P5 recorded the highest GCA estimates under drought stress. P3 and P4 showed the highest GCA effects for number of spikelets per spike, the number of grains per spike, and grain weight per spike under normal conditions. P5 presented the maximum GCA effects and proved to be the best combiner under drought stress conditions. The cross P1× P3 showed the highest positive specific combining ability (SCA) effects for shoot fresh weight under normal conditions, while P2×P3 excelled under water deficit conditions. P1× P2, P1 × P3, and P4× P5 were most effective for shoot dry weight under normal conditions, whereas P1×P3 and P3×P5 showed significant SCA effects under drought stress. Positive SCA effects for root fresh weight and shoot length were observed for P3×P5 under stressed conditions. Additionally, P4×P5 consistently recorded the highest SCA for root length in both environments, and P3×P5 excelled in the number of spikelets, grains per spike, and grain weight per spike under drought conditions. The evaluated genotypes were categorized based on their agronomic performance under drought stress into distinct groups ranging from drought-tolerant genotypes (group A) to drought-sensitive ones (group C). The genotypes P5, P2×P5, and P3×P5 were identified as promising genotypes to improve agronomic performance under water deficit conditions. The results demonstrated genetic variations for drought tolerance and highlighted the potential of ISSR and SCoT markers in wheat breeding programs for developing drought-tolerant cultivars."
Leucaena interspecific hybrid ‘KX4-Hawaii’ as a source of agricultural biomass in a water-scarce small island developing state,"Overall, it is a good piece of work and the manuscript is written well in most parts but there are a few points that need to be clarified.

1. First of all, KX4 is a giant leucaena, it is not a common leucaena. The authors may write a few lines explaining its difference from common leucaena. Please find the differences between giant and common leucaena explained in Bageel et al. (2020) (see attached).

2. Authors should also mention how KX4 was developed. Again, it is stated in Table 2 of Bageel et al. (2020).

3. Pruning or pollarding was done four times in 16 months. How many sprouts and shoots grew after each pruning?

4. How was biomass quantified? Generally, shoots or branches that are 5 millimeters and less, can be considered as green fresh biomass, while more than 5 mm considered woody biomass.

5. After each pruning, herbicide Roundup was sprayed to control weeds in the plots. Were the stumps of the pruned plants protected from herbicide? Or, did they also absorb some herbicide?

6. The following references, related to this work, may be added.",Authentic,"Background: Leucaena leucocephala is a useful multipurpose tree species for agroforestry systems, but traditional seeded cultivars often become weedy and invasive. A seedless hybrid cultivar, 'KX4-Hawaii', offers a potential solution to this problem. However, relevant agronomic information and information on the performance of 'KX4-Hawaii' under varying growth conditions is required. The goal of this research was to evaluate 'KX4-Hawaii' as a source of agricultural biomass in Barbados, a small island developing state with limited arable land.

Methods: 'KX4-Hawaii' air layers were imported into Barbados to create stock trees. Air layering was used to create propagation material and a field study was established with a 'KX4-Hawaii' hedgerow planted as a field border. Three plant spacings (50, 75, and 100 cm) were evaluated and data on the growth and biomass yields of the trees were collected at 4-month intervals. Precipitation data were used to investigate climatic effects on 'KX4-Hawaii' productivity.

Results: 'KX4-Hawaii' was successfully propagated via air layers and could be planted directly in the field with irrigation. All recorded growth and biomass yields were correlated with precipitation. However, the woody (lignified stems and branches) biomass was more responsive to precipitation than the green (leaves and green tender stems) biomass and made up a large fraction of the total biomass produced. 'KX4-Hawaii' was productive even under drought conditions and biomass yields per meter of hedgerow increased with closer spacings. Of the tested spacing treatments, 75 cm was optimum for a 4-month pruning interval under the conditions seen in Barbados as it produced similar yields to the 50 cm spacing treatment but would require less propagation material."
Using transcriptome sequencing (RNA-Seq) to screen genes involved in ?-glucan biosynthesis and accumulation during oat seed development,"* The manuscript needs major linguistic adjustments. In addition, the sentences should be written shorter and clearer by avoiding the use of unnecessary words.
• Lines 19-22: The sentence is badly written in standard English; accordingly, kindly reformulate it.
• Lines 166-170: The sentence is cumbersome; accordingly, kindly reformulate in order to make it clearer and more aiming.
• Line 177: “……..GC content The GC content…..” where dos the first sentence ends and where does the second starts?
• Lines 276-280: The sentence is cumbersome; accordingly, kindly reformulate in order to make it clearer and more aiming.
• Line 292: “.. module (r=0.93, p=9e-05)” what does p=9e-05 stands for?
• Lines 323-330: The sentence is cumbersome; accordingly, kindly reformulate in order to make it clearer and more aiming.
• Lines 337-341: The sentence is badly written in standard English; accordingly, kindly reformulate it.
• Line 349, 352, 365: Please write the references in the sentences according to the Journals instructions.
*Care should be taken in the use of abbreviations:
• Line 24: “Differentially expressed genes (DEGs)” not Differential expression genes…
• Line 120: The explanations of the abbreviations must be stated in the first place: “Fragments per kilobase million (FPKM)”
• There are so many abbreviations without explanations.
*Lines 15-34: The “Abstract” should be written as one section according to the journals instructions.
* Lines 31-33: “In total,………oat seeds”: Since the results given in the abstract part is insufficient, the conclusion made in this sentence seem to have no ground.
* Lines 36-37: A more recent literature must be cited or a recent data from FAO must be given for production value and ranking. Six to nine years past since the cited references.
*Lines 69-73, “In this study,…… biosynthesis”: The aim of the study must be putted on the line rather than mentioning what is done.
*Lines 74-76, “This study …… quality”: This sentence is more suitable for conclusion part.",Authentic,"Oat (Avena sativa L.) is an annual grass that has a high nutritional value and therapeutic benefits. ?-glucan is one of the most important nutrients in oats. In this study, we investigated two oat varieties with significant differences in ?-glucan content (high ?-glucan oat varieties BY and low ?-glucan content oat variety DY) during different filling stages. We also studied the transcriptome sequencing of seeds at different filling stages. ?-glucan accumulation was highest at days 6-16 in the filling stage. Differentially expressed genes (DEGs) were selected from the dataset of transcriptome sequencing. Among them, three metabolic pathways were closely related to the biosynthesis of ?-glucan by Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis, including xyloglucan:xyloglucosyl transferase activity, starch and sucrose metabolism, and photosynthesis. By analyzing the expression patterns of DEGs, we identified one CslF2 gene and 32 transcription factors. Five modules were thought to be positively correlated with ?-glucan accumulation by weighted gene co-expression network analysis (WGCNA). Moreover, the expression levels of candidate genes obtained from the transcriptome sequencing were further validated by quantitative real-time PCR (RT-qPCR) analysis. Our study provides a novel way to identify the regulatory mechanism of ?-glucan synthesis and accumulation in oat seeds and offers a possible pathway for the genetic engineering of oat breeding for higher-quality seeds."
"Vepris amaniensis: a morphological, biochemical, and molecular investigation of a species complex","n general, the taxonomic content is clearly presented and justifies the two principal conclusions: 1) that a number of specimens previously included in Vepris amaniensis do not correspond to the original concept of that species; and 2) that those specimens, despite their variability in several usually diagnostic characters, are most properly considered as a single new species, Vepris usambarensis. Tracked changes on pages 1-6 of the manuscript concern only minor corrections that need no further comment. Beginning with the new species description on page 7, however, some more substantive issues arise:

1. On page 7, line 248, the authors must add the abbreviation “sp. nov.” or equivalent to indicate their intention to publish a new species.
2. Vepris usambarensis is described as a new species with J. Lovett 263 at K as its holotype. It would be highly desirable to cite the barcode number of this specimen if it has one.
3. The holotype of Teclea amaniensis, Warnecke in Herb. Amani 516K, was destroyed at Berlin. A neotype, Salmon 171, has been designated at K, with an isoneotype at EA. A direct comparison of the holotype of V. usambarensis and the neotype of Vepris/Teclea amaniensis is essential to fix the application of the two names. This could logically be placed in one of two places: 1) immediately following the holotype designation on page 7; or 2) Under “Recognition” on pages 9-10. Regardless of placement, this really needs to be done.
4. The statements about the presence or absence of stamens/staminodes in the descriptions of the female flower on lines 284, 285, and 290 seem to contradict each other and must be resolved.
5. The section “Representative specimens examined” is very confused in content and presentation.
a. Specimens are listed in seemingly random order, and it would be highly desirable to group the cited specimens together by country and political district.
b. District names for Tanzanian specimens are irregularly indicated – sometimes by the old designations used in the Flora of Tropical East Africa, sometimes by their modern equivalents, and sometimes not at all. I have corrected these to the best of my ability and am confident that they are all accurate now.
c. A few post facto estimated geographic coordinates for older specimens are given to an excessive degree of precision, e.g. with seconds of latitude or longitude given to three or four decimal places. It is not possible to be that precise with the kind of data available, and these should be rounded to the nearest whole minute of latitude or longitude.
d. Two specimens are cited with only a District name and estimated coordinates, but no definite locality. These specimen labels should be checked for the locality details.
6. The summary of geographical distribution is somewhat confused and misleading. Based on the cited specimens, all Tanzanian collections came from four mountain ranges: East Usambara, West Usambara, Nguru, and Uluguru. I have edited the text accordingly.",Authentic,"Vepris Comm. ex A. Juss. is a genus of 96 species extending from Africa to India that are distinct in their unarmed stems and their digitately (1-)3(-5) foliolate leaflets, and whose many secondary compounds earn them uses in traditional medicine. Mziray (1992) subsumed six related genera into Vepris, with Vepris amaniensis (Engl.) Mziray becoming somewhat of a dustpan for ambiguous specimens (Cheek & Luke, 2023). This study, using material from the Kew herbarium, sought to pull out novel species from those previously incorrectly filed as Vepris amaniensis, and here describes the new species Vepris usambarensis sp. nov. This species is morphologically distinct from Vepris amaniensis with its canaliculate to winged petioles, 0.5-2.3 cm long inflorescences, 1-3 foliolate leaflets, and hairs on inflorescences and stem apices. Phytochemical analysis attributed seven compounds to Vepris usambarensis: tecleanthine (1), evoxanthine (2), 6-methoxytecleanthine (3), tecleanone (4), 1-(3,4-methylenedioxyphenyl)-1,2,3-propanetriol (5), lupeol (6), and arborinine (7). This is a unique mixture of compounds for a species of Vepris, though all are known to occur in the genus, with the exception of 1-(3,4-methylenedioxyphenyl)-1,2,3-propanetriol (5) which was characterized from a species in the Asteraceae. An attempt at constructing a phylogeny for Vepris using the ITS and trnL-F regions was made, but these two regions could not be used to differentiate at species level and it is suggested that 353 sequencing is used for further research. Originally more than one new species was hypothesized to be within the study group; however, separating an additional species was unsupported by the data produced. Further phylogenetic analysis is recommended to fully elucidate species relationships and identify any cryptic species that may be present within Vepris usambarensis."
Heterotic grouping of wheat hybrids based on general and specific combining ability from line × tester analysis,"I have carefully reviewed the article “Heterotic grouping based on general and speciûc combining ability in line × tester wheat hybrids” where I find the study a good research work towards crop improvement programme. Though studies have been reported till date regarding wheat breads but one study was found earlier mentioned in the comment section which is close to this current study which needs an extensive explanation. Moreover, some major mistakes are found which are mentioned below therefore my suggestion will be that to go thoroughly through the whole manuscript and rectify as needed.
In abstract part
1. Keywords are missing in the abstract part
In introduction part
1. In Line 36 is not well written. The importance of Wheat crop is needed worldwide where turkey is also included. So it is not required to mention separately. kindly rewrite it
2. In line 39, Here the report is from 2015, Can you add a recent report?
3. In 51-52, Add a reference here. If a certain informative line is provided it is good to add a reference
4. Lack of reference in this paragraph. From line 49-57, there is only one reference for the whole paragraph. It is good to cite more references for any informative lines. kindly add
5. In line 93, Can you explain what is the difference between current study and this study Kutlu, ?., & S?rel, Z. (2019). Using line× tester method and heterotic grouping to select high yielding genotypes of bread wheat (Triticum aestivum L.). ?
6. In the end part of Introduction, Mention elaborately about this current study how it is novel from other studies?",Authentic,"The most important step in plant breeding is the correct selection of parents, and it would be wise to use heterotic groups for this. The purpose of this study is to analyse yield and its components as well as genetic diversity in line × tester wheat populations. It also seeks to present a coherent framework for the isolation of early superior families and the development of heterotic groups in bread wheat. F1 and F2 generations of 51 genotypes, including 36 combinations between 12 lines and three testers and 15 parents, were evaluated for yield and its components in a three-replication experiment according to the randomized block design. Line × tester analysis of variance, general and specific combining abilities, heterosis, heterobeltiosis and inbreeding depression were calculated. Heterotic groups created based on general and specific combining abilities were compared with each other. The results showed that there was sufficient genetic variation in the population and that further genetic calculations could be made. The selections made based on general and specific combining abilities, heterosis values and average performance of genotypes without heterotic grouping indicated different genotypes for each feature. The creation of heterotic groups made it possible to select genotypes that were superior in terms of all the criteria listed. It was concluded that heterotic groups created based on specific combining abilities may be more useful for breeding studies."
Environmental risks and agronomic benefits of industrial sewage sludge-derived biochar,"The research question is well-defined and relevant to the field of environmental science and agriculture.
The investigation is thorough, covering various aspects of the impact of biochar on soil and plant health. The methods are described in sufficient detail to be replicated by others.

The study has a significant impact as it addresses the ecotoxicological safety of using treated sewage sludge in agriculture, which is a pressing environmental issue. The study appears to be statistically sound, with appropriate controls and statistical tests mentioned.",Generic,"The main objective of the present work was to assess the ecotoxicological safety of the use of thermochemically treated sewage sludge from the wastewater treatment plant (WWTP) of a distillery plant as a soil additive in agricultural soils based on its physicochemical characteristics and the bioaccumulation of selected elements in the plant tissues of maize (Zea mays). We have carried out physicochemical characterization (pH, EC, Corg, Cinorg, CEC, N, H, ash content, PAHs) of sewage sludge feedstock (SS) and sludge-derived biochar (BC) produced by slow pyrolysis at a temperature of 400 °C. The effect of 1% (w/w) amendment of SS and BC on soil physicochemical properties (pH, EC, Cinorg), germination of ryegrass, soil rhizobacteria and microorganisms, as well as on the accumulation and translocation of selected elements in maize (Zea mays) was studied. The results show that pyrolysis treatment of distillery WWTP sludge at 400 °C increases pH (from 7.3 to 7.7), Corg(from 28.86% to 36.83%), N (from 6.19% to 7.53%), ash content (from 23.59% to 50.99%) and decreases EC (from 2.35 mS/cm to 1.06 mS/cm), CEC (from 118.66 cmol/kg to 55.66 cmol/kg), H (from 6.76% to 1.98%) and ?18 PAHs content (from 4.03 mg/kg to 3.38 mg/kg). RFA analysis of SS and BC showed that pyrolysis treatment multiplies chromium (Cr) (2.2 times), nickel (Ni) (2.96 times), lead (Pb) (2.13 times), zinc (Zn) (2.79 times), iron (Fe) (1.26 times) in the obtained BC, but based on an ecotoxicological test with earthworms Eisenia fetida, we conclude that pyrolysis treatment reduced the amount of available forms of heavy metals in BC compared to SS. We demonstrated by a pot experiment with a maize that a 1% addition of BC increased soil pH, decreased EC and Cinorg and had no significant effect on heavy metal accumulation in plant tissues. According to the results of the three-level germination test, it also does not affect the germination of cress seeds (Lepidium sativum). There was a significant effect of 1% BC addition on soil microbial community, and we observed a decrease in total microbial biomass and an increase in fungal species variability in the soil. Based on these results, we conclude that BC represents a promising material that can serve as a soil additive and source of nutritionally important elements after optimization of the pyrolysis process."
Transcriptomic and coexpression network analyses revealed the regulatory mechanism of Cydia pomonella infestation on the synthesis of phytohormones in walnut husks,"Very well written. My only comment for improvement is that figure 3 is very busy. Suggestions: Show one pathway per graph with different time points so that any changes can be observed. Only need the key in one place rather than on each graph, as it detracts from viewing the data.
This is a solid paper with very clear data, well described context, and a path forward for future investigations.",Generic,"The codling moth (Cydia pomonella) has a major effect on the quality and yield of walnut fruit. Plant defences respond to insect infestation by activating hormonal signalling and the flavonoid biosynthetic pathway. However, little is known about the role of walnut husk hormones and flavonoid biosynthesis in response to C. pomonella infestation. The phytohormone content assay revealed that the contents of salicylic acid (SA), abscisic acid (ABA), jasmonic acid (JA), jasmonic acid-isoleucine conjugate (JA-ILE), jasmonic acid-valine (JA-Val) and methyl jasmonate (MeJA) increased after feeding at different time points (0, 12, 24, 36, 48, and 72 h) of walnut husk. RNA-seq analysis of walnut husks following C. pomonella feeding revealed a temporal pattern in differentially expressed genes (DEGs), with the number increasing from 3,988 at 12 h to 5,929 at 72 h postfeeding compared with the control at 0 h postfeeding. Walnut husks exhibited significant upregulation of genes involved in various defence pathways, including flavonoid biosynthesis (PAL, CYP73A, 4CL, CHS, CHI, F3H, ANS, and LAR), SA (PAL), ABA (ZEP and ABA2), and JA (AOS, AOC, OPR, JAZ, and MYC2) pathways. Three gene coexpression networks that had a significant positive association with these hormonal changes were constructed based on the basis of weighted gene coexpression network analysis (WGCNA). We identified several hub transcription factors, including the turquoise module (AIL6, MYB4, PRE6, WRKY71, WRKY31, ERF003, and WRKY75), the green module (bHLH79, PCL1, APRR5, ABI5, and ILR3), and the magenta module (ERF27, bHLH35, bHLH18, TIFY5A, WRKY31, and MYB44). Taken together, these findings provide useful genetic resources for exploring the defence response mediated by phytohormones in walnut husks."
Exogenous melatonin promoted seed hypocotyl germination of Paeonia ostia ‘Fengdan’ characterized by regulating hormones and starches,"This study explored the effects of melatonin on the physiological mechanisms of peony seed germination and has original innovation. Overall, this manuscript has clear English, sufficient background, and clear figures. But in my opinion, further improvement is needed.
1. A clear purpose needs to be added to the abstract.
2. References are needed. Lines 73-75, 79-82.
3. Please provide the longitude and latitude. Line 126
4. The experimental method description of this study is too simplistic, and the measurement method needs to include detailed steps and provide information on the required reagents and instruments
5. The sentences should be put into discussion. Lines 174-175, 184-185.
6. Please check figure. 1 A about the letters of statistical difference.
7. Most of the discussion in this manuscript is focused on repeating the results and does not compare or discuss with other related studies. I suggest adding more comparisons and discussions on relevant research.",Generic,"Background: Seed hypocotyl germination signifies the initiation of the life cycle for plants and represents a critical stage that heavily influences subsequent plant growth and development. While previous studies have established the melatonin (MEL; N-acetyl-5-methoxytrytamine) effect to stimulate seed germination of some plants, its specific role in peony germination and underlying physiological mechanism have yet to be determined. This study aims to evaluate the MEL effect for the hypocotyl germination of peony seeds, further ascertain its physiological regulation factors.

Methods: In this work, seeds of Paeonia ostia 'Fengdan' were soaked into MEL solution at concentrations of 50, 100, 200, and 400 µM for 48 h and then germinated in darkness in incubators. Seeds immersed in distilled water without MEL for the same time were served as the control group.

Results: At concentrations of 100 and 200 µM, MEL treatments improved the rooting rate of peony seeds, while 400 µM inhibited the process. During seed germination, the 100 and 200 µM MEL treatments significantly reduced the starch concentration, and ?-amylase was the primary amylase involved in the action of melatonin. Additionally, compared to the control group, 100 µM MEL treatment significantly increased the GA3 concentration and radicle thickness of seeds, but decreased ABA concentration. The promotion effect of 200 µM MEL pretreatment on GA1 and GA7 was the most pronounced, while GA4 concentration was most significantly impacted by 50 µM and 100 µM MEL.

Conclusion: Correlation analysis established that 100 µM MEL pretreatment most effectively improved the rooting rate characterized by increasing ?-amylase activity to facilitate starch decomposition, boosting GA3 levels, inhibiting ABA production to increase the relative ratio of GA3 to ABA. Moreover, MEL increased radicle thickness of peony seeds correlating with promoting starch decomposition and enhancing the synthesis of GA1 and GA7.

Keywords: Melatonin; Peony; Physiological mechanism; Plant hormones; Seed germination."
Gamma-aminobutyric acid elicits H2O2 signalling and promotes wheat seed germination under combined salt and heat stress,"Abiotic stress is an important phenomenon we should find how some row crops like wheat respond to this adverse environmental condition and how we reduce the adverse impact of it. Your research encompasses valuable insights significant for understanding the impact of Gamma-aminobutyric acid on wheat seed germination under abiotic stress conditions. Nevertheless, it is imperative to address certain technical intricacies in refining your article. I strongly advocate for a comprehensive review of the reviewers' suggestions, coupled with a discerning consideration of each recommendation. In cases where you may disagree with specific suggestions, providing clear and well-justified rationale for your perspective would prove beneficial.",Generic,"Background: In the realm of wheat seed germination, abiotic stresses such as salinity and high temperature have been shown to hinder the process. These stresses can lead to the production of reactive oxygen species, which, within a certain concentration range, may actually facilitate seed germination. ?-aminobutyric acid (GABA), a non-protein amino acid, serves as a crucial signaling molecule in the promotion of seed germination. Nevertheless, the potential of GABA to regulate seed germination under the simultaneous stress of heat and salinity remains unexplored in current literature.

Methods: This study employed observational methods to assess seed germination rate (GR), physiological methods to measure H2O2 content, and the activities of glutamate decarboxylase (GAD), NADPH oxidase (NOX), superoxide dismutase (SOD), and catalase (CAT). The levels of ABA and GABA were quantified using high-performance liquid chromatography technology. Furthermore, quantitative real-time PCR technology was utilized to analyze the expression levels of two genes encoding antioxidant enzymes, MnSOD and CAT.

Results: The findings indicated that combined stress (30 °C + 50 mM NaCl) decreased the GR of wheat seeds to about 21%, while treatment with 2 mM GABA increased the GR to about 48%. However, the stimulatory effect of GABA was mitigated by the presence of ABA, dimethylthiourea, and NOX inhibitor, but was strengthened by H2O2, antioxidant enzyme inhibitor, fluridone, and gibberellin. In comparison to the control group (20 °C + 0 mM NaCl), this combined stress led to elevated levels of ABA, reduced GAD and NOX activity, and a decrease in H2O2 and GABA content. Further investigation revealed that this combined stress significantly suppressed the activity of superoxide dismutase (SOD) and catalase (CAT), as well as downregulated the gene expression levels of MnSOD and CAT. However, the study demonstrates that exogenous GABA effectively reversed the inhibitory effects of combined stress on wheat seed germination. These findings suggest that GABA-induced NOX-mediated H2O2 signalling plays a crucial role in mitigating the adverse impact of combined stress on wheat seed germination. This research holds significant theoretical and practical implications for the regulation of crop seed germination by GABA under conditions of combined stress."
QTL mapping for seed vigor-related traits under artificial aging in common wheat in two introgression line (IL) populations,"In the introduction section, the first two sentences required citation. Firstly, we highly recommend that the authors give some information related to the importance of Wheat and after continuing the gap. You are studying the genetic mapping of the Wheat introgression population so the wheat ploidy, chromosome number, and so on... required. For insurance, for example, ? think that these phrases were not so appreciated. But generally well-structured. Abbreviations are required or you must give the long name of short usage such as QTL, cM, SSR, and so on...

This trial was conducted about 4 years ago. Why did you wait until now, so long time is not acceptable but we can easily see that you are making more to do Quantitative trait loci (QTL) analysis. What is the pattern of trial design? How about the “standard germination method”? Can you please give a citation?
The “Calculation of traits related to wheat seed vigor” this section has required a citation.
Please upload the phenotypic data and genomic data as supplementary table

In line 216, minor-polygene, On what basis are you saying this? Is this an assumption? A high LOD value increases the probability that the QTL sought is the QTL sought. Still, it also strengthens the possibility that QTLs with a high r square value and a high p probability value are the QTLs responsible for the relevant feature. Therefore, it would be beneficial to keep markers with low LOD values.
Please cite the figures and tables included in supplementary tables in the main text.
Line 404-409. In order to effectively......seed vigor during seed aging, this needs citations.
Line 462-464. Needs citation/s
Line 466. For example, not appreciated in here
Line 467-468. Similarly, Shi et al. (2020) (Shi, et al., 2020) duplicated.
Line 472. Same problem. Shi et al (2020) (Shi, et al., 2020) m
(Shi, et al., 2020), the comma is in the wrong place, please correct it.
In the citation, name et al. (year), the et should be written italics in the whole text
Line 482. Li et al. (Li et al., 2014). Please correct it.",Authentic,"Background: Seed vigor recognized as a quantitative trait is of particular importance for agricultural production. However, limited knowledge is available for understanding genetic basis of wheat seed vigor.

Methods: The aim of this study was to identify quantitative trait loci (QTL) responsible for 10 seed vigor-related traits representing multiple aspects of seed-vigor dynamics during artificial aging with 6 different treatment times (0, 24, 36, 48, 60, and 72 h) under controlled conditions (48 °C, 95% humidity, and dark). The mapping populations were two wheat introgression lines (IL-1 and IL-2) derived from recipient parent (Lumai 14) and donor parent (Shaanhan 8675 or Jing 411).

Results: A total of 26 additive QTLs and 72 pairs of epistatic QTLs were detected for wheat seed-vigor traits. Importantly, chromosomes 1B and 7B contained several co-located QTLs, and chromosome 2A had a QTL-rich region near the marker Xwmc667, indicating that these QTLs may affect wheat seed vigor with pleiotropic effects. Furthermore, several possible consistent QTLs (hot-spot regions) were examined by comparison analysis of QTLs detected in this study and reported previously. Finally, a set of candidate genes for wheat seed vigor were predicted to be involved in transcription regulation, carbohydrate and lipid metabolism.

Conclusion: The present findings lay new insights into the mechanism underlying wheat seed vigor, providing valuable information for wheat genetic improvement especially marker-assisted breeding to increase seed vigor and consequently achieve high grain yield despite of further investigation required."
"Integration of full-length Iso-Seq, Illumina RNA-Seq, and flavor testing reveals potential differences in ripened fruits between two Passiflora edulis cultivars","The experimental materials were well designed, and the two varieties with flavour differences were selected for full-length transcriptome analysis to obtain better differential expression results; interestingly, the two varieties were completely different after enrichment annotation of up- and down-regulated genes.
?1?The quality of the icon is not high and the clarity is not good.
?2?The reference to aroma in the background section seems to be incomplete and inconsistent with the title and the flavour indicators in the experimental material; flavour includes traits such as sugar-acid and aroma, and should be revised to flavour.
?3?Passifora edulis and P. edulis. are the Latin name, italicised, and the author is requested to correct it in full.
?4?In Figure 1, ""b Total acid content (TA, g-kg-1);"" appears twice.
?5?The article chose the TPS family for analysis more in consideration of aroma than other flavour-related substances to form related gene families, and the authors are requested to explain.
?6?Figure 9 AtTPS gene should be deleted gene, this figure is made from protein sequences and should be plural.
?7?The genes should be written in italics throughout the text, and the authors are requested to revise the text in its entirety.
?8?Figure 10 b, should be a graph of gene expression trends, not ""expression profiles of 17 TPS genes"".",Authentic,"Background: Passion fruit (Passiflora edulis) is loved for its delicious flavor and nutritious juice. Although studies have delved into the cultivation and enhancement of passion fruit varieties, the underlying factors contributing to the fruit's appealing aroma remain unclear.

Methods: This study analyzed the full-length transcriptomes of two passion fruit cultivars with different flavor profiles: ""Tainong 1"" (TN1), known for its superior fruit flavor, and ""Guihan 1"" (GH1), noted for its strong environmental resilience but lackluster taste. Utilizing PacBio Iso-Seq and Illumina RNA-Seq technologies, we discovered terpene synthase (TPS) genes implicated in fruit ripening that may help explain the flavor disparities.

Results: We generated 15,913 isoforms, with N50 lengths of 1,500 and 1,648 bp, and mean lengths of 1,319 and 1,463 bp for TN1 and GH1, respectively. Transcript and isoform lengths ranged from a maximum of 7,779 bp to a minimum of 200 and 209 bp. We identified 14,822 putative coding DNA sequences (CDSs) averaging 1,063 bp, classified 1,007 transcription factors (TFs) into 84 families. Additionally, differential expression analysis of ripening fruit from both cultivars revealed 314 upregulated and 43 downregulated unigenes in TN1 compared to GH1. The top 10 significantly enriched Gene Ontology (GO) terms for the differentially expressed genes (DEGs) indicated that TN1's upregulated genes were primarily involved in nutrient transport, whereas GH1's up-regulated genes were associated with resistance mechanisms. Meanwhile, 17 PeTPS genes were identified in P. edulis and 13 of them were TPS-b members. A comparative analysis when compared PeTPS with AtTPS highlighted an expansion of the PeTPS-b subfamily in P. edulis, suggesting a role in its fruit flavor profile.

Conclusion: Our findings explain that the formation of fruit flavor is attributed to the upregulation of essential genes in synthetic pathway, in particular the expansion of TPS-b subfamily involved in terpenoid synthesis. This finding will also provide a foundational genetic basis for understanding the nuanced flavor differences in this species."
Taxonomic relevance of leaf surface micromorphology in Korean Clematis L. (Ranunculaceae),"- clear and unambiguous, professional English used throughout
- the manuscript is prepared with using correct, clear and professional English. The authors give a wide background for the results they present with a rich list of cited references
- the structure of the manuscript is correct – it includes all the chapters crucial for this kind of elaboration. - the introduction give the sufficient information about the studied taxa,
- the figures, tables and other graphics are clear, readable and supply the described results properly.
- the raw data are shared.

In my opinion the manuscript presents a valuable an interesting research performed to a high standard of scientific research and fits to aims and scopes of the journal.
The hypothesis and the tools chosen for its verification are presented in the introduction.
The methods are described with sufficient details so the experiments are replicable basing on the manuscript",Generic,"Clematis, one of the largest genera of the family Ranunculaceae, has a wide array of morphological variation and is considered the most difficult group of taxa in terms of infrageneric discrimination. This study assessed the leaf micromorphological features of 19 Clematis taxa (16 species and three varieties) found in Korea. The leaf surface features were studied under scanning electron microscopy, and the stomatal counting and measurement were carried out under light microscopy. Clematis are hypostomatic, meaning the stomata are only found on the abaxial surface of the leaf. Observed taxa showed near uniformity in the epidermal cell type, structure, and morphology on both surfaces of the leaf. Differences were observed in the presence and absence and/or abundance of trichomes on both the adaxial and abaxial surfaces, the epidermal cell boundary, and the periclinal and anticlinal wall of the cells. Differences were also observed in the number of the epidermal cells connected with the stomata on the abaxial surface, with small differences noted in epidermal cell shapes. The ANOVA showed a significant variation in the stomata density in the studied taxa (P < 0.0001). The cluster analysis based on 13 leaf micromorphological features generated four major clusters. These results indicated similarities in certain key leaf micromorphological features among taxa from the Tubulosae, Clematis, and Virona sections. In the genus Clematis, as with other morphological characteristics, using leaf micromorphological characters alone, which possess limited taxonomic value, proves inadequate for resolving infrageneric relationships. However, incorporating certain features with other morphological characteristics offers a possible alternative means of determining the infrageneric relationships within the genus."
Enhancing genetic diversity in Pelargonium: insights from crossbreeding in the gene pool,"Your study contains important information regarding the breeding of an important globally widespread ornamental plant, and your presentation is clear and concise. However, it needs to be improved. Complete minor revisions by addressing the following deficiencies in addition to the reviewers' comments. Even though one of the reviewers marked it as a major revision, when I read the comments I saw minor revisions. Since I believe that you will complete the changes I want in a short time, my decision is for minor revisions.

-In the Materials and Method section, you wrote that you chose 16 female and 20 male parents, but there are 20 hybrids made between them. When I read this I expect a 16 x 20 crossover. But you did not crossbreed them all with each other. How did you decide to make these crossbreeds? You must explain.
-It will be more understandable if you show male and female parents in separate columns and hybrids in a separate column in Table 1. It's hard to keep track like that.
-Table 3 is quite long and does not look good in shape. Supp it. I recommend you present it as a file. Or you can prepare separate tables for male parent, female parent and hybrids.
-I recommend that you add a table showing the genetic distance values ??of the genotypes or writing down the members of each group. It is difficult to follow from the dendrogram. You should also improve its appearance. It would be better if you create a circular dendogram.",Authentic,"This study aimed to enrich the Pelargonium gene pool through crosses and assess genetic variation among 56 genotypes from five Pelargonium species. Seventeen morphological descriptors were used, and NTSYS-pc software was employed to define genetic relationships, and a UPGMA-generated dendrogram reflected these relationships. Moreover, principal component analysis (PCA) was performed to determine which parameter was more effective in explaining variation. Results showed wide variation in genetic similarity rates, with the most similar genotypes being P. zonale ‘c1’ and a hybrid of P. zonale ‘c1’ x P. zonale ‘c2’ (90% similarity). According to the dendrogram results, it was observed that the genotypes were distributed in six clusters. In contrast, the most distant genotypes were P. zonale ‘c11’ and a hybrid of P. zonale ‘c10’ x P. zonale ‘c11’ (0.04% similarity). Hybrids from the female parent P. x hortorum ‘c1’ exhibited unique placement in the dendrogram. In the crossing combinations with this genotype, the individuals obtained in terms of flower type, flower color, flower size, bud size, early flowering, and leaf size characters showed different characteristics from the parents. Surprising outcomes in flower types, colors, and shapes contributed to gene pool enrichment, promising increased breeding variation success. The study holds practical implications for commercial breeding and serves as a valuable guide for future research endeavors."
Seasonal dynamics of seed dormancy and germination in the weed Diplachne fusca,"The authors have asked good questions, and the research was carefully done. The manuscript is easy to follow, and the data support the conclusions. However, I do suggest some minor changes in the wording in various places in the manuscript.
Line 25, 30. Change “two” to “2” If a number less than 10 is followed by a unit of measure – do not spell the number.
Line 53. Change to “and are prolific producers of seeds”
Line 75. Change “tempt” to “stimulate”
Line 76. Change to “ likely to die after the below-freezing temperatures of winter begin (Finch-Savage….”
Line 80 Insert “(Poaceae)” after “fusca”
Line 81. Delete “within the Poaceae family”
Line 85. Change to “Hebei provinces of China, becoming…”
Line 86. Change to “both crop quality and yield (Yuan…”
Line 94. Insert “that do not rely solely on herbicides” at the end of the sentence
Line 96. Change “Those” to “These”
Line 97. Change “the germination” to “seed germination”
Line 98. Delete “they discovered that”
Line 102. Change “type” of “kind” In the dormancy classification system the word “type” refers to subcategories of nondeep physiological dormancy. Here, you are talking about kinds of classes of seed dormancy.
Line 113. Change “a week” to “1 week”
Line 114. Change to “2 weeks”
Line 124. Change to “filter paper moistened with 1 mL distilled water.”
Line 127. Change to “Germination was tested at 4ºC…”
Line 133. Change to “respectively, each day.”
Lines 137-138. Change to “incubated in both light and dark at 5/15, ………… for 14 days.”
Line 151. Change to “in both light and dark at 5/15…. For 14 days.”
Line 156. Change “types” to either :”kinds” or “classes”
Line 1589. Change to “incubated in light at 20/30ºC for 14 days.”
Line 160. Change to “daily, and germinated seeds were discarded.”
Line 165. Change to “incubated in light at 20/30ºC for 14 days.”
Line 167. Change to “daily, and germinated seeds were discarded.”
Line 171. Move “soil” to before “depth”
Line 177. Change to “incubated in both light and dark at 4, ….”
Line 183. Delete comma after GA3
Line 192. Change to 2 weeks” and delete “percentage” “Percentage” and “%” mean the same -saying the same thing two times
Line 199. Insert “was” before “extended”
Line 200. Change to “improvement in seed germination percentages (Fig. 1).”
Line 201. Change to “ reached 88 + 4.3% germination when incubated in light at 20/30ºC.” Need to have only one number after the decimal, i.e. round-off
Line 202. Change to “Conversely, germination in dark was only 23 + 4.1% (Fig. 1d)…”
Line 210. Change “rose” to “increased”
Line 211. Change to “ripening, germination reached 80+1.6%.”
Line 212. Change “though” to “although”
Lines 212. Change to “mmol/L, seeds germinated to 58+6.4 and 61+8.4%% ....”
Line 221-222. Change to “20/30ºC. Germination percentages at these temperatures remained high until June…”
Line 222. Insert “int eh field” before “reached”
Line 223. Insert “respectively” before “and”
Line 225. Change “once more” to “again”
Line 228. Change to “2 years”
Line 232. Insert “those from” after “with”
Line 235. Change to “percentage in light than in darkness.”
Line 247. Change to “PD is divided into three levels: non-deep, intermediate and deep (Baskin and Baskin 2014). Non-deep….”
Line 250. Delete “only” that appears before “works”
Line 253. Delete “hence”
Line 261. Insert “ for seedlings to grow enough to reach the soil surface” after “buried”
Line 261. Change “Following” to “During”
Line 257. Change “though” to “although”
Line 273. Change to “allows seeds to germinate early in…”
Line 274. Change “their” to “plant”
Line 274-75. Change to “production, and this germination strategy also reduces…”
Line 276. Delete “the” before “germination” and before “seeds”
Line 277. Delete “the” before “mean” and insert “field” before “temperatures”
Line 290. Change to “2 years”
Line 291. Delete “which may be attributed to their hard seed coat (McIntyre et al., 1989).” Seeds of grasses do not have a hard seed coat, i.e. a water-impermeable seed coat. Seeds of grasses have a ‘physiological problem’ in the embryo. You have already said that seeds have PD, thus talking about ‘hard seed coats’ is very confusing (and incorrect) for the reader.
Line 291. Change “This indicated that “ to “Thus,”
Line 294. Seeds of Astragalus have physical dormancy, i.e. hard seeds or a water-impermeable seed coat. I suggest you delete Astragalus and use Ambrosia artemisiifolia instead. Seeds of A.t. germinated in the 40th year of the Beal buried seed experiment (see Amer. J. Bot. 64(9): 1174-1176. 1977 for details about this).
line 317. Insert “seeds” after “weed”
Line 335. Change to “2 years”
Line 337. Change to “fall germination, which prevents seedlings from begin killed by freezing temperatures…”",Authentic,"Background: Understanding the reproductive biology of weeds is crucial for managing them effectively. Diplachne fusca (Poaceae) is a widely distributed weed species that poses significant challenges to agricultural productivity. Nevertheless, it remains unclear how the soil seed bank of D. fusca responds to environmental shifts, and whether a dormancy cycle is present in this species.

Methods: We investigated how seed dormancy in D. fusca is broken and how it responds to natural environmental changes. The impact of incubation temperature, light exposure, cold stratification at 4 °C, and gibberellic acid (GA3) on seed germination/dormancy-break was investigated, along with assessing seasonal changes in germinability through monthly excavation and laboratory incubation of buried seeds over 2 years.

Results: Results indicated that newly ripened seeds of D. fusca were dormant, with germination facilitated by GA3, cold stratification, and after-ripening at ambient room conditions. Exposure to darkness inhibited germination. Seasonal patterns of germination were observed, with peak germination occurring in cooler months and a marked decline during the hot summer months. After 2 years of being buried, approximately 40% of the seeds remained viable.

Conclusion: In summary, seeds of D. fusca exhibit non-deep physiological dormancy and maintain a persistent soil seed bank. Seeds buried in the soil undergo a yearly dormancy/non-dormancy cycle. This dormancy cycle prevents seed germination and seedling emergence in autumn, which boosts the survival of seedlings in less favorable seasons, yet it also makes it more challenging to eradicate this weed."
Phenotyping 172 strawberry genotypes for water soaking reveals a close relationship with skin water permeance,"The article phenotyped 172 strawberry genotypes for water soaking and found a correlation of WS with other traits. Also, the authors have introduced WS assay. The article has some interesting findings. Here are some improvement suggestions for authors.

The title is vague. I recommend rethinking.

The abstract is vague. It fails to formulate a question and provide answers in the form of results. A good abstract first introduces a problem, then provides ideas that could solve it, then material and methods, then results, and then a conclusion. This abstract is all over the place and does not provide clarity or context. Highly recommend rephrasing.

Is there a relationship between water soaking and diseases? If yes, it will be crucial to mention and discuss briefly in the introduction. All the different ways WS can cause ill effects on strawberries will be important.",Generic,"Water soaking is a commercially important disorder of field-grown strawberries that is exacerbated by surface wetness and high humidity. The objective was to establish the effect of genotype on susceptibility to water soaking. Three greenhouse-grown model 'collections' were used comprising a total of 172 different genotypes: (1) a segregating F2 population, (2) a collection of strawberry cultivars and breeding clones, and (3) a collection of wild Fragaria species. A standardized immersion assay was used to induce water soaking. Potential relationships between water soaking and water uptake characteristics, depth of the achene depressions, fruit firmness, cuticle mass and strain relaxation and microcracking were investigated. Further, the effect of downregulating the polygalacturonase genes (FaPG1 and FaPG2) on the susceptibility to water soaking was investigated. The collection of wild species was most susceptible to water soaking. This was followed by the collection of cultivars and breeding clones, and by the F2 population. Susceptibility to water soaking was strongly correlated with water uptake rate (mass of water, per fruit, per time). For the pooled dataset of 172 genotypes, 46% of the variability in water soaking was accounted for by the permeance of the skin to osmotic water uptake. Susceptibility to water soaking was not, or was only poorly correlated with measurements of fruit surface area or of the osmotic potential of the expressed fruit juice. The only exceptions were the wild Fragaria species which were highly variable in fruit size and also in fruit osmotic potential. For genotypes from the F2 and the wild species collections, firmer fruit were less susceptible to water soaking than softer fruit. There were no relationships between fruit firmness and susceptibility to water soaking in transgenic plants in which FaPG1 and FaPG2 were down-regulated. Susceptibility to water soaking was not related to cuticle mass per unit fruit surface area, nor to strain relaxation of the cuticle upon isolation, nor to achene position. In summary, strawberry's susceptibility to water soaking has a significant genetic component and is closely and consistently related to the skin's permeance to osmotic water uptake."
A short stature allele enhances tolerance to zinc deficiency and translocation of zinc in barley,"In addition to reviewers' comments, complete major revisions by correcting the following deficiencies.

-Line 24: should be expressed as ""stage of heading and physiological maturity"".
-Line 34-35. What importance might the barley plant have for biofortification? It is not consumed much as human food. Does this indirect route provide sufficient biofortification when used as animal feed? I recommend thinking about this throughout the manuscript and organizing your text accordingly.
-Move the literature you wrote in parentheses to the end of the sentence without interrupting it.
-Line 117: until heading stage
-I do not think it is right to perform statistical analyzes separately for Zn- and Zn+. In Figure 3, you put your genotypes side by side for Zn+ and Zn- and compared them. Therefore, you should re-analyze the statistics so that we can see the interactions and redraw the graphs you show in Figures 1,2 and 4,5. I did not read that part because it would completely change the narrative of your findings. However, I believe that the statistical analysis you made in this way is not correct.",Authentic,"Background: Zinc (Zn) content is of great importance in healthy human diet, crop productivity and stress tolerance in soils with zinc deficiency. The genes used to increase yield per unit area such as semi-dwarf 1 (sdw1) is commonly considered to reduce mineral content of grain.

Methods: In the present study, influence of sdw1.d, a widely used allele for short plant height in barley breeding, on zinc accumulation and tolerance to zinc deficiency were investigated. A near isogenic line of sdw1.d allele, its recurrent parent Tokak 157/37 and donor parent Triumph were grown in zinc-deficient and-sufficient hydroponic cultures. Two experiments were conducted until heading stage and physiological maturity.

Results: In zinc-deficient conditions, sdw1.d allele increased shoot dry weight by 112.4 mg plant-1, shoot Zn concentration by 0.9 ppm, but decreased root Zn concentration by 6.6 ppm. It did not affect grain characteristics, but increased grain Zn content. In zinc-sufficient conditions, sdw1.d allele increased shoot Zn content, and decreased root Zn content. sdw1.d did not affect grain weight but increased grain Zn concentration by about 30% under zinc-sufficient conditions. The results showed that sdw1.d allele has no negative effect on tolerance to zinc deficiency, and even promotes tolerance to zinc deficiency by more Zn translocation. It was revealed that sdw1.d allele improves Zn accumulation under both zinc-deficient and zinc-sufficient condition. The sdw1.d allele could contribute to solving the problems in plant growth and development caused by zinc-deficiency via improving tolerance to zinc-deficiency. It could also provide a better Zn biofortification."
Investigating the influence of eco-friendly approaches on saline soil traits and growth of common bean plants (Phaseolus vulgaris L.),"Overall the English and grammar are clear but some sentences need more detail to be complete. For example lines 90-97 the sentence is unclear and a run on sentence without completing the context for clarity. Line 64-68 a different tense of verbs are used. Consider revising the sentences and going through introduction for same tense of sentences and not switching from past tense to present tense helps the reader. The content is organized and literature review relevant some minor English revisions suggested.

The purpose was written well to define the need for best management strategies that incorporate eco-friendly methods to address saline soils and agriculture production of beans.

Thank you for the relevant tables and figures that explain the data but adding more details to explain the figures relevance would help. Please see Zhao et al. 2016 with the sunflowers and plastic mulching as an example. For example figure 5 defines the abbreviations for each mulch type strengthening the figure legend to understand without having to search the manuscript for what Ch 0-3 means and the UNM, WPM, RSM, SDM means. This example also explains the value or standard error.",Generic,"Soil salinization significantly impacts agricultural lands and crop productivity in the study area. Moreover, freshwater scarcity poses a significant obstacle to soil reclamation and agricultural production. Therefore, eco-friendly strategies must be adopted for agro-ecosystem sustainability under these conditions. A study conducted in 2022 and 2023 examined the interaction effects of various soil mulching materials (unmulched, white plastic, rice straw, and sawdust) and chitosan foliar spray application (control, 250 mg L?1 of normal chitosan, 125 mg L?1 of nano chitosan, and 62.5 mg L?1 of nano chitosan) on the biochemical soil characteristics and productivity of common beans in clay-saline soil. Higher organic matter, available nutrient content, and total bacteria count in soils were found under organic mulching treatments (rice straw and sawdust). In contrast, the white plastic mulching treatment resulted in the lowest values of soil electrical conductivity (EC) and the highest soil water content. Conversely, chitosan foliar spray treatments had the least impact on the chemical properties of the soil. Plants sprayed with 62.5 mg L?1 of nano chitosan exhibited higher chlorophyll content, plant height, fresh weight of shoots and roots, seed yield, and nutrient content compared to other chitosan foliar spray applications. All treatments studied led to a significant reduction in fungal communities and Na% in plants. The combined effect of organic mulch materials and foliar spray application of 62.5 mg L?1 nano chitosan appeared to enhance biochemical saline soil properties and common bean productivity."
The role of DNA topoisomerase 1? (AtTOP1?) in regulating arabidopsis meiotic recombination and chromosome segregation,"The study aims to understand the function of AtTOP1? in Arabidopsis meiosis, a crucial process for sexual reproduction. Also, the authors employed various cytological and molecular techniques like DAPI staining, FISH, and PCR to analyze chromosome behaviour, gene expression, and pollen viability. The generation and analysis of the atm×top1?1 double mutant strengthens the role of AtTOP1? in meiotic progression. Overall, this research provides valuable insights into the role of AtTOP1? in Arabidopsis meiosis.
Please find the comments in the attached annotated PDF file
Experimental design is relatively well illustrated
Key Findings:
• AtTOP1? deficiency leads to reduced fertility and abnormal pollen development.
• Meiotic chromosomes display altered heterochromatin distribution and clustered centromere signals in the top1?1 mutant.
• Disruption of AtTOP1? function affects 45s rDNA localization during meiosis.
• AtTOP1? may promote crossover formation during meiosis.",Authentic,"Meiosis is a critical process in sexual reproduction, and errors during this cell division can significantly impact fertility. Successful meiosis relies on the coordinated action of numerous genes involved in DNA replication, strand breaks, and subsequent rejoining. DNA topoisomerase enzymes play a vital role by regulating DNA topology, alleviating tension during replication and transcription. To elucidate the specific function of DNA topoisomerase 1? ($AtTOP1 \alpha$) in male reproductive development of Arabidopsis thaliana , we investigated meiotic cell division in Arabidopsis flower buds. Combining cytological and biochemical techniques, we aimed to reveal the novel contribution of $AtTOP1 \alpha$ to meiosis. Our results demonstrate that the absence of $AtTOP1 \alpha$ leads to aberrant chromatin behavior during meiotic division. Specifically, the top1?1 mutant displayed altered heterochromatin distribution and clustered centromere signals at early meiotic stages. Additionally, this mutant exhibited disruptions in the distribution of 45s rDNA signals and a reduced frequency of chiasma formation during metaphase I, a crucial stage for genetic exchange. Furthermore, the atm-2×top1?1 double mutant displayed even more severe meiotic defects, including incomplete synapsis, DNA fragmentation, and the presence of polyads. These observations collectively suggest that $AtTOP1 \alpha$ plays a critical role in ensuring accurate meiotic progression, promoting homologous chromosome crossover formation, and potentially functioning in a shared DNA repair pathway with ATAXIA TELANGIECTASIA MUTATED (ATM) in Arabidopsis microspore mother cells."
Assessing the opportunity for selection to impact morphological traits in crosses between two Solanum species,"The paper is clearly written, with sufficient description of background, methods, data collection, and statistical analisys.

The manuscript provides a comprehensive background on the importance of understanding genetic architectures in plant breeding and evolutionary biology.

The figures and tables are clear and referenced in the text. Minor point (opinion): Only figure 5 and 6 (with the core results) are truly needed. The paper is short and doesn't call for 6 figures.

I did not have access to the raw data.

The Methods section is detailed, the justification for the study is clear. The statistical analysis, an information-theoretic approach to account for model selection uncertainty, is a strength of the study.

Potential limitations, like the lack of F1 phenotype data and the relatively small sample sizes for some traits, could be more explicitly discussed.",Generic,"Within biology, there have been long-standing goals to understand how traits impact fitness, determine the degree of adaptation, and predict responses to selection. One key step in answering these questions is to study the mode of gene action or genetic architecture of traits. The genetic architecture underlying a trait will ultimately determine whether selection can lead to a change in the phenotype. Theoretical and empirical research have shown that additive architectures are most responsive to selection. The genus Solanum offers a unique system to quantify the genetic architecture of traits. Crosses between Solanum pennellii and S. lycopersicum, which have evolved unique adaptive traits for very different environments, offer an opportunity to investigate the genetic architecture of a variety of morphological traits that often are not variable within species. We generated cohorts between strains of these two Solanum species and collected phenotypic data for eight morphological traits. The genetic architectures underlying these traits were estimated using an information-theoretic approach to line cross analysis. By estimating the genetic architectures of these traits, we were able to show a key role for maternal and epistatic effects and infer the accessibility of these traits to selection."
"Variation in floral morphology, histochemistry, and floral visitors of three sympatric morning glory species","Ms presents a study describing the relationship between the morphology of flowers, some of their histochemical characteristics, and flower visitors in three sympatric species of the genus Argyreira. The plants studied are endemic to Thailand and their populations are not abundant. The research includes two species (A. mekongensis and A. versicolor) that were previously studied in terms of flower size, phenology, and floral visitors (Jirabanjongjit et al 2024), and one more, A. lycioides, which has not been previously studied.

The title of the work suggests a study with a methodological design and results that include for each of the species the recording and observation of different parameters of the flowers, which allow comparing their morphology, such as the size of petals, stamens, and anthesis and phenology. ; as well as very specific observations of floral visitors, their frequency and visiting behaviors. However, many floral characteristics, phenology, and floral visitors were previously published for A. mekongensis and A. versicolor, two of the species indicated in the study (Jirabanjongjit et al 2024). This generates a lot of confusion since these two species are presented in the method as if they were going to be studied in the present work, and then it is mentioned that there is already data on them. The writing becomes even more confusing since the data on phenology and floral visitors of these two species in the results section are presented as if they had been obtained in the present study.

I consider that the authors should make it clear in the introduction that some aspects of the reproductive biology of A. mekongensis and A. versicolor are already known and that the present study aims to know some aspects of the reproductive biology of A. lycioides, as well as know some aspects of the histochemistry of the three species, to finally compare the floral morphology, phenology and histochemical characteristics between the three species, and relate these aspects to the floral visitors.",Authentic,"Three morning glory species in the genus Argyreia Lour., A. lycioides (Choisy) Traiperm & Rattanakrajang, A. mekongensis Gagnep & Courchet, and A. versicolor (Kerr) Staples & Traiperm, were found co-occurring and co-flowering. Argyreia mekongensis and A. versicolor are rare, while A. lycioides is near threatened and distributed throughout Myanmar and Thailand. We investigated key floral characters (floral morphology and phenology, as well as the micromorphology of the floral nectary disc and staminal trichomes) and screened for important chemical compounds hypothesized to contribute to pollinator attraction. Our findings demonstrate that some aspects of floral morphology (e.g., corolla size, limb presence, and floral color) of the three studied congeners exhibit significant differences. Moreover, pollinator composition appears to be influenced by floral shape and size; morning glory species with wider corolla tubes were pollinated by larger bees. The morphology of the floral nectary disc was similar in all species, while variation in staminal trichomes was observed across species. Glandular trichomes were found in all three species, while non-glandular trichomes were found only in A. versicolor. Histochemical results revealed different compounds in the floral nectary and staminal trichomes of each species, which may contribute to both floral attraction and defense. These findings demonstrate some segregation of floral visitors among sympatric co-flowering morning glory species, which appears to be influenced by the macro- and micromorphology of flowers and their chemical compounds. Moreover, understanding the floral morphology and chemical attractants of these sympatric co-flowering Argyreia species may help to maintain their common pollinators in order to conserve these rare and endangered species, especially A. versicolor."
Effect of exogenous melatonin on growth and antioxidant system of pumpkin seedlings under waterlogging stress,"Please rewrite the abstract with adding results details and add sentence to clear the importance of the findings
- Write the physiological indicators methods of determination with details
In results
- I suggest to add figure of correlations To clarify the extent of connection and relationships between treatments and results
- I suggest to add figure To clarify and conclude the experimental treatments with melatonin and parameters on pumpkin and the mechanism of action of melatonin and how it improved plant health under waterlogging
Conclusion
- I suggest adding a sentence to clarify the economic benefit from the experience and a future outlook for this work",Generic,"Melatonin regulates defense responses in plants under environmental stress. This study aimed to explore the impact of exogenous melatonin on the phenotype and physiology of 'BM1' pumpkin seedlings subjected to waterlogging stress. Waterlogging stress was induced following foliar spraying of melatonin at various concentrations (CK, 0, 10, 100, 200, and 300 ?mol·L-1). The growth parameters, malondialdehyde (MDA) content, antioxidant enzyme activity, osmoregulatory substance levels, and other physiological indicators were assessed to elucidate the physiological mechanisms underlying the role of exogenous melatonin in mitigating waterlogging stress in pumpkin seedlings. The results indicate that pumpkin seedlings exhibit waterlogging symptoms, such as leaf wilting, water loss, edge chlorosis, and fading, under waterlogging stress conditions. Various growth indicators of the seedlings, including plant height, stem diameter, root length, fresh and dry weight, and leaf chlorophyll content, were significantly reduced. Moreover, the MDA content in leaves and roots increased significantly, along with elevated activities of superoxide dismutase, catalase, peroxidase, and soluble protein contents. When different concentrations of melatonin were sprayed on the leaves post waterlogging stress treatment, pumpkin seedlings showed varying degrees of recovery, with the 100 ?mol·L-1 treatment displaying the best growth status and plant morphological phenotypes. There were no significant differences compared to the control group. Seedling growth indicators, chlorophyll content, root activity, antioxidant enzyme activities, soluble protein content, and osmotic adjustment substance content all increased to varying degrees with increasing melatonin concentration, peaking at 100 ?mol·L-1. Melatonin also reduced membrane damage caused by oxidative stress and alleviated osmotic imbalance. Exogenous melatonin enhanced the activities of antioxidant enzymes and systems involved in scavenging reactive oxygen species, with 100 ?mol·L-1 as the optimal concentration. These findings underscore the crucial role of exogenous melatonin in alleviating waterlogging stress in pumpkins. The findings of this study offer a theoretical framework and technical assistance for cultivating waterlogging-resistant pumpkins in practical settings. Additionally, it establishes a theoretical groundwork for the molecular breeding of pumpkins with increased tolerance to waterlogging."
Forage yields and nutritive values of oat and triticale pastures for grazing sheep in early spring,"The aim of this research is well-thought-out and points out a common problem for the rangelands in Turkey. However, I did not understand the suggestions related to the research findings. The author should implicate why and how grain pastures are grazed.
The English of the text is quite poor so I suggest the author might use the help of a fluent speaker. Additionally, the author/s should be aware of wordiness.
Generally, proper references were used but some of them are too old and should be changed with the recent studies.
Raw data is clear and the article structure is proper. However, I suggest mineral contents should be excluded from this article. It makes the paper very complicated.
Tables should be edited, especially years should be given above the periods and authors should see the variation among the years, periods, and between the pasture types in terms of forage quality characteristics.
The study is well-organized but the hypothesis should be explained clearly and the results/discussion should be given relevant to the hypothesis.
The title is very important for the readers and the title of this study should be revised. For example ""Possibility of Reducing Grazing Pressure in Early Spring via Artificially Established Spring Grain (Oat and Triticale) Pasture""",Generic,"Background Small-grain winter cereals can be utilized as early spring pastures in temperate climates to relieve grazing pressure and potentially mitigate feed shortages. This study was conducted to determine the effects of triticale and oat cereal pastures grazed by sheep during early spring on forage yields, nutritive values, and nutritional requirements of sheep. Methods The research was carried out over three consecutive years, from 2015 to 2017, at the Sheep Research Institute in Band?rma-Bal?kesir, located in the Marmara region of Türkiye. The treatments were arranged in a completely randomized block design, with the two forage species, triticale and oat, randomized within each of three blocks. The animal material for the study consisted of 24 Karacabey Merino sheep, each 2 years old, with an average live weight of 57.6 ± 0.5 kg, all in the late lactation stage. In each replication, four sheep were included, resulting in a total of 12 sheep grazing in each of the triticale and oat pastures. The sheep grazed exclusively on the cereal pastures without any additional feed, and had unrestricted access to water throughout the entire period of the experiment. The dry matter yields (DMY), dry matter intakes (DMI), nutritive values, and mineral contents of the cereal species were determined. Results The DMY showed significant differences over the years ( P < 0.05). No differences in DMY were observed between pastures, with oats yielding 11.99 t ha ?¹ and triticale yielding 11.08 t ha ?¹ . During the grazing period, the change in DMY was significant in all years ( P < 0.05). The average DMI of the sheep was 2,003.5 g d ?¹ for triticale and 2,156.6 g d ?¹ for oat, respectively, and DMI exhibited no significant differences across pastures. Although there was no difference in DMI between 2015 and 2016, the lowest consumption occurred in 2017 ( P < 0.05). Additionally, while DMI showed different trends each year based on the periods, it generally decreased by the end of the grazing period. While both pastures provided similar nutritive values, significant differences were observed in the crude protein (CP), acid detergent fiber (ADF), in vitro true DM digestibility (DDM), and metabolisable energy (ME) values across the years. Over the years, as the grazing period progressed, CP levels decreased while neutral detergent fiber (NDF), ADF, and acid detergent lignin (ADL) increased, resulting in reduced DDM and ME values. The phosphorus (P) content in triticale was higher than in oats, but there were no differences in the content of other minerals between them. Between the years, significant differences were observed in the levels of phosphorus (P) and iron (Fe), while changes in other elements were insignificant. The variation in mineral content during the grazing process differed over the three years. Study results indicated that the nutritional values of triticale and oat pastures are similar, and both can effectively be used to provide sufficient feed to meet the early spring forage requirements for sheep."
Plant growth promoting bacteria (PGPB)-induced plant adaptations to stresses: an updated review,"The review “Plant growth promoting bacteria and PGPB-Induced plant adaptations to stresses: an updated review” addresses a topic of great relevance and interest and focuses on the results reported in the literature in recent years.
The manuscript reads smoothly, however it has some problems.
Some paragraphs go beyond the topic covered and need to be completely rewritten. In paragraph 4.1, the production of siderophores for example, lines 375-379 should be deleted. Or the paragraph on the production of proteases: it does not deal at all with the function that these enzymes can have in association with the plant, but deals with topics that are completely off topic. Even paragraph 4.2.3 on the function of amylases does not touch on the function that they can perform in favor of the plant.",Generic,"Plants and bacteria are co-evolving and interact with one another in a continuous process. This interaction enables the plant to assimilate the nutrients and acquire protection with the help of beneficial bacteria known as plant growth-promoting bacteria (PGPB). These beneficial bacteria naturally produce bioactive compounds that can assist plants' stress tolerance. Moreover, they employ various direct and indirect processes to induce plant growth and protect plants against pathogens. The direct mechanisms involve phytohormone production, phosphate solubilization, zinc solubilization, potassium solubilization, ammonia production, and nitrogen fixation while, the production of siderophores, lytic enzymes, hydrogen cyanide, and antibiotics are included under indirect mechanisms. This property can be exploited to prepare bioformulants for biofertilizers, biopesticides, and biofungicides, which are convenient alternatives for chemical-based products to achieve sustainable agricultural practices. However, the application and importance of PGPB in sustainable agriculture are still debatable despite its immense diversity and plant growth-supporting activities. Moreover, the performance of PGPB varies greatly and is dictated by the environmental factors affecting plant growth and development. This review emphasizes the role of PGPB in plant growth-promoting activities (stress tolerance, production of bioactive compounds and phytohormones) and summarises new formulations and opportunities."
"A systematic review on the morphology structure, propagation characteristics, resistance physiology and exploitation and utilization of Nitraria tangutorum Bobrov.","Clarity and Structure: Assess whether the paper has a clear structure with a logical flow of information. Ensure that the introduction provides adequate context for Nitraria tangutorum, including its significance in desert ecosystems.
Language and Grammar: Check for grammatical errors and awkward phrasing. Suggest improvements where necessary for clarity and readability.
Use of Figures and Tables: Review the quality and relevance of figures and tables. Are they properly labeled and do they complement the text? Recommend any adjustments for improved clarity, such as adding legends or adjusting scales.
Literature Review and References: Examine the thoroughness of the literature review. Does the paper provide a comprehensive background on Nitraria tangutorum, including its ecological role, distribution, and any previous research? Verify the accuracy of references and suggest additional relevant studies if needed.

Research Question and Hypothesis: Determine if the study's research question is clearly defined and if the hypothesis is explicitly stated. Assess whether the hypothesis is testable and aligns with the overall research goal.
Methodology and Data Collection: Review the study's methodology for robustness and reproducibility. Does the paper provide sufficient detail for other researchers to replicate the study? Comment on the sampling methods, experimental design, and data collection techniques. If the study involves fieldwork, assess whether the locations and conditions are well-documented.
Statistical Analysis: Evaluate the statistical methods used. Are they appropriate for the type of data collected? Recommend additional analyses if the current ones seem insufficient or inappropriate. Ensure that the paper reports significant results correctly and uses appropriate statistical tests.",Generic,"Nitraria tangutorum Bobrov., belonging to the family Nitrariaceae, is a drought-tolerant and salt-loving plant and has drawn attention for its good economic and ecological value. As one of the main group species and dominant species in China's desert and semi-desert regions, N. tangutorum possesses superior tolerance to drought, high temperature, cold, barren, high salinity and alkalinity and wind and sand. Its root system is well developed, with many branches and a strong germination capacity. Once buried in sandy soil, N. tangutorum can quickly produce a large number of adventitious roots, forming new plants and continuously expanding the shrubs, forming fixed and semi-fixed shrub sand dunes. Sand dune shrubs can trap and fix a large amounts of quicksand, prevent desert expansion and erosion, and play an important role in maintaining regional ecosystem balance and improving ecological environmental quality. In addition, the phytochemical screening studies report that N. tangutorum contains an abundance of various compounds including flavonoids, alkaloids, phenolic acids and polysaccharides. These compounds confer a range of beneficial bioactivities such as antioxidant, anti-inflammatory, anti-tumor, anti-fatigue, liver protection, neuroprotection, cardiovascular protection, lowering blood lipid, regulating blood sugar level and immunoregulation. The fruits of N. tangutorum also contain vitamin C, amino acids, minerals and microelements. It has been traditionally used as a nutritional food source and in folk medicine to treat diseases of the spleen and stomach, abnormal menstruation, indigestion, and hyperlipidemia. N. tangutorum, as a wild plant with medicinal and edible homology, possesses remarkable economic and medicinal values. This detailed, comprehensive review gathers and presents all the information related to the morphological structure, propagation characteristics, resistance physiology and exploitation and utilization of N. tangutorum, providing a theoretical basis for the researchers to conduct future in-depth research on N. tangutorum."
Enhancing rice yield prediction: a deep fusion model integrating ResNet50-LSTM with multi source data,"The study presents a good integration of ResNet50 and LSTM for rice yield prediction, which is commendable. The potential impact of this research on practical applications in agriculture is significant. The paper makes a valuable contribution to the field of agricultural predictive modeling. Addressing the aforementioned comments will enhance the clarity, reproducibility, and impact of the work. However, there are some suggestions:
1. Write your objectives, contributions in proper way in introduction
2. You have not highlighted the problems in existing studies in introduction part.
3. Word Novel is not suitable
4. Abstract is too long, concise to upto 250 words and shift rest of the work to introduction
5. Approach is Cost effective in terms of what ?
6. Revise your results section such that divide it into different subsections as per your experiments to enhance the clarity of the section.
7. Methodology section is not providing an insightful elaboration of the proposed framework.
8. The manuscript maintains a good standard of English but could benefit from simplification of complex sentences for enhanced readability.
9. Minor grammatical errors and awkward phrasing need correction, e.g., ""affected production"" should be ""affected the production.""
10. Ensure consistency in terminology throughout the paper, such as consistently referring to ""ResNet50"" as ""residual neural network (ResNet50)"" or ""ResNet50.""
11. The methodology section requires additional clarification, especially in the data preprocessing steps (lines 28-32) to allow for replication.
12. Provide a rationale for the specific configurations and architectures of the LSTM models, and consider comparing with alternative configurations to strengthen the study.
13. Discuss the criteria used for selecting feature combinations and their impact on model performance in more detail.",Authentic,"Rice production is pivotal for ensuring global food security. In Pakistan, rice is not only the dominant Kharif crop but also a significant export commodity that significantly impacts the state's economy. However, Pakistan faces challenges such as abrupt climate change and the COVID-19 pandemic, which affect rice production and underscore the need for predictive models for informed decisions aimed at improving productivity and ultimately the state's economy. This article presents an innovative deep learning-based hybrid predictive model, ResNet50-LSTM, designed to forecast rice yields in the Gujranwala district, Pakistan, utilizing multi-modal data. The model incorporates MODIS satellite imagery capturing EVI, LAI, and FPAR indices along with meteorological and soil data. Google Earth Engine is used for the collection and preprocessing of satellite imagery, where the preprocessing steps involve data filtering, applying region geometry, interpolation, and aggregation. These preprocessing steps were applied manually on meteorological and soil data. Following feature extraction from the imagery data using ResNet50, the three LSTM model configurations are presented with distinct layer architectures. The findings of this study exhibit that the model configuration featuring two LSTM layers with interconnected cells outperforms other proposed configurations in terms of prediction performance. Analysis of various feature combinations reveals that the selected feature set (EVI, FPAR, climate, and soil variables) yields highly accurate results with an R2 = 0.9903, RMSE = 0.1854, MAPE = 0.62%, MAE = 0.1384, MRE = 0.0062, and Willmott's index of agreement = 0.9536. Moreover, the combination of EVI and FPAR is identified as particularly effective. Our findings revealed the potential of our framework for globally estimating crop yields through the utilization of publicly available multi-source data."
Testing intra-species variation in allocation to growth and defense in rubber tree (Hevea brasiliensis),"This perspective manuscript is the revised version of a manuscript which I also reviewed. The authors investigated if the defence of rubber tree (latex yield) shows a trade off with tree growth, however the author did not find any trade-off between defend and growth.
The manuscript has been significantly revised, and I admire the new version with more important details. I agree with the points made in rebuttal letter as well.

There might be a few missing punctuation marks or a typos in some places that I am sure can be dealt with during the proof-reading stage.
Line 55: Such a trade - off will result in trees that grow quickly tend to have weak defenses, while those with strong defenses tend to grow slowly (Fichtner et al., 2017; Z¸st & Agrawal, 2017; Zhu et al., 2018).
…tend.. to tending
Line 67: remove “those of”
Line 96 “source – sink” to source-sink
Line 264 ” Bloomberg's” to Blomberg’s
Line 298 “was marginally related to”
Figure 3: What is the maxim value of Blomberg's K
Table 1, Line 4: “N: number of clones analysed”. The number of the clones is not in the table. I suppose “N” in the table refers to soilN in the footnote (Line 6). Please verify it.",Authentic,"Background: Plants allocate resources to growth, defense, and stress resistance, and resource availability can affect the balance between these allocations. Allocation patterns are well-known to differ among species, but what controls possible intra-specific trade-offs and if variation in growth vs. defense potentially evolves in adaptation to resource availability.

Methods: We measured growth and defense in a provenance trial of rubber trees (Hevea brasiliensis) with clones originating from the Amazon basin. To test hypotheses on the allocation to growth vs. defense, we relate biomass growth and latex production to wood and leaf traits, to climate and soil variables from the location of origin, and to the genetic relatedness of the Hevea clones.

Results: Contrary to expectations, there was no trade-off between growth and defense, but latex yield and biomass growth were positively correlated, and both increased with tree size. The absence of a trade-off may be attributed to the high resource availability in a plantation, allowing trees to allocate resources to both growth and defense. Growth was weakly correlated with leaf traits, such as leaf mass per area, intrinsic water use efficiency, and leaf nitrogen content, but the relative investment in growth vs. defense was not associated with specific traits or environmental variables. Wood and leaf traits showed clinal correlations to the rainfall and soil variables of the places of origin. These traits exhibited strong phylogenetic signals, highlighting the role of genetic factors in trait variation and adaptation. The study provides insights into the interplay between resource allocation, environmental adaptations, and genetic factors in trees. However, the underlying drivers for the high variation of latex production in one of the commercially most important tree species remains unexplained."
"Advances in research on the main nutritional quality of daylily, an important flower vegetable of Liliaceae","1. The section of Conclusion and Outlook can be further expanded, for example, by including prospects for future research directions or predictions on the impact of industrial development, in order to make the entire article more logically complete.
2. The article details the main nutritional components of daylily, including soluble sugars, ascorbic acid, flavonoids, dietary fiber, carotenoids, mineral elements, and polyphenols. These contents are relatively complete, but it may be beneficial to consider incorporating some of the latest research findings or providing a more detailed comparison and analysis of the nutritional differences among different varieties.
3. When describing the medicinal value of daylily, various health benefits such as soothing nerves and improving vision are mentioned. It may be helpful to include some research data supported by literature in this section to enhance the credibility and persuasiveness of the description.
4. Regarding the ornamental value of daylily, the article mentions its application in garden landscaping and flower blooming characteristics. This section can be further expanded, for example, by introducing the ornamental characteristics of different varieties, in order to provide a more comprehensive description of the ornamental value.",Generic,"Daylily (Hemerocallis citrina) is a perennial herb of the genus Hemerocallis of Liliaceae. It is also an economically important crop and is widely cultivated. Daylily has nutritional, medicinal and ornamental values. The research literature shows that daylily is a high-quality food raw material rich in soluble sugars, ascorbic acid, flavonoids, dietary fiber, carotenoids, mineral elements, polyphenols and other nutrients, which are effective in clearing heat and diuresis, resolving bruises and stopping bleeding, strengthening the stomach and brain, and reducing serum cholesterol levels. This article reviews the main nutrients of daylily and summarizes the drying process of daylily. In addition, due to the existence of active ingredients, daylily also has a variety of biological activities that are beneficial to human health. This article also highlights the nutritional quality of daylily, the research progress of dried vegetable rehydration technology and dried daylily. In the end, the undeveloped molecular mechanism and functional research status of daylily worldwide are introduced in order to provide reference for the nutritional quality research and dried processing industry of daylily."
Arbuscular mycorrhizal fungi enhance drought resistance in Bombax ceiba by regulating SOD family genes,"This manuskript provides an intriguing study on the positive effects of arbuscular mycorrhizal fungi (AMF) in boosting drought resistance in Bombax ceiba by altering the expression of superoxide dismutase (SOD) genes. The authors did an extensive work, using genomic and physiological analysis to explore the complex interactions between AMF and plant's response to stress. They've identified 13 SOD genes and thoroughly analyzed their expression under drought situations, offering deep insights into the mechanisms behind the tolerance induced by AMF to stress. However, this research could be even stronger with a more detaild statistical analysis, like explaining why certain sample sizes were choosen. This is especially important for such complex biological interactions as those between plants and fungi under drought. Also, clarifying the normalization methods used in the qRT-PCR data for gene expression would be beneficial, particularly how the reference gene(s) for normalization were picked and their consistency checked across different conditions. This is crucial for validating gene expression data. Moreover, making the figures clearer and providing more info on experimental methods would make the paper easier to read and replicate. Adding these improvements would make the study even more solid and accessible.",Generic,"The physiological activity facilitated by arbuscular mycorrhizal fungi (AMF) contributes to plants' ability to tolerate drought. Nevertheless, it is unclear if AMF colonization affects the expression of genes in the host plant that encode antioxidant enzymes in the superoxide dismutase (SOD) family, which help alleviate drought stress in plants. Here, we conducted a pot trial to determine whether colonization by the AMF Rhizophagus irregularis improves drought resistance in Bombax ceiba. We comprehensively analyzed the SOD gene family and evaluated genome-wide expression patterns of SODs and SOD activity in AMF-colonized and non-mycorrhizal plants under simulated drought. We identified a total of 13 SODs in the genome of B. ceiba, including three FeSODs (BcFSDs), three MnSODs (BcMSDs), and seven Cu/ZnSODs (BcCSDs). Phylogenetic analysis based on binding domain revealed that SOD genes from B. ceiba and various other plant species can be divided into three separate groups, showing significant bootstrap values. Our examination of gene composition and patterns suggests that most BcSOD genes in these three subgroups are significantly conserved. Additionally, it was noted that hormones and stress-responsive cis-regulatory elements were found in all BcSOD promoters. Expression profiling by qRT-PCR demonstrated that AMF increased relative expression levels of Cu/Zn-SODs in both roots and shoots under drought stress, except for BcCSD3 in roots. Furthermore, AMF colonization increased the relative expression of BcMSD1a and BcMSD1b in roots, augmenting SOD activities and increasing ROS scavenging during drought. In general, this work offers molecular evidence in support of the beneficial effect of AMF colonization on drought tolerance in B. ceiba. It also elucidates the expression patterns of SOD genes, which will support efforts to optimize mycorrhizal seedling cultivation under stressful conditions."
"Assessment and phenotypic identification of millet germplasm (Setaria italica L.) in Liaoning, China","The paper titled ""Assessment and phenotypic identiûcation of millet germplasm
(Setaria italica [L.]) in Liaoning, China"": The study provides valuable insights into the phenotypic diversity of millet germplasm in Liaoning, China. The findings can be instrumental in breeding programs aimed at developing improved millet varieties.
-Comments and Suggestions for Authors
In title, (Setaria italica [L.]) could be written as (Setaria italica L.)
Abstract, the abstract is well written.
Introduction, the introduction could benefit from a smoother transition between the general information on millet and the specific focus on Liaoning Province. Perhaps a sentence mentioning the importance of studying germplasm resources at a regional level could bridge the gap.
In materials and methods, consider adding a table summarizing the 31 traits measured (quantitative and qualitative) could enhance readability.
Line 63: ""analysed"" can be changed to ""analyzed"" (US spelling).
In results section, line 69, Results and analysis (this title usually written as ""Results"")
- Line 75: ""a focus on"" can be replaced with ""focused on"" for better flow.
- Line 133: ""directly represent"" can be replaced with ""contribute most to"".
- line 167: In the analysis of 12 quantitative traits in Table 6, the qualities of each group were also compared."" This sentence could be rephrased for better clarity as the following:
""Table 6 also compares the quality traits for each group identified in the cluster analysis.""
- Revise the numbering of subtitles in results section
-The discussion section effectively summarizes the findings and their implications for breeding.
-Some sentences could be rephrased for better clarity. For example, sentence in lines 220- 224 can be rephrased as: ""Correlation analysis of qualitative traits (assigned values based on phenotype) revealed a correlation between pigmentation traits. Additionally, black hull (BH), plant height (TH), spike density (SD), shattering habit (SH), seedling leaf attitude (SC), and lodging resistance (LR) showed correlations with yield, consistent with the results of the diversity analysis.""
In conclusion, consider replacing ""rich diversity"" with more specific details like ""high variation in tillering ability and ear size.
If possible, mention the range of observed values for key traits related to high yield.
Briefly mention potential future research directions based on the findings.",Authentic,"Aims: This study evaluated millet germplasms in Liaoning Province to support the collection, preservation and innovation of millet germplasm resources.

Methods: The study was conducted from 2018 to 2020, involved the selection of 105 millet germplasm resources from the Germplasm Bank of the Liaoning Academy of Agricultural Sciences (LAAS), the observation and recording of 31 traits, and the application of multivariate analysis methods to assess phenotypic diversity.

Results: From the diversity analysis and correlation analysis, it was found that the tested traits had abundant diversity and complex correlations among them. Principal component analysis (PCA) comprehensively analyzed all quantitative traits and extracted seven principal components. Grey relational analysis (GRA) highlighted the varied contributions of different traits to yield. Through systematic cluster analysis (SCA), the resources were categorized into six groups at Euclidean distance of 17.09. K-mean cluster analysis determined the distribution interval and central value of each trait, then identified resources with desirable traits.

Conclusion: The results revealed resources that possess characteristics such as upthrow seedling leaves, more tillers and branches, larger and well-formed ears, and lodging resistance prefer to higher grain yield. It was also discovered that the subear internode length (SIL) could be an indicator for maturity selection. Four specific resources, namely, Dungu No. 1, Xiao-li-xiang, Basen Shengu, and Yuhuanggu No. 1, were identified for further breeding and practical applications."
Localization of potato browning resistance genes based on BSA-seq technology,"This is basic research that provides a basis for exploring the browning resistance genes in potatoes. However, this manuscript needs to be improved before publication with the following issues/concerns resolved.
1. The abstract written is too general. Need to highlight the results with follow-up.
2. Cite the reference for each protocol followed in the study.
3. The methods for 'Library Construction and Sequencing' need more in detail. e.g Data filtration, SNP calling etc. What criteria were used to identify the uniquely mapped reads?
4. Line 168- Group Construction should be replaced by the 'formation of extreme bulks'
5. Before outsourcing for sequencing, how much quantity of DNA of each sample is used for preparing the bulks?
6. In Table 4-Browning weight should be rewritten as browning dark
7. Lines 185-186 'Subsequent screening resulted in 7,910,511 high-quality and reliable SNPs and
186 InDel sites (Table 6 and Fig. 1)'. What it refers to?
8. Line 183- heading is not appropriate.
9. Line 254-Through extensive discussions, -Discussion here is not appropriate, authors can write-investigation.
10. Why did the authors not use the interval parameter and sliding window approach for delta SNP-index calculation as in Takagi et al., 2013, and instead used the ED approach?
11. A total of 21 putative candidate genes within 3.0 Mb interval were identified in this study, which should be verified by RT-PCR experiment. Meanwhile, the published RNA-seq is suggested to be cited in this manuscript to investigate the expression pattern in response to browning, and qRT-PCR experiments helps to confirm the key browning resistance candidate genes. If the authors add these relative results, the manuscript will be more integral to provide significantly valuable information for other researchers.
12. Have you submitted the raw data to NCBI?",Authentic,"Browning is a common problem that occurs during potato processing; it is typically resolved by adding chemicals during the production process. However, there is a need to develop potato varieties that are resistant to browning due to a growing consumer interest in healthier diets. This study initially identified 275 potato varieties that are resistant to browning; these were narrowed down to eight varieties, with four of them being highly resistant. A hybrid population was developed by crossing the highly resistant CIP395109.29 with the easily browned Kexin 23. Bulked segregant analysis (BSA) was conducted, which identified 21 potato genes associated with anti-browning properties through sequencing data analysis and organization. The findings of this study lay a solid groundwork for future research on breeding potatoes with anti-browning traits, offer molecular markers for identifying anti-browning varieties, and serve as a valuable reference for further investigations into potato browning mechanisms."
Research progress on the effects of postharvest storage methods on melon quality,"a. The manuscript is generally well-written. However, there are minor issues with grammar and phrasing that could be streamlined for better readability. For example, some sentences are overly complex or contain redundant words, which can obscure the intended meaning and make the text less accessible. Spelling errors in English words also need to be corrected.

b. The manuscript extensively cites current literature and provides a thorough background on the postharvest storage of melons. It covers various methods and their impacts on melon quality. The manuscript covers various methods and their impact on the quality of melons, providing a comprehensive summary.

c. The structure of the manuscript adheres to academic standards with clear sections including the introduction, methods, results, and conclusions. Figures are used effectively to support the findings, but some minor modifications are necessary.",Generic,"Background
As an important global agricultural cash crop, melon has a long history of cultivation and a wide planting area. The physiological metabolism of melon after harvest is relatively strong; if not properly stored, melon is easily invaded by external pathogens during transportation, resulting in economic losses and greatly limiting its production, development and market supply. Therefore, the storage and freshness of melon are the main challenges in realizing the annual supply of melon, so postharvest storage has received increasing amounts of attention from researchers.

Methods
This study used academic, PubMed, and Web of Science resources to retrieve keywords related to postharvest storage and melon quality; read, refined, classified, and sorted the retrieved literature; sorted and summarized the relevant research results; and finally completed this article.

Results
This article reviews the mechanism and effects of physical, chemical and biological preservation techniques on the sensory quality, compound contents and respiratory physiological activities of different varieties of melon fruits. When maintaining normal metabolism and not producing physiological disorders, melon inhibits cell wall metabolism, reactive oxygen species metabolism and the ethylene biosynthesis pathway, etc., to the greatest extent during postharvest storage, thereby reducing the material consumption of fruits, delaying the ripening and senescence process, and prolonging the postharvest life and shelf life.

Conclusion
The literature provides a theoretical basis for postharvest preservation technology in the melon industry in the future and provides corresponding guidance for the development of the melon industry."
Systematic review assessing the effects of amendments on acidic soils pH in tea plantations,"A total of 58 literatures on the effect of improvement materials on the pH of tea plantations soil were obtained through literature search. the effect of applying improvement materials on increasing the pH of tea plantations soil and its influencing factors were explored. There are some issues with the article, and it is recommended to make major revisions.
1. The title does not match the research content. The title is ""Effects of different improvement materials on acidified soil improvement in tea plantations "", but the content of the article only involves increasing pH. Acidic soils not only suffer from low pH but also from issues such as toxic heavy metal pollution and active aluminum leaching.
2. The data analysis in the article was not thorough. As for the relevant analysis in the chapter, I don't know which factor has a greater impact, it is just a suggestion for readers.
In the relevant analysis of 3.7, if it is possible to tell readers which properties of the modifier have a greater or lesser impact on pH, the effect is better.
3. The logic in the text was unclear. 3.5 includes different modifiers, why is biochar listed separately in 3.6?
4. There are many errors in the article. For example, in Chapter 3.7, Fig.9 is referred to as Fig.8, and the title and content labeling of Fig.9 are incorrect. Suggest checking and modifying the entire text.",Authentic,"Soil acidification has emerged as a critical limiting factor for the sustainable development of the tea industry. In this article, a comprehensive review of 63 original research articles focusing on the impact of amendments on the pH in tea plantations soil was conducted. Through meta-analysis, the effect of applying soil amendments to increase the pH of tea plantation soil and its influencing factors were investigated. The results revealed that lime had a significant impact, increasing the pH by 18% in tea plantation soil, while rapeseed cake had a minimal (2%) effect. It was observed that as the quantity of amendments and pH levels increased, so did their impact on the pH of tea plantation soil. Subgroup analysis within biochar showed varying effects, depending on soil pH, with an 11% increase in acidic soil. Among these amendments, biochar produced at pyrolysis temperature ranging from 501-600 °C and derived from animal waste demonstrated significant effect on increasing soil pH in tea plantations by 9% and 12%, respectively. This study offers valuable insights into improving and ensuring the health and sustainability of tea plantations."
Genetic variation for tolerance to pre-harvest sprouting in mungbean (Vigna radiata) genotypes,"The manuscript gave a very good explanation of the genetic variability for tolerance to pre-harvest sprouting, which is a serious threat, in diverse mungbean genotypes and the association of yield-contributing traits with pre-harvest sprouting, which makes it more interesting. First and foremost, I commend you on the clarity and organization of your writing. Your manuscript is well-structured, making it easy for readers to follow the flow of ideas. The logical progression from introduction to conclusion demonstrates a thoughtful and cohesive approach to presenting your research. I also want to acknowledge the clarity of your figures and tables, which greatly enhance the readability of your manuscript. Your methodology is robust, and the data analysis appears comprehensive. The thoroughness of your research design strengthens the validity of your findings, contributing significantly to the overall quality of the manuscript. The visual representations effectively complement the textual content, providing a well-rounded presentation of your research. However, I would like to kindly highlight a few minor revisions for improvement and offer constructive suggestions to enhance the overall quality of the manuscript. They are listed below:
1. There are a few spacing errors in some places in the manuscript.
2. Check if the values mentioned in lines 162-165 are consistent with the values in Table 1? You may try to highlight the value. It may confuse the reader.",Authentic,"Pre-harvest sprouting (PHS) is one of the important abiotic stresses in mungbean which significantly reduces yield and quality of the produce. This study was conducted to evaluate the genetic variability for tolerance to pre-harvest sprouting in diverse mungbean genotypes while simultaneously deciphering the association of yield contributing traits with PHS. Eighty-three diverse mungbean genotypes (23 released varieties, 23 advanced breeding lines and 37 exotic germplasm lines) were investigated for tolerance to PHS, water imbibition capacities by pods, pod and seed physical traits. Wide variation in PHS was recorded which ranged between 17.8% to 81% (mean value 54.34%). Germplasm lines exhibited higher tolerance to PHS than the high-yielding released varieties. Correlation analysis revealed PHS to be positively associated with water imbibition capacity by pods (r = 0.21) and germinated pod % (r = 0.78). Pod length (r = ?0.13) and seeds per pod (r = ?0.13) were negatively influencing PHS. Positive associations between PHS and water imbibition capacity by pods, germinated pod % and 100-seed weight was further confirmed by multivariate analysis. Small-seeded genotypes having 100-seed weight <3 g exhibited higher tolerance to PHS compared to bold-seeded genotypes having 100-seed weight more than 3.5 g. Fresh seed germination among the selected PHS tolerant and susceptible genotypes ranged from 42% (M 204) to 98% (Pusa 1131). A positive association (r = 0.79) was recorded between fresh seed germination and PHS. Genotypes M 1255, M 145, M 422, M 1421 identified as potential genetic donors against PHS could be utilized in mungbean breeding programs."
Comprehensive evaluation of drought stress on medicinal plants: a meta-analysis,"The authors of this research article have undertaken a comprehensive study on the “Comprehensive evaluation of drought stress on medicinal plants: a meta-analysis” and tried to cover a detail and thorough analysis of how drought conditions affect medicinal plants. Tan and Hatice examined various aspects such as physiological and biochemical responses of these plants to water scarcity. By focusing on medicinal plants, the research emphasizes the implications of drought stress on plants that have significant therapeutic value. The comprehensive nature of the evaluation suggests that the study synthesizes existing research, potentially identifying patterns, critical insights, and knowledge gaps related to how drought impacts these valuable plant species. The methodology is clearly described. The results are quite interesting; however, the manuscript still needs some revision before accepted for final publication in this journal. Following points need to be addressed carefully;
1. There are a number of criticisms/weaknesses that need to be addressed to validate the arguments of the authors and to convey them effectively. In addition, the authors have several typos and grammatical errors in the manuscript and should use more careful writing. Therefore, it is recommended to thoroughly check the manuscript from top to bottom and make the corrections accordingly.
2. In abstract:
• Line-8: stated that limit their growth and productivity should be changed to growth and development.
• Line-18: Other parameters ……… ‘O’ should be in
• Authors need to use SI units for all the parameters and also better to mention the obtained results in number along with their units in all text but especially in the abstract. Also need explanation of the abbreviations first time used in abstract such as DPPH, FRAP and ABTS for general readers.
3. Keywords: Keywords should be in alphabetical order (A…….Z)
4. Manuscript should be uniformly arranged in terms of “units, titles, heading, formula, space in between the lines and font size etc.” as per the format of the journal. Why citations style is different from each other e.g. Line-43: poorly understood. (Kemp et al., 2022) and Line-49: (Asseng et. al. 2011)? Also there is no need of full stop before the citation as already mention above. In some citations authors wrote A & B while in some as A and B why? Authors are suggested to check the whole article thoroughly and make the corrections according the journal recommended format as given in the main webpage of the journal via name instruction to authors.
5. Although statistical analysis is appropriate, however, the authors need to mention about the replicates/repeats per treatment.",Authentic,"Drought stress significantly affects plants by altering their physiological and biochemical processes, which can severely limit their growth and development. Similarly, drought has severe negative effects on medicinal plants, which are essential for healthcare. The effects are particularly significant in areas that rely mostly on traditional medicine, which might potentially jeopardize both global health and local economies. Understanding effects of droughts on medicinal plants is essential for developing strategies to enhance plant adaptability to drought stress, which is vital for sustaining agricultural productivity under changing climatic conditions. In this study, a meta-analysis was conducted on 27 studies examining various parameters such as plant yield, chlorophyll content, relative water content, essential oil content, essential oil yield, non-enzymatic antioxidants, enzymatic antioxidants, phenols, flavonoids, and proline content. The analysis explored the effects of drought across different stress conditions (control, moderate, and severe) to gain deeper insights into the drought's impact. The categorization of these stress conditions was based on field or soil capacity: control (100-80%), moderate (80-50%), and severe (below 50%). This classification was guided by the authors' descriptions in their studies. According to meta-analysis results, enzymatic antioxidants emerge as the most responsive parameters to stress. Other parameters such as relative water content (RWC) and yield also exhibit considerable negative mean effect sizes under all three stress conditions. Therefore, when evaluating the impacts of drought stress on medicinal plants, it is beneficial to include these three parameters (enzymatic antioxidants, RWC, and yield) in an evaluation of drought stress. The chlorophyll content has been determined not to be a reliable indicator for measuring impact of drought stress. Also, measuring antioxidants such as flavonoids and phenols could be a better option than using radical scavenging methods like DPPH (2, 2-difenil-1-pikrilhidrazil), FRAP (ferric reducing antioxidant power), and ABTS (2, 2'-Azino-bis (3-ethylbenzothiazoline-6-sulfonic acid))."
Associations between SNPs and vegetation indices: unraveling molecular insights for enhanced cultivation of tea plant (Camellia sinensis (L.) O. Kuntze),"Overall, the English language usage in this manuscript is quite fluent, meeting the standards for academic publications. The article employs specialized scientific terminology and adheres to the structural and guideline norms of scientific research reporting. However, it should be noted that the results section could be presented more clearly and accurately. The discussion should elucidate the significance of these results and establish a stronger connection with the existing literature on SNP analysis of tea plants (Camellia sinensis).
The references throughout the article provide an adequate background on studies related to the genetics and phenotypic research of tea plants, nitrogen use efficiency, biosynthesis of secondary metabolites, and the impact of nitrogen deficiency on plant physiological and biochemical characteristics. The authors demonstrate a deep understanding of current research trends and existing knowledge. Additionally, by citing the latest research, the article underscores its relevance and timeliness.
The structure and figures of the article appear professional, with designs that are both professional-looking and informative, aiding in the visual presentation of the research findings. However, the resolution of the figures is relatively low, and attention should be paid to whether they meet the journal’s requirements.",Generic,"Background Breeding programs for nutrient-efficient tea plant varieties could be advanced by the combination of genotyping and phenotyping technologies. This study was aimed to search functional SNPs in key genes related to the nitrogen-assimilation in the collection of tea plant Camellia sinensis (L.) Kuntze. In addition, the objective of this study was to reveal efficient vegetation indices for phenotyping of nitrogen deficiency response in tea collection. Methods The study was conducted on the tea plant collection of Camellia sinensis (L.) Kuntze of Western Caucasus grown without nitrogen fertilizers. Phenotypic data was collected by measuring the spectral reflectance of leaves in the 350–1100 nm range calculated as vegetation indices by the portable hyperspectral spectrometer Ci710s. Single nucleotide polymorphisms were identified in 30 key genes related to nitrogen assimilation and tea quality. For this, pooled amplicon sequencing, SNPs annotation and effect prediction with SnpEFF tool were used. Further, a linear regression model was applied to reveal associations between the functional SNPs and the efficient vegetation indices. Results PCA and regression analysis revealed significant vegetation indices with high R2 values (more than 0.5) and the most reliable indices to select ND-tolerant genotypes were established: ZMI, CNDVI, RENDVI, VREI1, GM2, GM1, PRI, and Ctr2, VREI3, VREI2. The largest SNPs frequency was observed in several genes, namely F3’5’Hb , UFGTa , UFGTb , 4Cl , and AMT1.2 . SNPs in NRT2.4 , PIP , AlaDC , DFRa , and GS1.2 were inherent in ND-susceptible genotypes. Additionally, SNPs in AlaAT1 , MYB4 , and WRKY57 , were led to alterations in protein structure and were observed in ND-susceptible tea genotypes. Associations were revealed between flavanol reflectance index (FRI) and SNPs in ASNb and PIP , that change the amino acids. In addition, two SNPs in 4Cl were associated with water band index (WBI). Conclusions The results will be useful to identify tolerant and susceptible tea genotypes under nitrogen deficiency. Revealed missense SNPs and associations with vegetation indices improve our understanding of nitrogen effect on tea quality. The findings in our study would provide new insights into the genetic basis of tea quality variation under the N-deficiency and facilitate the identification of elite genes to enhance tea quality."
Effect of carbon nanoparticles on the growth and photosynthetic property of Ficus tikoua Bur. plant,"I reviewed the manuscript ""Effect of carbon nanoparticles on the growth and photosynthetic property of Ficus tikoua Bur. plant"". I found it well-written, but it has some flaws that can be easily corrected. The experimental design is appropriate, and the results are solid. However, it isn't ready for publication in its current form. Some issues must be corrected and justified in-depth, based on the following comments:

1) The background does not justify why CNP should be administered to the Ficus tikoua Bur plant. More than just wanting to know the effect of spherical CNPs on morphology and photosynthetic parameters is required.
2) There is no specific research question.
3) Is Ficus tikoua a model plant? If so, it should be clarified.
4) Furthermore, if carbon nanotubes were used in all the studies cited in the background, why were spherical nanoparticles used in this work?
5) Something of concern is that there is no explicit justification for administering carbon nanoparticles to plants in general. If nanoparticles translocate from roots to stems and leaves, could they also be translocated to fruits? In any case, the ultimate purpose of all these plants is human consumption. It is well known that some non-functionalized carbon nanoparticles can be very toxic to animal cells; why was this topic not discussed?",Generic,"The application of nanomaterials in different plants exerts varying effects, both positive and negative. This study aimed to investigate the influence of carbon nanoparticles (CNPs) on the growth and development of Ficus tikoua Bur. plant. The morphological characteristics, photosynthetic parameters, and chlorophyll content of F. tikoua Bur. plants were evaluated under four different concentrations of CNPs. Results indicated a decreasing trend in several agronomic traits, such as leaf area, branching number, and green leaf number and most photosynthetic parameters with increasing CNPs concentration. Total chlorophyll and chlorophyll b contents were also significantly reduced in CNPs-exposed plants compared to the control. Notably, variations in plant tolerance to CNPs were observed based on morphological and physiological parameters. A critical concentration of 50 g/kg was identified as potentially inducing plant toxicity, warranting further investigation into the effects of lower CNPs concentrations to determine optimal application levels."
"Impact of nitrogen fertilizer type and application rate on growth, nitrate accumulation, and postharvest quality of spinach","I reviewed the paper titled ""Impact of Nitrogen Source and Application Rate on Growth, Nitrate Accumulation, and Postharvest Quality of Spinach"". The study effectively investigates the key aspects of the research on nitrogen source and application for spinach cultivation.
-Comments and Suggestions for Authors
Abstract
- The abstract is well-written and informative.
- Consider mentioning the specific storage duration investigated in the study (e.g., 10 days) to provide better context for the nitrate content results
- Line 22: Consider replacing ""better"" with ""improved"" for a more formal tone.
- Line 30: ""SPAD values"" could be defined in parentheses for readers unfamiliar with the term (e.g., SPAD values (…..)).
Introduction
- Strengthen the connection between yield and nitrate: While you mention Vico et al. (2020) finding similar yields with organic amendments and conventional practices despite higher nitrate in organically fertilized spinach, you could delve deeper into the potential trade-offs between yield and nitrate content.
- Consider mentioning limitations of existing research: Briefly acknowledge the limited research on slow-release fertilizers' impact on nitrate accumulation in leafy vegetables.
Materials and methods
- This section provides a comprehensive overview of the experimental methods used. It allows readers to understand how the experiment was conducted and evaluate the validity of the results.
- Line 185: remove ""Methods""
Results
- The study only investigated a single spinach cultivar. Results may not be applicable to other varieties.
- The study examined nitrate content during storage, but it's not clear if the storage conditions typically encountered by consumers were mimicked.",Authentic,"Background: A balanced supply of nitrogen is essential for spinach, supporting both optimal growth and appropriate nitrate (NO3 -) levels for improved storage quality. Thus, choosing the correct nitrogen fertilizer type and application rate is key for successful spinach cultivation. This study investigated the effects of different nitrogen (N) fertilizer type and application rates on the growth, nitrate content, and storage quality of spinach plants.

Methods: Four fertilizer types were applied at five N doses (25, 50, 200, and 400 mg N kg-1) to plants grown in plastic pots at a greenhouse. The fertilizer types used in the experiment were ammonium sulphate (AS), slow-release ammonium sulphate (SRAS), calcium nitrate (CN), and yeast residue (YR). Spinach parameters like Soil Plant Analysis Development (SPAD) values (chlorophyll content), plant height, and fresh weight were measured. Nitrate content in leaves was analyzed after storage periods simulating post-harvest handling (0, 5, and 10 days).

Results: The application of nitrogen fertilizer significantly influenced spinach growth parameters and nitrate content. The YRx400 treatment yielded the largest leaves (10.3 ± 0.5 cm long, 5.3 ± 0.2 cm wide). SPAD values increased with higher N doses for AS, SRAS, and CN fertilizers, with AS×400 (58.1 ± 0.8) and SRAS×400 (62.0 ± 5.8) reaching the highest values. YR treatments showed a moderate SPAD increase. Fresh weight response depended on fertilizer type, N dose, and storage period. While fresh weight increased in all fertilizers till 200 mg kg-1 dose, a decrease was observed at the highest dose for AS and CN. SRAS exhibited a more gradual increase in fresh weight with increasing nitrogen dose, without the negative impact seen at the highest dose in AS and CN. Nitrate content in spinach leaves varied by fertilizer type, dose, and storage day. CNx400 resulted in the highest NO3 - content (4,395 mg kg-1) at harvest (Day 0), exceeding the European Union's safety limit. This level decreased over 10 days of storage but remained above the limit for CN on Days 0 and 5. SRAS and YR fertilizers generally had lower NO3 - concentrations throughout the experiment. Storage at +4 °C significantly affected NO3 - content. While levels remained relatively stable during the first 5 days, a substantial decrease was observed by Day 10 for all fertilizers and doses, providing insights into the spinach's nitrate content over a 10-day storage period.

Conclusion: For rapid early growth and potentially higher yields, AS may be suitable at moderate doses (200 mg kg-1). SRAS offers a more balanced approach, promoting sustained growth while potentially reducing NO3 - accumulation compared to AS. Yeast residue, with its slow nitrogen release and consistently low NO3 - levels, could be a viable option for organic spinach production."
Estimation of environment stability for fruit yield and capsaicin content by using two models in Capsicum chinense Jacq. (Ghost Pepper) with multi-year evaluation,"The review article entitled, ""Estimation of environment stability for fruit yield and capsaicin content by using two models in Capsicum chinense Jacq. (Ghost Pepper) with multi-year evaluation (#96473) combines knowledge and application of the methodology. Readers of the journal and other researchers working on the topic internationally may find this publication useful. However, some information is missing. Given the following considerations, I believe that this work should be published, following minor changes:
1. Describe more about Figures 3 and 4 in the discussion.
2. The conclusion is very short. Revise to include the more important findings.
3. Stick to the journal's reference format for reference.
4. The references list's formatting contains several formatting problems. Here, journal titles are published in full for some and abbreviated. Please revise as per Journal style guide.
5. For specific references, there exist DOIs. Include all potential DOI numbers in the reference list, along with more recent references.
6. In addition to the previously given comments, authors are requested to proofread the entire text again to make sure that the amended version is free of any common grammatical problems.",Authentic,"Background: Capsicum chinense Jacq. (Ghost Pepper) is well-known for its high pungency and pleasant aroma. The recent years witnessed a significant decline in popularity of this important crop due to the use of inferior planting material and lack of elite lines. To maintain constant performance across a variety of settings, it is crucial to choose stable lines with high yield and capsaicin content, as these are the most promising traits of Ghost Pepper.

Method: In this study, 120 high-capsaicin genotypes were subjected to a 3-year (kharif 2017, 2018 and 2019) stability investigation utilizing two well-known stability methods: Eberhart-Russell (ER) and additive main effects and multiple interaction (AMMI). Three replications were used following Randomized Complete Block Design for 11 traits. The experiment soil was sandy loam with pH 4.9. Minimum and maximum temperature of 18.5 °C, 17.5 °C, 17.4 °C and 32.2 °C, 31.3 °C, 32.7 °C and rainfall of 1,781, 2,099, 1,972 mm respectively was recorded for the study period.

Result: The genotype-environment linear interaction (G×E Lin.) was highly significant for days to 50% flowering, capsaicin content, fruit length and girth, fruit yield per plant and number of fruits per plant at p < 0.005. G×E interaction for fruit yield and capsaicin content in AMMI-analysis of variance reported 67.07% and 71.51% contribution by IPCA-1 (interactive principal component axis) and 32.76% and 28.49% by IPCA-2, respectively. Eight genotypes were identified to be stable with high yield and capsaicin content. The identified stable lines can be opted for cultivation to reduce the impact of crop failure when grown in different macro-environments. Moreover, the pharmaceutical and spice sectors will also be benefitted from the lines with high capsaicin content. Further research assessing the lines' performance across various regions of India can provide a solid foundation for the crop's evaluation at national level."
Endophytic actinomycetes promote growth and fruits quality of tomato (Solanum lycopersicum): an approach for sustainable tomato production,"The article is dealt with the effect of endophytic actionmycetes in enhancing the tomato growth and fruit quality. In the current scenario, the development of microbial bio-agents is gaining importance specifically for the sustainable management of soil and plant health. Though, several studies has been reported for Bacillus and derived genera, the exploration of actinomycetes is still in the nascent stage. In this context, the reported study adds scientific knowledge on the subject.

The following are the point-wise comments to the authors to address.
1.In the introduction section, the authors have mentioned that the excessive use of chemicals lead to decline in the soil and plant performance and the plant growth promoting microorganisms is helpful to reduce the use of chemicals in tomato cultivation(line no. 58-60). However, the same is missing in the hypothesis statement.
2.There are grammatical mistakes in the manuscript, for example, line no. 119. and Line no. 277. Reorganize the sentences. Check the grammar and sentences throught the manuscript.
3.The uniform unit of measurement should be given. For example line no. 141 and 142, weight is given in different forms (grams/g).

1.The experimental design fails to address the partial replacement of chemicals with plant growth promoting actinomycetes.The nutrients supplementation to the crop plants are given evenly for both inoculated and un-inoculated plants. The authors itself mentioned that the actinomycetes isolates used in the study have growth and nutrient supplementation properties (line no. 112-113). And, the major purpose of the study is to find an alternative to chemicals in tomato cultivation and the same is not emphasized in this study.
2.Line no. 106- detail the control
3.:Line no. 111- whether the isolates were procured isolated in this study?. If isolated, the details of isolation and screening should be given.",Authentic,"Background: Tomato, a fruit with a high vitamin content, is popular for consumption and economically important in Thailand. However, in the past year, the extensive usage of chemicals has significantly decreased tomato yields. Plant Growth-Promoting Rhizobacteria (PGPR) is an alternative that can help improve tomato production system growth and yield quality while using fewer chemicals. The present study aimed to determine whether endophytic actinomycetes promote growth and fruit quality of tomato (Solanum lycopersicum).

Methods: The experiment was conducted in a net-houses at the Center for Agricultural Resource System Research, Faculty of Agriculture, Chiang Mai University, Chiang Mai province, Thailand. The randomized completely block design (RCBD) was carried out for four treatments with three replications, which was control, inoculation with TGsR-03-04, TGsL-02-05 and TGsR-03-04 with TGsL-02-05 in tomato plant. Isolated Actinomycetes spp. of each treatment was then inoculated into the root zone of tomato seedlings and analyzed by Scanning Electron Microscopy (SEM). The height of tomato plants was measured at 14, 28, 56, and 112 days after transplanting. Final yield and yield quality of tomato was assessed at the maturity phase.

Results: The SEM result illustrated that the roots of tomato seedlings from all treatments were colonized by endophytic actinomycetes species. This contributed to a significant increase in plant height at 14 days after transplanting (DAT), as found in the TGsR-03-04 treatment (19.40 cm) compared to the control. Besides, all inoculated treatments enhanced tomato yield and yield quality. The highest weight per fruit (47.38 g), fruit length (52.37 mm), vitamin C content (23.30 mg 100 g-1), and lycopene content (145.92 µg g-1) were obtained by inoculation with TGsR-03-04. Moreover, the highest yield (1.47 kg plant-1) was obtained by inoculation with TGsL-02-05. There was no statistically significant difference in the number of fruits per plant, fruit width, brix, and antioxidant activity when various inoculations of endophytic actinomycetes were applied. Therefore, the use of endophytic actinomycetes in tomato cultivation may be an alternative to increase tomato yield and yield quality."
Expression profile analysis of cotton fiber secondary cell wall thickening stage,"The mature cotton fiber has a unique cell wall structure that is distinct from xylem of monocot and dicot plants due to the massive deposition of cellulose during the fiber SCW thickening stage. Cotton fiber cell walls not only determine fiber morphogenesis, but also determine fiber quality such as length and strength. It is therefore of interest to understand SCW biosynthesis and regulation for genetic improvement of cotton fibers. Liu et al selected three cotton cultivars: Sea Island cotton (Xinhai 32) and upland cotton (17-24 and 62-33) which have fibers with different qualities and performed RNA-seq analysis in order to identify candidate genes involved in fiber SCW thickening.
The major concern is why this three cultivars were selected and can be used to identify candidate genes involved in SCW biosynthesis, more specifically genes determining fiber strength. In addition, readers cannot get useful information from most of the main figures. Moreover, no experimental evidence shows that any of genes was indeed involved in SCW biosynthesis.",Generic,"To determine the genes associated with the fiber strength trait in cotton, three different cotton cultivars were selected: Sea Island cotton (Xinhai 32, with hyper-long fibers labeled as HL), and upland cotton (17-24, with long fibers labeled as L, and 62-33, with short fibers labeled as S). These cultivars were chosen to assess fiber samples with varying qualities. RNA-seq technology was used to analyze the expression profiles of cotton fibers at the secondary cell wall (SCW) thickening stage (20, 25, and 30 days post-anthesis (DPA)). The results showed that a large number of differentially expressed genes (DEGs) were obtained from the three assessed cotton cultivars at different stages of SCW development. For instance, at 20 DPA, Sea Island cotton (HL) had 6,215 and 5,364 DEGs compared to upland cotton 17-24 (L) and 62-33 (S), respectively. Meanwhile, there were 1,236 DEGs between two upland cotton cultivars, 17-24 (L) and 62-33 (S). Gene Ontology (GO) term enrichment identified 42 functions, including 20 biological processes, 11 cellular components, and 11 molecular functions. Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis identified several pathways involved in SCW synthesis and thickening, such as glycolysis/gluconeogenesis, galactose metabolism, propanoate metabolism, biosynthesis of unsaturated fatty acids pathway, valine, leucine and isoleucine degradation, fatty acid elongation pathways, and plant hormone signal transduction. Through the identification of shared DEGs, 46 DEGs were found to exhibit considerable expressional differences at different fiber stages from the three cotton cultivars. These shared DEGs have functions including REDOX enzymes, binding proteins, hydrolases (such as GDSL thioesterase), transferases, metalloproteins (cytochromatin-like genes), kinases, carbohydrates, and transcription factors (MYB and WRKY). Therefore, RT-qPCR was performed to verify the expression levels of nine of the 46 identified DEGs, an approach which demonstrated the reliability of RNA-seq data. Our results provided valuable molecular resources for clarifying the cell biology of SCW biosynthesis during fiber development in cotton."
"Exploring main soil drivers of vegetation succession in abandoned croplands of Minqin Oasis, China","Firstly, I acknowledge the authors for their impressive job improving the original manuscript. However, there is still some confusion about the hypothesis, the aims of the study and the final results that support (or not) the hypothesis. Evidence of this appears just at the beginning of the manuscript, i.e., the mismatch between the title, the abstract, and the last paragraph of the introduction. Please, amend it because this is critical to not lose the readers and most importantly, to be consistent with the scientific method.

In general, the references included have also been improved, though I have some suggestions for the authors regarding the previous research in the area:

L100 Could the authors include the references? “However, few studies have explored these connections.”

Please, read Section 1 about the research question. Being aware of limitations in resources and in representative areas with enough age range etc., it’s true that the study lacks power not including other areas different from the Hu area. I totally understand why the authors restricted their study to the Hu area but they should include the explanation of this limitation in the manuscript, and highlight potential research needs in their discussion.

There is no need to include that you recorded the scientific names of the species, L146: where each shrub or herbaceous species was recorded using binomial nomenclature",Generic,"Background
The Minqin Oasis, which is located in Wuwei City, Gansu Province, China, faces a very serious land desertification problem, with about 94.5% of its total area desertified. Accordingly, it is crucial to implement ecological restoration policies such as cropland abandonment in this region. In abandoned croplands, abiotic factors such as soil properties may become more important than biotic factors in driving vegetation succession. However, the connections between soil properties and vegetation succession remain unclear. To fill this knowledge gap, this study investigated these connections to explore major factors that affected vegetation succession, which is meaningful to designing management measures to restore these degraded ecosystems.

Methods
This study investigated seven 1–29-year-old abandoned croplands using the “space for time” method in Minqin Oasis. Vegetation succession was classified into different stages using a canonical correlation analysis (CCA) and two-way indicator species analysis (Twinspan). The link between soil properties and vegetation succession was analyzed using CCA. The primary factors shaping community patterns of vegetation succession were chosen by the “Forward selection” in CCA. The responses of dominant species to soil properties were analyzed using generalized additive models (GAMs).

Results
Dominant species turnover occurred obviously after cropland abandonment. Vegetation succession can be classified into three stages (i.e., early, intermediate, and late successional stages) with markedly different community composition and diversity. The main drivers of vegetation succession among soil properties were soil salinity and saturated soil water content and they had led to different responses of the dominant species in early and late successional stages. During the development of vegetation succession, community composition became simpler, and species diversity decreased significantly, which was a type of regressive succession. Therefore, measures should be adopted to manage these degraded, abandoned croplands."
Elucidating the regulatory role of long non-coding RNAs in drought stress response during seed germination in leaf mustard,"The quality of English needs to be improved and authors are advised to check their manuscript with a fluent English speaker. e.g. Ensure consistency of terminology throughout the introduction. For example, ""drought-tolerant"" and ""drought-sensitive"" are hyphenated in some cases and not in others. Consistent hyphenation and terminology improves readability.

Methods not sufficiently described I could not find any information on the RNA extraction method or the sequencing profile or criteria for the identification of LncRNA..

This study provides excellent information about this neglected plant. The results and findings are presented in a nice way. Although the discussion is really disorganised and does not agree with the author's findings,the discussion should mainly focus on the research findings and consolidate their findings with other similar research rather than re-explaining the findings.
the authors must draw conclusions, which should be based on the objectives, method and material, results, discussion and future perspectives.",Generic,"Leaf mustard (Brassica juncea L. Czern & Coss), an important vegetable crop, experiences pronounced adversity due to seasonal drought stress, particularly at the seed germination stage. Although there is partial comprehension of drought-responsive genes, the role of long non-coding RNAs (lncRNAs) in adjusting mustard's drought stress response is largely unexplored. In this study, we showed that the drought-tolerant cultivar 'Weiliang' manifested a markedly lower base water potential (-1.073 MPa vs -0.437 MPa) and higher germination percentage (41.2% vs 0%) than the drought-susceptible cultivar 'Shuidong' under drought conditions. High throughput RNA sequencing techniques revealed a significant repertoire of lncRNAs from both cultivars during germination under drought stress, resulting in the identification of 2,087 differentially expressed lncRNAs (DELs) and their correspondingly linked 12,433 target genes. It was noted that 84 genes targeted by DEL exhibited enrichment in the photosynthesis pathway. Gene network construction showed that MSTRG.150397, a regulatory lncRNA, was inferred to potentially modulate key photosynthetic genes (Psb27, PetC, PetH, and PsbW), whilst MSTRG.107159 was indicated as an inhibitory regulator of six drought-responsive PIP genes. Further, weighted gene co-expression network analysis (WGCNA) corroborated the involvement of light intensity and stress response genes targeted by the identified DELs. The precision and regulatory impact of lncRNA were verified through qPCR. This study extends our knowledge of the regulatory mechanisms governing drought stress responses in mustard, which will help strategies to augment drought tolerance in this crop."
A taxonomic revision of Rhizophora L. (Rhizophoraceae) in Thailand,"The study was sufficiently thorough, and the author's keen efforts in this work are apparent. However, the research question was not well defined, relevant or meaningful. The knowledge gap that the research is addressing is not identified. Rhizophora apiculata and R. mucronata are two of the most easily identified mangrove species, thus the necessity of a taxonomic revision was not fully demonstrated in the manuscript. The author should explain why a detailed taxonomic revision of Thai Rhizophora is needed. Are the Rhizophora species in Thailand botanically unique in any way?

Also, how does this relate to Rhizophora species found in neighbouring countries? while comparing the various botanical feature of Thai specimens to those in previous studies, it is important to note where the previous studies were conducted, in order to understand the geographical relevance of these differences.

The rationale and benefit to the literature has not been demonstrated clearly.

Minor comment: It would be better to cite Tomlinson 2016 (second edition) instead of Tomlinson 1986 (first edition).",Generic,"A taxonomic revision of Rhizophora L. (Rhizophoraceae) in Thailand is presented. Two species, R. apiculata Blume and R. mucronata Poir., are enumerated with updated morphological descriptions, illustrations and a taxonomic identification key, together with notes on distributions, habitats and ecology, phenology, conservation assessments, etymology, vernacular names, uses, and specimens examined. Three names in Rhizophora, are lectotypified: R. apiculata and two associated synonyms of R. mucronata (i.e., R. latifolia Miq. and R. macrorrhiza Griff.). R. longissima Blanco, a synonym of R. mucronata, is neotypified. All two Rhizophora species have a conservation assessment of Least Concern (LC). Based on the morphological identification, these two species can be distinguished from one another by the shape and width of the leaf laminae and the length of a terminal stiff point of the leaf laminae; the type and position of the inflorescences and the number of flowers per inflorescence; the character and color of the bracteoles; the presence or absence of the flower pedicels; the shape of the mature flower buds; the shape, color, and texture of the sepals; the shape, character, and the presence or absence of hairs of the petals; the number of stamens per flower; the size of the fruits; the color and size of the hypocotyls; the color and diameter of the cotyledonous cylindrical tubes; and the color of the colleters and exudate. The thick cuticles, sunken stomata, large hypodermal cells, and cork warts are adaptive anatomical features of leaves in Rhizophora that live in the mangrove environments. The pollen grains of Thai Rhizophora species are tricolporate, prolate spheroidal or oblate spheroidal shapes, small-sized, and reticulate exine sculpturing."
HFSA: hybrid feature selection approach to improve medical diagnostic system,"Basic reporting
This paper introduces an advanced Diagnostic System (DS) leveraging Artificial Intelligence (AI) techniques for precise and efficient medical diagnostics. The proposed system incorporates a novel Hybrid Feature Selection Approach (HFSA) that combines filter-based and wrapper-based methodologies to optimize feature selection and enhance the performance of the Naive Bayes (NB) classifier. Through its multi-layered structure—Rejection Layer (RL), Selection Layer (SL), and Diagnostic Layer (DL)—the system demonstrates significant improvements in diagnostic accuracy, precision, and recall. The revision should focus on enhancing the scientific rigor of the methodology, clarifying certain ambiguous details, and strengthening the experimental validation.

a. The novelty and computational feasibility of combining Genetic Algorithm (GA) with the Tiki-Taka Algorithm (T²A) in the Accurate Stage (AS) of HFSA should be elaborated. Include an analysis of why this hybrid optimization is superior to existing methods for feature selection in medical diagnostics.

b. Clarify the choice of chi-square as the filter methodology in the Fast Stage (FS). Provide a justification of its suitability compared to other statistical methods, particularly in handling high-dimensional medical datasets.

c. Expand the explanation of the Rejection Layer (RL) to include details about how GA identifies and removes outliers. Specify whether the definition of outliers is domain-specific or generalized across datasets.

d. Incorporate a thorough evaluation of HFSA against state-of-the-art feature selection approaches. Compare their computational complexity, runtime efficiency, and diagnostic performance to substantiate the claim that HFSA outperforms existing methods.

e. The experimental setup requires more details, including dataset characteristics, parameter configurations, and cross-validation techniques used. This ensures reproducibility and provides transparency for performance metrics such as accuracy, precision, recall, and error rates.

f. The integration of the NB classifier within the Diagnostic Layer (DL) should be critically assessed. Discuss the trade-offs in using NB versus more advanced classifiers like Support Vector Machines (SVM) or ensemble methods in the context of medical diagnostics.

g. Add a detailed discussion of potential limitations of the proposed system. For example, how sensitive is HFSA to changes in data distribution, class imbalance, or noisy features in real-world medical datasets?

h. The paper should include a future research direction section that addresses the scalability of the proposed system, particularly when dealing with rapidly growing datasets or integrating multi-modal medical data (e.g., text, images, and numerical records).

The Literature citation is not adequate, and the related work to machine learning should be discussed:
1. Robust semi-supervised multi-label feature selection based on shared subspace and manifold learning
2. Sparse feature selection using hypergraph Laplacian-based semi-supervised discriminant analysis
3. Nonnegative Matrix Factorization in Dimensionality Reduction: A Survey

Experimental design
not

Validity of the findings
not",Authentic,"Thanks to the presence of artificial intelligence methods, the diagnosis of patients can be done quickly and accurately. This article introduces a new diagnostic system (DS) that includes three main layers called the rejection layer (RL), selection layer (SL), and diagnostic layer (DL) to accurately diagnose cases suffering from various diseases. In RL, outliers can be removed using the genetic algorithm (GA). At the same time, the best features can be selected by using a new feature selection method called the hybrid feature selection approach (HFSA) in SL. In the next step, the filtered data is passed to the naive Bayes (NB) classifier in DL to give accurate diagnoses. In this work, the main contribution is represented in introducing HFSA as a new selection approach that is composed of two main stages; fast stage (FS) and accurate stage (AS). In FS, chi-square, as a filtering methodology, is applied to quickly select the best features while Hybrid Optimization Algorithm (HOA), as a wrapper methodology, is applied in AS to accurately select features. It is concluded that HFSA is better than other selection methods based on experimental results because HFSA can enable three different classifiers called NB, K-nearest neighbors (KNN), and artificial neural network (ANN) to provide the maximum accuracy, precision, and recall values and the minimum error value. Additionally, experimental results proved that DS, including GA as an outlier rejection method, HFSA as feature selection, and NB as diagnostic mode, outperformed other diagnosis models."
Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation,"Basic reporting
The paper is well-written and easy to follow. However, it would benefit from more emphasis on the rationale behind the development of the proposed method. While the paper thoroughly explains ""what"" was done, the reasoning behind the specific design choices remains unclear. Further clarification on this would strengthen the work. See below for additional details.

Experimental design
To summarize the contribution, the paper adapts the method from Kim et al. (2010), originally designed for simulating small bubbles in water, and applies it to simulate fire flakes using a relatively simple fire model from Nguyen et al. (2002). This contribution seems somewhat minimal, especially considering the length of the paper.

The authors make several claims about why the motion of fire flakes is fundamentally different from that of bubbles:
50: ""On the other hand, fire-flake particles, being lighter than air and significantly influenced by air resistance, exhibit relatively more chaotic and disorderly movement.""
135: ""Most existing methods have focused on representing bubbles in water, and there has been relatively less research specifically dedicated to modeling fire-flake particles represented by flames.""
253: ""unlike air bubbles in water, near the flame, there are turbulent interactions between fuel and air that give rise to chaotic movements.""
447: ""Unlike air bubbles, fire-flake particles exhibit highly dynamic movements in chaotic regions.""
However, these assertions are not supported by evidence (citations would be helpful), and yet, they still proceed to apply an existing bubble simulation technique to model fire flakes, which appears contradictory.

Additionally, the paper proposes training a neural network to learn the emission, trajectories, and lifespan of fire flakes. However, the training uses synthetic data generated by the paper's own simulation method, which raises the question of why an already established model is being ""re-learned.""

Validity of the findings
The main result of the paper shows that the proposed method generates more break-up in the motion of fire flake particles compared to previous methods that did not incorporate chaotic advection. While this outcome is expected, it is also unsurprising and aligns with the findings of Kim et al. (2010), who achieved something similar with bubbles.

The machine learning (ML) aspect of the paper is intriguing, but as noted earlier, its purpose remains unclear. The approach essentially re-learns an already established model (which, not surprisingly, yields similar results), and the paper does not demonstrate any performance benefits of using an ML technique over traditional simulation methods.

Additional comments
All of the simulations in the paper are conducted in 2D, and there is no supplemental video provided, making it difficult to evaluate the quality of the results.

Overall, I would consider the paper's contribution to be quite limited. It seems premature for publication and would require significant revisions.",Authentic,"In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments."
Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation,"Basic reporting
The overall workflow of the work is appreciable.

Experimental design
Experimental design and the results provided is goo and sufficient.

Validity of the findings
The article meets the standard of the journal due to the experimental results given is good.",Generic,"In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments."
Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation,"Basic reporting
This paper proposes an efficient technique to simulate dispersed fire-flake particles that respond to the movement of a flame. The paper is well-organized, and the findings are clearly represented. I would recommend the paper for publication after minor revision.
1. Check the comma and the punctuation in the text and at the ends of equations (Equations 3, 6 and the rest).
2. Check the equation number on line 215.
3. Tell us more about the numerical method used. What are the limitations of the method? Are there other robust methods? What motivates the choice of the current method to present the results.
4. The abstract should contain some obtained results through an ending sentence.
5. On Line 222, replace ‘In this paper’ by ‘In addition’ or any equivalent expression as it is written in the previous sentence.
6. Discuss the implications of your findings for real-world applications.

Experimental design
Yes

Validity of the findings
Yes

Additional comments
NO",Authentic,"In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments."
FLASC: a flare-sensitive clustering algorithm,"Basic reporting
No comment
Experimental design
1. Why was the FLASC algorithm compared with the K-means algorithm, if it is known by the clustering community that this algorithm does not detect groups of irregular figures? I think that being very popular is not a good argument for the comparison.
2. What are the values of the parameters used by the DBSCAN* algorithm.
3. How was the optimal number of groups determined in the K-means algorithm?
4. It is recommended to compare the proposed algorithm with an OPTICS algorithm.
5. Could you explain and give arguments for the “Evaluation and settings” sections of all the cases presented.
6. Could you explain what is written in lines 402-405, why K=5, min cluster size Mc = 100…..
7. How was the efficiency of the clusters obtained by the three algorithms measured?
Validity of the findings
No comment",Authentic,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc."
FLASC: a flare-sensitive clustering algorithm,"Basic reporting
1. The selection criteria of the algorithm parameters (e.g., distance threshold, minimum branch size, etc.) are not discussed in detail during the experiment. It is suggested to add the comparison experiment of the performance of the algorithm under different parameters to show the robustness and adaptability of the FLASC algorithm.

2. In the past three years, several studies have addressed the challenge of flare detection. Notable examples include, “Clustering by Measuring Local Direction Centrality for Data with Heterogeneous Density and Weak Connectivity” (Nature Communications), and TWStream (IEEE TFS). These algorithms merit further discussion.

Experimental design
1.The experimental comparison object focuses on a major density clustering algorithm (HDBSCAN*). In order to more fully demonstrate the superiority of FLASC, it is suggested to introduce other density clustering algorithms, such as, Block-Diagonal Guided DBSCAN Clustering(IEEE TKDE github: https://github.com/y66y/TKDE) and M3W (IEEE TNNLS github: https://github.com/Du-Team/M3W).

**PeerJ Staff Note:** It is PeerJ policy that additional references/work suggested during the peer-review process should only be included if the authors are in agreement that they are relevant and useful.

2.The current experimental datasets are mainly simulated data or datasets of specific nature, and lack the testing of real application scenarios. It is suggested to add some datasets from real applications to verify the practicality of FLASC.

Validity of the findings
The time complexity of FLASC algorithm is theoretically analyzed in the paper, so the time comparison experiments are very much expected.",Authentic,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc."
FLASC: a flare-sensitive clustering algorithm,"Basic reporting
* There are certain terms that are not clear (at least to this reviewer) in the following lines:

- 14: ""their shape can represent"", please be more explicit in the paper what a shape is, provide a definition or an intuition of it.

- 17: in how far do branches 'describe' within-cluster connectivity? please elaborate on that within the paper

- 19: 'stable outputs', stable in which term/with respect to what? Please elaborate on that

- 20: 'flare sensitive' what is meant by that term? please define or provide intuitions for that within the text

- 25: 'traditional algorithms', what is understood by the authors via the term 'traditional'?

- 31: 'subpopulations': what is meant by that term? do you mean subsets of objects/data points?

- 36: ""Flares are connected in the simplicial complex"", please provide a definition of flares in context of topological data analysis (TDA)

- 55+56: ""branching hierarchies"" and ""branch-based subgroups of clusters"": Please provide a definition of these terms. Furthermore what is the difference in context of your manuscript regarding the terms subgroups and subpopulations?

- 62+63: good! nice reference of existing work for branch detection in 3d models of plants

- 70: ""...intuitive ... and branch sizes..."" in how far are branch sizes intuitive? please elaborate

- The elaboration on HDBSCAN* is very well done and elaborate!

Experimental design
- 155: ""distance between them is smaller than or equal to 1/lambda_t"", why is that the case or thresholded by that? please elaborate on that matter

- 178: ""no cluster selection epsilon"", what is meant by that? do you refer to an epsilon-neighborhood radius? please elaborate

-236: ""The eccentricity function (Equation (4)) is more complex to analyse..."" with respect to what? what makes it so complex? please elaborate

- You compare FLASC against k-means and HDBSCAN*, while the latter makes sense, since your proposed method shares many theoretical and algorithmic foundations, it remains unclear why you do not compare against other methods. Most important this reviewer recommends to compare also against Hierarchical Clustering (e.g. SLINK) and Spectral clustering. Both of them rely on trees/graphs as their underlying structures. Prior works by Hess et. al [The relationship of DBSCAN to matrix factorization and spectral clustering] and Beer et. al [Connecting the Dots--Density-Connectivity Distance unifies DBSCAN, k-Center and Spectral Clustering], also revealed commonalities between DBSCAN, Spectral Clustering.

**PeerJ Staff Note:** It is PeerJ policy that additional references suggested during the peer-review process should only be included if the authors are in agreement that they are relevant and useful.

Furthermore regarding the discovery of branches, this reviewer recommends to compare FLASC against 4C, a density-based clustering algorithm [Computing Clusters of Correlation Connected Objects] that is capable to detect linear correlations (to a certain degree discovering linear branches) within a given dataset.

To test your method against the proposed algorithms has the purpose to substantially underline the power of your method by comparing against other methods that should, in theory, be capable to detect it.

- while the figures with the ARI scores are convincing (cf. Fig. 4), a question that comes up is: how does FLASC behave if the shape of three branches is more like a Y-shape or in 3D two Y-shapes with a certain viccinity to each other. Please provide 2-3 more complex datasets in which FLASC is challenged to convince the readers that it is also capable to deal with different shapes and different proximities of clusters to each other

Validity of the findings
The used methods are well established and based on the datasets on which the experiments have been conducted they seem meaningful, however, as mentioned in 2. Experimental Design, more experiments are needed to investigate the capabilities and limitations of that method.",Authentic,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc."
FLASC: a flare-sensitive clustering algorithm,"Basic reporting
This paper was written very well and has a good opportunity to be published in this great journal, after addressing the following concerns in the first round:

- I encourage you to add more detail about your core contributions in the abstract. Abstract has five-section and you should follow the best practices in your area! Please also mention the novelties in the abstract.
- Long paragraphs.
- Please bring some facts and figures in the introduction to support the ideas.

Experimental design
- Literature review is very short and old! We are in jan 2024! You have not covered the knowledge edge! Please clarify the contribution of the paper according to the research gap. Many recent papers in the area can be added to the literature review. You have not referred to the main works in this area. I do not propose you any special reference due to ethical issues. The authors should review the recent works in these areas. I simply search and find following works and similar algorithm like, TGA, SEO, RDA, HBA, FHO, etc. Do and implement your algorithm in professional way by needed experiments... Also, you should cover these algorithms in the LR and refer to the works as samples for their usages.

Also, you should cover these algorithms in the LR and refer to the works as samples for their usages.
HBA, KA, SEO, FHO, TGA, etc.

Validity of the findings
- Please add research gap section.


I go with a major revision in this step and waiting for your corrections. Then, I give you my technical comments.

Additional comments
- Check the English presentation of this paper to remove the typos mistakes.
- Findings, limitations, and recommendations of this paper can be discussed more in the conclusion section.
- Please bring and focus on future research directions.",Authentic,"Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc."
Dynamic prediction of carbon prices based on the multi-frequency combined model,"Basic reporting
The topic is interesting and with possible applicability. However, I have some comments and suggestions which could greatly improve the manuscripts quality and these are:
1- The authors should highlight why COA is selected despite here are several other metaheuristic methods were available
2- Environmental setting used to implement the COA, and other machine learning should be added
3- Some new works using and applicable studies should be added.
4- What is the advantage over established techniques.
5- Conclusion is very weak. It should summarize the research, mention the best scores achieved, mention limitations and finally provide clear directions for the future research in this domain

Experimental design
Environmental setting used to implement the COA, and other machine learning should be added

Validity of the findings
The results are good

Additional comments
N/A",Authentic,"As a central participant and important leader in the global climate governance system, China is facing the urgent need to predict and regulate the price of carbon emissions to promote the sound development of its carbon market. In this article, a rolling prediction model based on Least Absolute Shrinkage and Selection Operator-cheetah optimization algorithm-extreme gradient boosting (Lasso-COA-XGBoost) carbon price decomposition integration is proposed to address the defects of low prediction accuracy and insufficient model stability of a single machine learning model in the carbon price prediction problem. During the modeling process, the adaptive Lasso method is first employed to select factors from 15 primary indicators of carbon prices, identifying the most important influencing factors. Next, the COA-XGBoost model is built and the parameters of the XGBoost model are optimized using the COA algorithm. Finally, the complete ensemble empirical Mode Decomposition with adaptive noise (CEEMDAM) method is utilized to decompose the residual sequence of the COA-XGBoost model and reconstruct it into high-frequency and low-frequency components. Appropriate frequency models are applied to achieve error correction, thereby constructing the combined Lasso-COA-XGBoost-CEEMDAN model. To further enhance the predictive accuracy and practicality of the model, a rolling time window is introduced for forecasting in the Hubei and Guangzhou carbon emission trading markets, ensuring that the forecasting model can adapt to market changes in real-time. The experimental results show that, taking the carbon price prediction in Hubei as an example, the proposed hybrid model has a significant improvement in prediction accuracy compared with the comparison model (XGBoost model): the RMSE is improved by 99.9987%, the MAE is improved by 99.9039%, the MAPE is improved by 99.9960%, and the R2 is improved by 0.2004%, and the advantages of this hybrid model are also verified in other experiments. The results provide an effective experimental method for future carbon price prediction."
Dynamic prediction of carbon prices based on the multi-frequency combined model,"Basic reporting
no comment

Experimental design
no comment

Validity of the findings
no comment

Additional comments
Authors used alot acroymn first without full meaning ensure that meaning are used before continuing with acryomn e.g COA, XGBoost e.t.c

Authors should review introduction section to include more recent research of Machine learning ehanced with optimization algorithm such as


The image quality require enhancement at least 300dpi

Avoid personal pronouns e.g we or our maintain formal and academic tone. Ensure formal and academic tone in the entire manuscript

Include quantitative description of findings in the abstract

Phrases such as e,g ""As a key player in the global response to climate change"", ""has become a hot topic of research"" do not demonstrate
shcolary articulation correct such sentences

The motivation and contribution is unclear in the introduction

There's no experiment to test COA algorthim before applying to the problem

Discuss the limitation of the study


#


1. The manuscript includes multiple instances where acronyms such as COA and XGBoost are used without first providing their full meanings. The authors must ensure that all acronyms are clearly defined upon first usage before continuing with their abbreviated forms.

2. The introduction should be revised to incorporate more recent research on machine learning enhanced with optimization algorithms.

3. The quality of the images must be improved to a resolution of at least 300 dpi to ensure clarity and compliance with publication standards.

4. The authors should avoid the use of personal pronouns such as ""we"" or ""our."" The entire manuscript should maintain a formal and academic tone.

5. The abstract should include a quantitative description of the findings to provide a clearer and more precise overview of the study's outcomes.

6. Phrases such as ""As a key player in the global response to climate change"" or ""has become a hot topic of research"" lack scholarly articulation. These sentences should be revised to reflect a more academic and professional style.

7. The motivation and contributions of the study are unclear in the introduction. This section should be refined to clearly articulate the novelty and significance of the work.

8. There is no evidence of experiments conducted to validate the COA algorithm before applying it to the problem. The authors should include a dedicated experiment or discussion to justify its application.

9. The manuscript does not address the limitations of the study. A section discussing potential constraints, challenges, or areas for future improvement should be included",Authentic,"As a central participant and important leader in the global climate governance system, China is facing the urgent need to predict and regulate the price of carbon emissions to promote the sound development of its carbon market. In this article, a rolling prediction model based on Least Absolute Shrinkage and Selection Operator-cheetah optimization algorithm-extreme gradient boosting (Lasso-COA-XGBoost) carbon price decomposition integration is proposed to address the defects of low prediction accuracy and insufficient model stability of a single machine learning model in the carbon price prediction problem. During the modeling process, the adaptive Lasso method is first employed to select factors from 15 primary indicators of carbon prices, identifying the most important influencing factors. Next, the COA-XGBoost model is built and the parameters of the XGBoost model are optimized using the COA algorithm. Finally, the complete ensemble empirical Mode Decomposition with adaptive noise (CEEMDAM) method is utilized to decompose the residual sequence of the COA-XGBoost model and reconstruct it into high-frequency and low-frequency components. Appropriate frequency models are applied to achieve error correction, thereby constructing the combined Lasso-COA-XGBoost-CEEMDAN model. To further enhance the predictive accuracy and practicality of the model, a rolling time window is introduced for forecasting in the Hubei and Guangzhou carbon emission trading markets, ensuring that the forecasting model can adapt to market changes in real-time. The experimental results show that, taking the carbon price prediction in Hubei as an example, the proposed hybrid model has a significant improvement in prediction accuracy compared with the comparison model (XGBoost model): the RMSE is improved by 99.9987%, the MAE is improved by 99.9039%, the MAPE is improved by 99.9960%, and the R2 is improved by 0.2004%, and the advantages of this hybrid model are also verified in other experiments. The results provide an effective experimental method for future carbon price prediction."
A genetic programming-based ensemble method for long-term electricity demand forecasting,"Basic reporting
1. The author may strengthen the introduction by providing an extended explanation of the difficulties involved in long-term electricity demand forecasting in developing countries, especially those with growing economic rates of growth and expanding urbanization such as Ethiopia. More development on the specific challenges that Ethiopian is facing would have given stronger reason for the study. Could you give us more instances on how accurate forecasting will assist in planning, formulation of policies and arrangements for energy facilities in Ethiopia?

2. It may also be useful to draw a more detailed comparison of the proposed method with other ensemble learning models employed in the field of electricity demand forecasting. Moreover, a better understanding of the areas within a more comprehensive literature review where previous research has shortcomings concentrating on the energy demand in Ethiopia and how this paper tries to address those shortages would enhance the paper.

3. Some inconsistency in naming conventions is observed, for instance, different naming for equivalent mathematical operations “GA_Quadratic” and “GA_Squared”. Kindly make sure that the same terms are used throughout the manuscript. Some of them contain unclear constructions which could be formulated better: for instance, the note on page 14, line 364, about the GA_Quadratic method.

4. It will be appreciated if each factual statement made in this report is substantiated by the necessary references. For instance, the points mentioned at page 5, line 35, about electrical consumption in the developed countries and at page 6, line 63, on the expansion of the economy of Ethiopia need citation.

5. Give more information about preprocessing if done on the data alongside with missing values and outliers to make the readers understand from where one is starting the analysis.

Experimental design
1. More justification is required for the incorporation of the selected input variables namely population, GDP, import and export. The reader should explain why these particular variables are chosen before the study and argue any limitation of such variables and other possible variable that could be used.

2. The decision on which specific regression models (linear, quadratic, exponential) to use should be more clearly articulated for the first stage of the ensemble method.

3. What is rationale for using the mean forecast values from first stage as inputs in second stage GP model? What can this approach offer as its benefits?

4. The parameters of the GP model used for training are given in the paper, however there is no detail about the criteria of their selection and how their choice may influence the results of the model. Can you specify further how you came up with your choices of the function set, the terminal set and the crossover/mutation rate?

5. To enhance the credibility of the proposed method additional comparisons with other typical ensemble methods like bagging or boosting should be made.

Validity of the findings
1. It is necessary to analyze how sensitive the model is and analyze the relative importance of input variables. Kindly perform sensitivity analysis to enable determine the impact of changes in the input variables (e.g., the rate of increase in population, the GDP changes).

2. The quadratic regression model fared better than the linear and exponential models. Could you please provide more details about the possible causes of such a superior performance? There is a need to know whether the observed result is a wakeup call due to the nature of data collected, the inherent characteristics of the Ethiopian environment, or any other reasons.

3. Although the paper gives the error values, it is necessary to have a deeper discussion about these values. Examine the findings of the error values in each of the stages and identify any pattern or observation drawn from the study. Is the model good on certain year(s) or certain scenario?

4. For additional measures of uncertainty around the forecasts, it is suggested to present the confidence intervals of the predictions.

5. Create a table or figure that compares the forecast results of the proposed ensemble method with those of the models used in the first stage. This will clearly show the benefit of using the ensemble approach in the current hybrid environment.

Additional comments
1. The discussion section should also be lengthened and contain a broader analysis of the implications of the study’s findings to policy makers and energy planners or any other stakeholder involved in the energy sector in Ethiopia. How does it help decision makers and planners to make decision and allocate resources?
2. It is also recommended to mention the possible difficulties when applying the proposed method in a real environment. As you have pointed out, is it all right to use it or are there any questions concerning the data availability or computation problems to limit its applicability?

3. The ideas for further research are well presented, however they can be expanded. For instance while debating on integrating climate variables, it will be appropriate to identify the most suitable climatic parameters that could be included in the model; and how such parameters can best be incorporated into the model.

4. It may be useful to offer a flowchart or a diagram to represent the general approach in order to shed light on the relationships between different steps and methods. This would improve the understanding of the reader on the proposed ensemble method for developing the model.",Authentic,"This study introduces a novel genetic programming-based ensemble method for forecasting long-term electricity consumption in Ethiopia. The technique utilizes a two-stage ensemble approach to project Ethiopia’s electricity consumption through 2031. In the initial stage, genetic algorithms, particle swarm optimization, and simulated annealing methods are applied to various regression models (linear, quadratic, and exponential). The preliminary forecast values generated in this stage were further refined in the second stage. Here, the genetic programming method was utilized to develop a formula based on the initial forecast values, which then provided the final forecast results. The most accurate predictions in the first stage were obtained using the GA_Quadratic, PSO_Quadratic, and SA_Quadratic methods, resulting in mean absolute percentage error (MAPE) values of 3.61, 3.63, and 4.68, respectively. In the second stage, the GP-based prediction achieved an even lower MAPE value of 2.83. Other error metrics, including MSE, root mean square error (RMSE), and R2, were also evaluated, with the proposed model outperforming all methods from the first stage on these metrics. The study projected Ethiopia’s total annual electricity consumption through 2031 under two different scenarios. Both scenarios indicate that by 2031, electricity consumption will have tripled compared to 2021 levels."
A genetic programming-based ensemble method for long-term electricity demand forecasting,"Basic reporting
The manuscript presents a genetic programming-based ensemble method for forecasting Ethiopia's long-term electricity consumption. Overall, the reporting quality is satisfactory with appropriate improvements made based on previous reviewer feedback.

Strengths:

-The manuscript is written in clear, professional English with appropriate technical language.
-The literature review adequately establishes the importance of electricity demand forecasting for Ethiopia and situates the work within existing research.
-The authors have effectively highlighted the knowledge gap in applying optimization methods to Ethiopia's energy forecasting.
-The article structure follows standard scientific presentation with clear sections for methodology, results, and discussion.
-Figures and tables are well-presented, with improvements made as requested by previous reviewers (e.g., consistent y-axis scales in Figure 3).
-The research questions have been added to the introduction as requested.
-The manuscript is self-contained and presents a complete body of work without unnecessary subdivision.

Areas for improvement:

-While many citations are provided, some additional references on recent ensemble forecasting methods could further strengthen the background.
-The authors could more explicitly state which table contains the raw data used in their analysis for better transparency.

Experimental design
The experimental design is sound and has been improved with the addition of statistical tests and synthetic data validation as requested by previous reviewers.

Strengths:

-The research question is clearly defined with two fundamental questions now added to the introduction.
-The methodology is rigorous, using a two-stage ensemble approach with multiple optimization algorithms.
-The authors have added explanations for hyperparameter selection through extensive trial-and-error processes.
-The use of synthetic data to address the small dataset limitation (16 instances) demonstrates good research practice.
-The addition of Wilcoxon signed-rank test provides statistical validation of model performance differences.
-The methods are described with sufficient detail to allow replication by other researchers.

Areas for improvement:

-While the authors now mention testing with synthetic data, more details on the specific Gaussian Copula-based method used to generate this data would be helpful.
-The rationale for specific algorithm selection (GA, PSO, SA, GP) has been addressed, but could be further strengthened with justification from performance benchmarks in similar domains.

Validity of the findings
The manuscript presents valid findings with appropriate limitations now acknowledged.

Strengths:

-The data analysis is sound and the results are clearly presented.
-The authors now explicitly address the small dataset size limitation (16 instances) and its potential impact on generalization ability.
-Test data validation using synthetic data strengthens the robustness of the findings.
-Error metrics (MSE, RMSE, R², MAPE) are appropriate for evaluating the model performance.
-Statistical tests have been added to validate performance differences between models.
-The conclusions are appropriately connected to the original research questions.
-The authors have revised overstated conclusions to better reflect the methodological approach and limitations.

Areas for improvement:

-While the synthetic data testing is a positive addition, the authors could provide more details on how well the model performs on completely new, unseen data patterns.
-The discussion on quadratic models' superior performance could further explore theoretical explanations beyond empirical results.

Additional comments
This manuscript makes a valuable contribution to the field of electricity demand forecasting for Ethiopia. The authors have been responsive to previous reviewer feedback and have substantially improved their work accordingly.

Particularly commendable aspects include:

-The clear separation of Results and Discussion sections as requested by reviewers.
-The addition of statistical tests to validate model performance differences.
-The use of synthetic data to address the small dataset limitation.
-More tempered conclusions that acknowledge the limitations of the study.
-The detailed explanation of hyperparameter selection.

The authors' GP-based ensemble approach demonstrates promising results for improving forecast accuracy. The model's ability to project Ethiopia's electricity consumption up to 2031 under two different scenarios provides valuable insights for energy planning and policy-making.

Future research might benefit from:

-Expanding the dataset with additional historical data or more input variables.
-Exploring other ensemble techniques beyond GP.
-Developing methods to better handle rapid changes in economic factors affecting electricity consumption.

Overall, the manuscript represents a solid contribution to the field and, with the improvements made in response to reviewer feedback, is suitable for publication.",Authentic,"This study introduces a novel genetic programming-based ensemble method for forecasting long-term electricity consumption in Ethiopia. The technique utilizes a two-stage ensemble approach to project Ethiopia’s electricity consumption through 2031. In the initial stage, genetic algorithms, particle swarm optimization, and simulated annealing methods are applied to various regression models (linear, quadratic, and exponential). The preliminary forecast values generated in this stage were further refined in the second stage. Here, the genetic programming method was utilized to develop a formula based on the initial forecast values, which then provided the final forecast results. The most accurate predictions in the first stage were obtained using the GA_Quadratic, PSO_Quadratic, and SA_Quadratic methods, resulting in mean absolute percentage error (MAPE) values of 3.61, 3.63, and 4.68, respectively. In the second stage, the GP-based prediction achieved an even lower MAPE value of 2.83. Other error metrics, including MSE, root mean square error (RMSE), and R2, were also evaluated, with the proposed model outperforming all methods from the first stage on these metrics. The study projected Ethiopia’s total annual electricity consumption through 2031 under two different scenarios. Both scenarios indicate that by 2031, electricity consumption will have tripled compared to 2021 levels."
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Basic reporting
The paper is well written and relatively easy to follow. The quality of English is satisfactory. Figures are of adequate quality and the data set is supplied.

Experimental design
The research question is formulated well. The authors propose a novel model for defect prediction and compare it to some of the existing ones.

Validity of the findings
There are several issues with this paper as it appears that the authors are trying to add a lot of content while not clearly reporting all the necessary details. Firstly, it is not clear how the MEPO and PO differ when it comes to computational efficiency. Some results regarding this should be given. Secondly, BMEPO, the purposed algorithm at the center of the stated contributions, is never clearly defined in the paper. The authors need to clarify all the metrics used during evaluation.",Authentic,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios."
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Basic reporting
• The manuscript is well-structured, with a logical flow from introduction to conclusion. The sections are clearly and appropriately titled.
• The language used is clear and precise, despite minor grammatical errors.
• The literature review provides sufficient context for the problem and appropriately references relevant studies, such as existing feature selection methods and ensemble learning techniques.
• Figures and tables are relevant and well-labeled. The inclusion of diagrams for algorithms and frameworks aids comprehension. However, Figure resolutions should be improved for clarity.

Experimental design
• The objectives of the study are well-defined, with a focus on improving software defect prediction using feature selection via a novel multi-strategy enhanced parrot optimization algorithm (MEPO).
• The methodology is robust, employing standard metrics (AUC) for performance evaluation and multiple comparative baselines (e.g., BPO, BWO, BCPO).
• The dataset preprocessing steps, including SMOTE for addressing class imbalance, are adequately detailed. However, additional explanation on why certain datasets were chosen (e.g., PROMISE repository datasets) would strengthen the rationale.
• The parameters of the algorithm are clearly described, but sensitivity analyses for key parameters like mutation rate or population size are missing.

Validity of the findings
• The findings are well-supported by experimental results, demonstrating the superiority of MEPO in optimization tasks and feature selection for defect prediction.
• Statistical analysis is sufficient; standard deviation and multiple independent runs ensure reliability. However, the manuscript lacks confidence intervals or statistical tests to compare algorithm performance rigorously.
• The heterogeneous data stacking ensemble (HEDSE) framework’s results outperform homogeneous models, providing a strong argument for its use. Yet, an ablation study isolating the effect of HEDSE would strengthen claims.

Additional comments
• The manuscript makes a valuable contribution by integrating a novel optimization algorithm with ensemble learning for a critical problem in software engineering.
• The writing can benefit from minor revisions for fluency and consistency. For example, terms like ""heterogeneous data stacking ensemble"" should be abbreviated consistently throughout.
• The introduction sets the stage well but could more directly highlight the practical implications of defect prediction improvements.
• Future work directions are insightful but would benefit from more specific suggestions, such as real-world deployment challenges or computational cost comparisons.

Specific Suggestions for Improvement:

1. Include a sensitivity analysis for MEPO parameters.
2. Provide statistical tests for performance comparisons.
3. Improve the quality of figures and ensure consistent formatting throughout.
4. Revise certain sentences for grammatical clarity and conciseness.
5. Expand on practical implications and limitations in the discussion.",Authentic,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios."
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Basic reporting
1. A few grammar corrections, spelling error, and acronyms must be looked at carefully throughout the manuscript
2. Equations citations should be done properly

Experimental design
Original primary research within the Aims and Scope of the journal.
The research question is well-defined, relevant, and meaningful. It states how research fills an identified knowledge gap and is well-demonstrated.
In the Results and discussion, details of the implementation environment and overview of the dataset are required

Validity of the findings
All underlying data have been provided; they are robust, statistically sound, & controlled.
Explanations for the dataset should be given
Conclusions are well stated, linked to original research question & limited to supporting results

Additional comments
First and foremost, I would like to appreciate the authors for the good and extensive work in evaluating the performance of their proposed model. The paper was organized well. The following suggestions would help the readers to enhance the readability:

1. The abstract is not clear. For instance, lines 17 to 19 could be rewritten. And what does the phrase” most datasets convey” mean? Furthermore, the abstract should answer the question: What is the significance of your work for researchers in the same field?”
2. In line 29, give valid reference for the Pareto principle
3. In line 38, “defectsSong et al.” leave a space between these words, and check for the same while citing references throughout the manuscript
4. All the acronyms should be abbreviated on their first usage, and the acronym should be referred to on its subsequent usage. Some of the acronyms have been abbreviated multiple times and few weren’t. Eg: line no. 234,238, 240, 241, 304,311 etc..
5. In line 48, change “The paper” to “This work” or “This paper”
6. In the significant contributions presented in the introduction section, there are some repeated statements from the previous paragraph:” Our proposed model has proven to outperform homogeneous data stacking ensemble learning models.” Also, in some places, you have used this model, and in other places, you have used our proposed model, so maintain uniformity in this usage.
7. What is this “BMCPO”?? If this refers to your proposed model, abbreviate it and use it throughout the paper. This would improve the readability
8. In line no.91 “Arar et al.Arar and Ayan (2015)” check this, and same kind of typo errors with reference management software occur at different places (For instance, in line no: 98, 102,104)
9. In line 120, what does “ each individual” refer to? And PAO can be explained simply to enhance readability and understandability
10. While using comma, leave a space after that: line no 124 – 127
11. Some special characters were inserted in line no.129
12. In stacked ensemble learning: line nos: 135 to 137, predictions + input data is fed as input to metamodel (as per explanation), but it is not reflected in Figure 1. Will the meta model in a stacked ensemble take only predictions as features or the input data? Clarify
13. In 3.2, 3.2.2, 3.2.3 capitalise the first letter of the section title
14. Why is the chaotic state parameter set to 0.5??
15. Usually equation is denoted as Eqn. check this in line no. 193
16. Change the 4.1 section title to a Result Analysis of MEPO and Experimental Comparison with other Models. Similarly, change 4.2, 4.3
17. In line no. 249, it is Table 5 I think, change it
18. In table 10. Check the column names, and NGBoost was not abbreviated
19. Why were specific models chosen for stacking, the justification for this is missing
20. In the Results and discussion, details of the implementation environment and overview of the dataset are required",Authentic,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios."
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Basic reporting
The paper presents a novel, well-structured, and practically significant methodology with clear experimental results. It is suitable for publication with minor changes to enhance clarity and comprehensiveness.

Experimental design
No Comment

Validity of the findings
No Comment",Generic,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios."
Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction,"Basic reporting
The paper is well written and easy to follow. It conforms to the journal's standards.

Experimental design
The research is within the aims of the journal.

Validity of the findings
The research findings are valid.

Additional comments
The authors have addressed my concerns and the paper can be accepted",Generic,"Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios."
A rapid method for methanol quantification in spirits using UV-visible spectroscopy and FTIR,"Basic reporting
1. This manuscript should be submitted to a professional research paper writing service to improve its language quality and address any linguistic inaccuracies.
Examples:
I. The term ""ultraviolet-visible spectrometry"" is abbreviated as both ""UV Vis"" and ""UV VIS"" in various sections. To enhance clarity for readers, it is recommended to adhere to ISO standards and maintain a consistent abbreviation throughout the article.
II. There are capitalization errors in the text, such as in line 31 where ""Methanol"" is capitalized in the middle of a sentence. Chemical names should not be capitalized unless they appear at the beginning of a sentence.",Authentic,"Although standards methods of food safety assessment are important, these methods are relatively expensive and require intensive work and time. In alcohol beverage industries, ultraviolet visible (UV-Vis) spectroscopy and Fourier transform infrared (FTIR) spectroscopy are reliable techniques for quality assessment of alcohol, however, testing methods are often varying with calibration techniques and instrument specification. In this work, methanol content in ethanol was assessed in two approaches using UV-Vis with a developed calibration technique and FTIR spectroscopy with a factory default scan function at every 2 nanometer (nm). For UV-Vis method, potassium dichromate was used as the chromogenic reagent, tested with methanol concentration ranging from 0.12% to 1% (mV−1). For FTIR method, spectra data was collected every 2 nm interval and calibration curve was built by increasing methanol ratio from 0% to 40% (mV−1) at the expense of ethanol while keeping deionised (DO) water constant at 5% (mV−1) concentration. This helps gauge the change in methanol concentration relative to ethanol. Results of analysis using UV-Vis showed a strong negative correlation for methanol concentration and absorbance value at UV region from 900 to 1,100 cm−1 (r = 98.00, RMSE = 0.023) relative to increasing ethanol concentration. A strong peak was observed for methanol concentration at spectral region of 975 cm−1 which is related to the methanoic acid C-O bond. The FTIR spectra region at 900 to 1,050 cm−1 was used for observing methanol concentration with absorbance. A strong correlation was established from spectral region of 1,010 to 1,026 cm−1, enabling quantification of methanol (r = 0.99, RMSEC = 0.55). Methanol peak was observed at 1,020 cm−1 region of the spectrum. A set of experimental repetition was made with methanol concentration of 0.02% to 0.5% and 0.1% to 5% for UV-Vis and FTIR, respectively, to determine the limit of detection (LOD) and limit of quantification (LOQ). The observation showed a 0.04% and 0.29% (mV−1) LOD for UV-Vis and FTIR method, respectively. The LOQ was 0.12% and 0.89% (mV−1) for UV-Vis and FTIR respectively. The integration of UV-Vis with potassium dichromate as chromogenic reagent and FTIR spectroscopy with comparatively 50% less data point still present a significant advancement in the test method for safety and quality control of alcohol beverage products. These techniques not only enhance the ability to detect harmful substances but also provide a cost-effective and rapid alternative to traditional methods, making them invaluable tools for distilleries aiming to uphold high standards of quality."
A rapid method for methanol quantification in spirits using UV-visible spectroscopy and FTIR,"Validity of the findings
1. The manuscript titled “A Rapid Method for Methanol Quantification in Spirits Using UV-Visible Spectroscopy and FTIR” (#104989) presents techniques for determining methanol content in spirits through the use of UV-Vis and FTIR spectroscopy. The UV-Visible spectrometry method described involves the use of potassium dichromate in acidic conditions to oxidize alcohols, leading to the reduction of chromium from the Cr (VI) (yellow color) oxidation state to Cr (III) (green color). It is worth noting that this method has been widely recognized since the 1990s. (Hari, M.; Deoki, N. Spectrophotometric Determination of Some Monohydric Alcohols Based on Their Oxidation by K2CrO4-HNO3 System. Indian J. Chem 1994, 33, 359–361. ; Tgd, S.; Rsrd, S.; Fgd, C.; Dda, S. ATR-FTIR and UV-Vis as Techniques for Methanol Analysis in Biodiesel-Washing Wastewater. QuÌmica Nova 2023, 46, 698–705.). Previously published methods have reported alcohols (ethanol and methanol) at a wavenumber of 600 cm⁻¹. The current manuscript provides a clarification regarding the correlation of methanol at 970 cm⁻¹, which is considered one of the novel aspects of this study. To enhance clarity, it would be beneficial to highlight this correlation by zooming in on the 970 cm⁻¹ region in Figure 2.

2. Another commonly reported method, FTIR, is widely recognized for its effectiveness in quantifying ethanol and methanol content in various samples (Pérez-Ponce, A.; de la Guardia, M. Partial Least-Squares–Fourier Transform Infrared Spectrometric Determination of Methanol and Ethanol by Vapour-Phase Generation. Analyst 1998, 123, 1253–1258. ; Coldea, T. E.; Socaciu, C.; Fetea, F.; Ranga, F.; Pop, R. M.; Florea, M. Rapid Quantitative Analysis of Ethanol and Prediction of Methanol Content in Traditional Fruit Brandies from Romania, Using FTIR Spectroscopy and Chemometrics. Not. Bot. Horti Agrobot. Cluj Napoca 2013, 41, 143.). Could you kindly compare the previously established FTIR methods for ethanol and methanol quantification with the current method you have developed, and emphasize the significant improvements? It is important to clearly demonstrate the advancements made, as presenting a method without novelty or improvement may not add substantial value to the literature.",Authentic,"Although standards methods of food safety assessment are important, these methods are relatively expensive and require intensive work and time. In alcohol beverage industries, ultraviolet visible (UV-Vis) spectroscopy and Fourier transform infrared (FTIR) spectroscopy are reliable techniques for quality assessment of alcohol, however, testing methods are often varying with calibration techniques and instrument specification. In this work, methanol content in ethanol was assessed in two approaches using UV-Vis with a developed calibration technique and FTIR spectroscopy with a factory default scan function at every 2 nanometer (nm). For UV-Vis method, potassium dichromate was used as the chromogenic reagent, tested with methanol concentration ranging from 0.12% to 1% (mV−1). For FTIR method, spectra data was collected every 2 nm interval and calibration curve was built by increasing methanol ratio from 0% to 40% (mV−1) at the expense of ethanol while keeping deionised (DO) water constant at 5% (mV−1) concentration. This helps gauge the change in methanol concentration relative to ethanol. Results of analysis using UV-Vis showed a strong negative correlation for methanol concentration and absorbance value at UV region from 900 to 1,100 cm−1 (r = 98.00, RMSE = 0.023) relative to increasing ethanol concentration. A strong peak was observed for methanol concentration at spectral region of 975 cm−1 which is related to the methanoic acid C-O bond. The FTIR spectra region at 900 to 1,050 cm−1 was used for observing methanol concentration with absorbance. A strong correlation was established from spectral region of 1,010 to 1,026 cm−1, enabling quantification of methanol (r = 0.99, RMSEC = 0.55). Methanol peak was observed at 1,020 cm−1 region of the spectrum. A set of experimental repetition was made with methanol concentration of 0.02% to 0.5% and 0.1% to 5% for UV-Vis and FTIR, respectively, to determine the limit of detection (LOD) and limit of quantification (LOQ). The observation showed a 0.04% and 0.29% (mV−1) LOD for UV-Vis and FTIR method, respectively. The LOQ was 0.12% and 0.89% (mV−1) for UV-Vis and FTIR respectively. The integration of UV-Vis with potassium dichromate as chromogenic reagent and FTIR spectroscopy with comparatively 50% less data point still present a significant advancement in the test method for safety and quality control of alcohol beverage products. These techniques not only enhance the ability to detect harmful substances but also provide a cost-effective and rapid alternative to traditional methods, making them invaluable tools for distilleries aiming to uphold high standards of quality."
A rapid method for methanol quantification in spirits using UV-visible spectroscopy and FTIR,"Basic reporting
Clear and unambiguous, professional English used throughout.

Literature references, sufficient field background/context provided.

Professional article structure, figures, tables. Raw data shared.

Self-contained with relevant results to hypotheses.

Experimental design
Original primary research within Aims and Scope of the journal.

Research question well defined, relevant & meaningful. It is stated how research fills an identified knowledge gap.

Rigorous investigation performed to a high technical & ethical standard.

Methods described with sufficient detail & information to replicate.

Validity of the findings
Meaningful replication encouraged where rationale & benefit to literature is clearly stated.

All underlying data have been provided; they are robust, statistically sound, & controlled.

Conclusions are well stated, linked to original research question & limited to supporting results.

Additional comments
The manuscript has been revised appropriately. The revised manuscript is acceptable.",Generic,"Although standards methods of food safety assessment are important, these methods are relatively expensive and require intensive work and time. In alcohol beverage industries, ultraviolet visible (UV-Vis) spectroscopy and Fourier transform infrared (FTIR) spectroscopy are reliable techniques for quality assessment of alcohol, however, testing methods are often varying with calibration techniques and instrument specification. In this work, methanol content in ethanol was assessed in two approaches using UV-Vis with a developed calibration technique and FTIR spectroscopy with a factory default scan function at every 2 nanometer (nm). For UV-Vis method, potassium dichromate was used as the chromogenic reagent, tested with methanol concentration ranging from 0.12% to 1% (mV−1). For FTIR method, spectra data was collected every 2 nm interval and calibration curve was built by increasing methanol ratio from 0% to 40% (mV−1) at the expense of ethanol while keeping deionised (DO) water constant at 5% (mV−1) concentration. This helps gauge the change in methanol concentration relative to ethanol. Results of analysis using UV-Vis showed a strong negative correlation for methanol concentration and absorbance value at UV region from 900 to 1,100 cm−1 (r = 98.00, RMSE = 0.023) relative to increasing ethanol concentration. A strong peak was observed for methanol concentration at spectral region of 975 cm−1 which is related to the methanoic acid C-O bond. The FTIR spectra region at 900 to 1,050 cm−1 was used for observing methanol concentration with absorbance. A strong correlation was established from spectral region of 1,010 to 1,026 cm−1, enabling quantification of methanol (r = 0.99, RMSEC = 0.55). Methanol peak was observed at 1,020 cm−1 region of the spectrum. A set of experimental repetition was made with methanol concentration of 0.02% to 0.5% and 0.1% to 5% for UV-Vis and FTIR, respectively, to determine the limit of detection (LOD) and limit of quantification (LOQ). The observation showed a 0.04% and 0.29% (mV−1) LOD for UV-Vis and FTIR method, respectively. The LOQ was 0.12% and 0.89% (mV−1) for UV-Vis and FTIR respectively. The integration of UV-Vis with potassium dichromate as chromogenic reagent and FTIR spectroscopy with comparatively 50% less data point still present a significant advancement in the test method for safety and quality control of alcohol beverage products. These techniques not only enhance the ability to detect harmful substances but also provide a cost-effective and rapid alternative to traditional methods, making them invaluable tools for distilleries aiming to uphold high standards of quality."
Authentication of Jarrah (Eucalyptus marginata) honey through its nectar signature and assessment of its typical physicochemical characteristics,"Basic reporting
English is ok.
Literature is sufficiently sampled.
The structure of the article is ok.

Experimental design
The experimental design is not complete.
- A first question is concerning the evaluation of authentic Jarrah honey and nectar from other geographical origins (whenever possible). How the authors evaluated this point?
- Additionally, did the authors evaluated mixtures of honey from WA and Jarrah to evaluate the LOD of the approach for detecting mixtures/dilutions practices? Eventually other non-WA honeys.
- In the present work the authors evaluated different HPTLC bands attributed to co-flowering, therefore blended samples. How is the allowed limit of blends for a pure Jarrah honey?
- Which are the values of reproducibility and accuracy of the rf values of bands for the proposed HPTLC approach?
- How many replicates have been considered in this study?

Validity of the findings
The novelty is not present.
The approach includes an HPTLC analysis combined with physicochemical characterizations. The authors also stated that this authentication approach allows, for the first time, the characterization of physicochemical characteristics of this honey. The present manuscript is an extension of a previous work from the same group, in which the same approach was applied to a restricted samples set of Jarrah honey (https://doi.org/10.1016/j.crfs.2022.02.014), while 31 were considered here.

Additional comments
The proposed approach resulted fruitful in determining compositional differences, typically in botanical assessments, or in the profiling of polyphenols compositions etc.. In fact the authors already published compositional differences between Manuka and Jarrah honeys in their recent work (https://doi.org/10.7717/peerj.12186). The main result of these investigations is concerning a qualitative evaluation of HPTLC bands, whereas the quantitative evaluation (e.g. saccharides content) could be achieved by other methods. The authentication process needs more accurate determinations.",Authentic,"Jarrah (Eucalyptus marginata) is a dominant forest tree endemic to the southwest of Western Australia. Its honey is appreciated for its highly desirable taste, golden colour, slow crystallisation, and high levels of bioactivity, which have placed Jarrah in the premium product range. However, whilst customers are willing to pay a high price for this natural product, there is currently no standard method for its authentication. As honey is naturally sourced from flower nectar, a novel route of authentication is to identify the nectar signature within the honey. This study reports on a high-performance thin layer chromatography (HPTLC)-based authentication system which allows the tracing of six key marker compounds present in Jarrah flower nectar and Jarrah honey. Four of these markers have been confirmed to be epigallocatechin, lumichrome, taxifolin and o-anisic acid with two (Rf 0.22 and 0.41) still chemically unidentified. To assist with the characterisation of Jarrah honey, a range of physicochemical tests following Codex Alimentarius guidelines were carried out. A blend of authenticated Jarrah honey samples was used to define the properties of this honey type. The blend was found to have a pH of 4.95, an electric conductivity of 1.31 mS/cm and a moisture content of 16.8%. Its water-insoluble content was 0.04%, its free acidity 19 milli-equivalents acid/kg and its diastase content 13.2 (DN). It also contains fructose (42.5%), glucose (20.8%), maltose (1.9%) and sucrose (<0.5%). The HPTLC-based authentication system proposed in this study has been demonstrated to be a useful tool for identifying Jarrah honey and might also act as a template for the authentication of other honey types."
Baltic Sea shipwrecks as a source of hazardous pollution,"Dear Authors,
Reviewers have evaluated your manuscript. it is recommended that you revise the manuscript to polish the draft to a standard that is acceptable in a journal with international audience. In addition to this, please consider the following additional EDITOR comments:

1. The abstract needs to be carefully revised following the journal guidelines found in the template at https://peerj.com/about/author-instructions/chemistry. Specifically, you need to properly introduce the study background (problem), present the methods used and the key findings (results).

2. Some of the literature cited are too old. For example, Andrulewicz et al. (1994, 1998), (HELCOM, 1974), and (Jeffery, 1990). Although some of these may be important in this study, such literature rarely present the current status of knowledge and should only be cited in exceptional cases. There is very recent relevant literature in this area.

https://doi.org/10.3390/w14223772
https://doi.org/10.1016/j.marpolbul.2021.112747
https://doi.org/10.1016/j.marenvres.2020.105036
https://doi.org/10.1016/j.marpolbul.2022.114426

3. The introduction needs to be rewritten altogether so that it does not merely present the history of the ship wreckages that occurred in the study area. This section should introduce the potential of ship wrecks as sources of pollution (see for example, https://doi.org/10.3390/jmse11020276), which should better position your study and justify why you need to report on the study area at this point in time.

4. Thoroughly check the manuscript for typos and grammar. For example, in L157, ''or'' should be ‘‘for’’.

5. Methodology (L156-178) needs to be elaborated. The analytical conditions used should be clearly indicated, including any quality control and quality assurance activities undertaken. See https://doi.org/10.1007%2Fs40201-019-00356-z for a similar reporting.

6. Your study does not have any dedicated section on how the data were analyzed or visualized. I believe that your study could benefit from health risk assessment, which should give a better view as to whether the current pollution levels pose any significant health threats to the local pollution.

7. In Table 3, it is not possible that Pazdro (2004) did their study in 2022. The ‘‘n.d.’’ in this Table should be replaced with the LOD or LOQ. For easy comparison, put the results of your study first, followed by previous studies.

**PeerJ Staff Note:** It is PeerJ policy that additional references suggested during the peer-review process should only be included if the authors agree that they are relevant and useful.

**PeerJ Staff Note:** Please ensure that all review, editorial, and staff comments are addressed in a response letter and that any edits or clarifications mentioned in the letter are also inserted into the revised manuscript where appropriate.",Authentic,"Shipwrecks on the Baltic Sea seabed pose a serious threat to the marine environment. Fuel, ammunition and chemicals in their holds can enter the ecosystem at any time, causing an ecological disaster. It is known that oil spills from ship accidents can affect life and health of different species of animals, both immediately after catastrophe and for many years thereafter. This article discusses the negative impact of shipwrecks on the ecological status of the Baltic Sea and presents the contamination status of bottom sediment core samples taken in the vicinity of shipwrecks located in the South Baltic, i.e., S/s Stuttgart, t/s Franken, S/T Burgmeister Petersen and m/s Sleipner. It is based on the results of research carried out by the Maritime Institute between 2011 and 2016."
Effect of acidity/alkalinity of deep eutectic solvents on the extraction profiles of phenolics and biomolecules in defatted rice bran extract,"Basic reporting
The manuscript presents a thorough investigation into the effect of acidity and alkalinity in deep eutectic solvents (DES) on the extraction of phenolics and other biomolecules from defatted rice bran (DFRB), a by-product of rice bran oil production. The text is well-structured and unveils intriguing findings regarding the utilization of DES for extracting phenolics and other biomolecules from DFRB. However, certain aspects of the study require further clarification and enhancement to elevate the manuscript's quality for publication.

A thorough proofreading is advised to enhance the manuscript’s quality. For example, Section 2.6.2 is missing a subtitle, and in Line 301, 'NR' should be subscripted as 'ENR'.

Experimental design
The rationale behind choosing the four DES (ChCl-lactic acid, K2CO3-glycerol, ChCl-glycerol, and ChCl-urea) needs to be explicitly delineated. Are there other DES that could have been considered? A comparative analysis regarding the cost, availability, environmental impact, and biocompatibility of these DES compared to conventional solvents would provide a more comprehensive understanding.

More granular information on the analysis of the phenolic extracts is desired. It is advisable to include data on the replicates for each extraction experiment and to report the standard deviation or standard error of the mean for all data exhibited in tables and figures.

Validity of the findings
A more in-depth exploration of the extraction mechanism of phenolics and other biomolecules by the DES is necessary. The impact of the pH environment of the DES on the solubility and stability of the extracted compounds should be discussed. Additionally, elucidating the interactions between the DES and DFRB components, along with the effect of extraction time and temperature on the yield and quality, would be beneficial.

A discussion on the molecular interactions that might account for the differential extraction efficiency observed would be insightful. This aspect could significantly enrich the manuscript.

Investigating and discussing the effect of DES composition on the stability and functionality of the extracted biomolecules, particularly focusing on proteins and phenolics, would be worthwhile.",Authentic,"This study investigated the influence of deep eutectic solvent (DES) acidity/alkalinity on the extraction profiles of phenolics and other biomolecules (phytic acid, reducing sugar, and protein) in defatted rice bran (DFRB). The DES with varying pH levels were prepared using different hydrogen bond acceptors (choline chloride (ChCl) and potassium carbonate (K2CO3)) and hydrogen bond donors (lactic acid, urea, and glycerol). The results reveal that the acidic DES (ChCl-lactic acid; pH 0.42) demonstrated superior extraction efficiency for total phenolic acids (4.33 mg/g), phytic acid (50.30 mg/g), and reducing sugar (57.05 mg/g) while having the lowest protein content (5.96 mg/g). The alkaline DES (K2CO3-glycerol; pH 11.21) showed the highest levels of total phenolic acid (5.49 mg/g) and protein content (12.81 mg/g), with lower quantities of phytic acid (1.04 mg/g) and reducing sugar (2.28 mg/g). The weakly acidic DES (ChCl-glycerol; pH 4.72) exhibited predominantly total phenolics (3.46 mg/g) with lower content of protein (6.22 mg/g), reducing sugar (1.68 mg/g) and phytic acid (0.20 mg/g). The weakly alkaline DES (ChCl-urea; pH 8.41) resulted in lower extraction yields for total phenolics (2.81 mg/g), protein (7.45 mg/g), phytic acid (0.10 mg/g), and reducing sugar (7.36 mg/g). The study also explored the distribution of phenolics among various DESs, with the alkaline DES (K2CO3-glycerol) containing the highest concentration of free phenolics. Notably, ChCl-based DESs predominantly contained soluble esterified bound phenolics and soluble glycosylated bound phenolics. Furthermore, a significant correlation between antioxidant activities and phenolic contents was observed. In conclusion, this study has revealed that the acidity and alkalinity of a DES significantly impact the extraction of phenolics and other value-added biomolecules in DFRB. These findings highlight the potential for manipulating the properties of DESs through pH variation, making them versatile solvents for extracting and isolating valuable compounds from agricultural by-products like DFRB and offering opportunities for sustainable utilization and value addition in various industries."
"Evaluating soil salinity dynamics under drip irrigation in the Manas River Basin, Xinjiang: a long-term analysis (1996–2019)","Basic reporting
Overall paper is written in clear and in professional English language with the provision of sufficient background knowledge, enough literature review, good illustrated figures and tables etc. However some suggestions are given in separate file to be considered.

Experimental design
Study experiments are well designed however gaps can be filled considering the minor suggestion attached file.

Validity of the findings
This study is based on older methods with the lake of novelty but still have sound knowledge.

Additional comments
This paper offers a valuable contribution to soil salinity research through its integration of advanced techniques and long-term data. However, addressing the methodological gaps, improving clarity and organization, and strengthening actionable recommendations would significantly enhance its impact and utility for both scientific and practical purposes.",Generic,"The Manas River Basin, located in Xinjiang, China, is one of the province’s four major agricultural irrigation regions and the first in the country to implement large-scale drip irrigation. While drip irrigation has enhanced water use efficiency, it has also contributed to soil salinization, negatively impacting crop yields and soil health. This study examines the spatial and temporal evolution of soil salinity in the oasis area of the basin from 1996 to 2019. The study evaluates salinization dynamics under long-term irrigation practices using soil salinity inversion models, regression analysis, water-salt balance calculations, geostatistical techniques, and ArcGIS. The results reveal significant improvements in soil salinity conditions, with 78.02% of the region experiencing reduced salinity and 10.09% exhibiting deterioration. From 1996 to 2019, non-salinized soil increased by 1,403.46 km2, mildly salinized soil expanded by 3,702.28 km2, while saline soils decreased by 7,685.6 km2. Statistical analysis indicates that soil salinity followed normal or logarithmic-normal distributions, with higher variability observed in 2016 and 2019. Despite these positive trends, challenges remain, particularly in the Shihezi, Manas, and Mosuowan irrigation zones, which still exhibit moderate to severe salinity. This study highlights the effectiveness of drip irrigation combined with improved management practices in mitigating soil salinity and enhancing soil quality. However, it emphasizes the need for targeted strategies to address residual salinization risks, ensuring sustainable agricultural development and ecological balance in arid regions."
"Comprehensive analysis of groundwater hydrochemistry and nitrate health risks in the Baiquan basin, Northern China","Basic reporting
The manuscript is generally clear and well-organized. However, some sections could benefit from more concise language to improve readability. Somes suggestions have been made in the document.

The study is well-contextualized within the existing literature, with appropriate references to previous research. Figures and tables are well-designed but could benefit from informative captions. The references are relevant and up to date, supporting the study’s background and rationale effectively.",Authentic,"Groundwater is a crucial water source and strategic resource, essential for sustaining both urban and rural livelihoods, supporting economic and social development, and maintaining ecological balance. This study investigates the hydrochemical properties and controlling factors of groundwater in the Baiquan basin (BQB) by analyzing water quality data collected during both dry and wet periods. Additionally, the suitability of groundwater for drinking and agricultural irrigation was evaluated. The findings reveal that groundwater in BQB is generally weakly alkaline and primarily consists of hard-fresh water. Although there are seasonal variations in the main ion concentrations, HCO3− and Ca2+ are the predominant anions and cations, respectively. Consequently, the hydrochemical type is mainly HCO3-Ca⋅Mg type, with a secondary classification of SO4⋅Cl-Ca ⋅ Mg. The hydrochemical composition is primarily influenced by the dissolution of carbonate and silicate minerals, as well as cation exchange processes. Additionally, it is affected by anthropogenic inputs, particularly from the use of agricultural fertilizers. The water quality assessment results indicated that all water samples are classified as either good or moderate, with a significant majority falling into the good category. Additionally, the northern section of the BQB exhibited lower entropy weight water quality index (EWQI) values during the dry season in comparison to the wet season. For irrigated agriculture, groundwater in the BQB serves as a high-quality water source for irrigation throughout both the dry and rainy seasons. Furthermore, non-carcinogenic risks are notably concentrated in the north-western and south-eastern regions of the study area. Health risks associated with nitrates in groundwater are elevated during the rainy season. Notably, non-carcinogenic risks for infants were significantly high across both seasons and substantially exceeded those for children and adults. These results provide valuable scientific insights for the management and development of groundwater resources in the BQB."
"Nutrient cycling characteristics along a chronosequence of forest primary succession in the Hailuogou Glacier retreat area, eastern Tibetan Plateau","Basic reporting
Some sections, particularly in the ""Materials & Methods"" and ""Results"" sections, contain lengthy, complex sentences that can be difficult to follow. Simplify and break down long sentences into shorter, more concise statements. Ensure each paragraph focuses on a single idea. While the introduction provides some context, it lacks a clear statement of the research gap and the significance of the study. It should better articulate the importance of the study and how it contributes to the field of nutrient cycling and forest succession. Some methodological details are insufficiently described, such as the specific techniques used for sample collection and analysis. Figures and tables are relevant but need clearer labeling and more descriptive captions. All visual data should be easily interpretable without referring back to the text. The literature cited is relevant and generally well-integrated into the manuscript. However, there could be a more comprehensive comparison of the study's findings with existing literature, especially in the discussion section.

Experimental design
While the methods are generally well-described, some aspects, such as the specific statistical methods used for data analysis, are not detailed enough. This lack of detail can hinder the replication of the study. Provide more comprehensive descriptions of the statistical analyses, including the software used, specific tests applied, and criteria for significance. The study does not include longitudinal data collected over multiple years, which could provide deeper insights into temporal variations in nutrient cycling. The methods section provides detailed descriptions of sample collection and analysis techniques for soil and vegetation. This includes specific information on the tools and procedures used, enhancing the reproducibility of the study. The discussion section provides a broad overview of the findings but lacks depth in interpreting the implications of the results. The discussion could be more thorough in explaining the significance of the findings in relation to existing literature. Deepen the analysis by comparing and contrasting the study’s results with previous research. Discuss the broader ecological implications and potential mechanisms underlying the observed nutrient cycling patterns.

Validity of the findings
The paper does not explicitly assess the impact and novelty of its findings within the broader context of ecological research. While the study presents original data and insights into nutrient cycling during forest succession in a glacial retreat area, it could benefit from a more explicit discussion of how these findings advance current knowledge and their potential implications for future research. you can include a dedicated section that explicitly discusses the impact and novelty of the study’s findings. Highlight how the research contributes new insights to the field and its potential implications for ecological theory and practice. Provide more detailed information on the statistical analyses performed, including the software used, specific tests applied, and criteria for significance. This transparency will enhance the credibility and reproducibility of the findings.

Additional comments
Overall, the paper presents a valuable and original contribution to the understanding of nutrient cycling in forest primary succession within a glacial retreat area. The study is well within the aims and scope of the journal, with a clear and well-defined research question. the discussion does not consider alternative explanations or potential limitations of the study, such as sampling biases or the constraints of the chrono sequence approach. There is also a tendency to overgeneralize the findings, which may not be fully supported by the data, especially concerning their applicability to other glacial retreat areas. The paper would benefit from a more detailed discussion of methodological limitations and a more cautious approach to generalizing results. Furthermore, the absence of explicit future research directions weakens the potential for guiding subsequent studies. The statistical analysis section needs more detailed explanations of the methods used and their appropriateness, enhancing the transparency and reproducibility of the findings. Addressing these weaknesses through a more thorough and critical discussion, better integration with existing research, and detailed methodological transparency would significantly improve the paper's quality and contribution to the field.",Authentic,"The Hailuogou Glacier has been continuously retreating since the end of the Little Ice Age, resulting in a 125-year soil chronosequence and a complete primary forest succession sequence. Nutrient cycling and utilization are the foundation to forest succession processes and dynamic changes, directly influencing the structure and stability of ecosystems. However, our understandings on the characteristics of ecosystem nutrient accumulation and recycling during succession, especially in the context of primary succession within glacier retreat areas, remain limited. To address this, we investigated nutrient characteristics across six forest primary succession sites in the Hailuogou Glacier retreat area."
Optimization and action mechanism of pollutant removal performance of unsaturated vertical flow constructed wetland (UVFCW) driven by substained-release carbon source,"Basic reporting
The article is very well written and with impactful research contributions in the field of COD, TN, and TP removal from wastewater. The use of UVFCW delivers excellent performance. However, the authors need to address few queries before the article can be accepted.
The authors need to justify the use of UVFCW? What do they mean by unsaturated and how it helps in creating the DO gradient needs to be justified.
The use of sustained release of carbon source needs to be justified.
Also, in the article it is written substained. Why is it so? It should be sustained right?

Experimental design
How is the influent concentration for COD, TP, ammonia are constant?
DO, ORP, and pH of influent and effluent and inside the reactor needs to be reported to establish nitrogen removal.
An actual figure of the reactor is required for the readers to better understand the configuration.
From Fig. 1 it is not clear how the carbon source is below or above the device? What do you mean by device?

Validity of the findings
No comment

Additional comments
No comment",Authentic,"Constructed wetland (CW) technology has attracted much attention due to its economical and environmentally friendly features. The low dissolved oxygen (DO) and low carbon/nitrogen (C/N) ratio in the wetland influent water affect the treatment performance of CW, resulting in a decrease in the removal efficiency of ammonia nitrogen (NH4+-N) and nitrate nitrogen (NO3−-N). In order to address this problem, this study optimized the pollutants removal performance of unsaturated vertical flow constructed wetland (UVFCW) by adding sustained-release carbon sources (corn cobs + polybutylene adipate terephthalate (PBAT)). The results showed that the sustained-release of carbon source increased the carbon source in UVFCW, thus increasing the abundance and activity of denitrifying microorganisms and enhancing the denitrification reaction, ultimately improving the removal of NO3−-N, with its removal efficiency reaching up to 95.50%. The placement method of sustained-release carbon source mainly affected the distribution of carbon source and DO in water body, thus influencing the relative abundance of microorganisms, finally affecting the removal of pollutants. Among them, the removal efficiency of total nitrogen (TN), NO3−-N, and total phosphorus (TP), and the relative abundance of denitrifying microorganisms in the CWR-Cu (uniform placement of sustained-release carbon source) were significantly higher than those in the CWR-Ca (centralized placement above) and CWR-Cb (centralized placement below) (p < 0.05). The surface C:O (carbon:oxygen) ratio of sustained-release carbon source after water treatment showed a decreasing trend, and CWR-Cu exhibited the greatest decrease in C:O ratio. In summary, CWR-Cu achieved the highest utilization of the carbon source and produced the largest number of heterotrophic microorganisms. This study reveals that CWR-Cu is a structural process for the efficient removal of nitrogen and phosphorus pollutants, and our findings provide theoretical basis and technical support for actual projects."
Time series (ARIMA) as a tool to predict the temperature-humidity index in the dairy region of the northern desert of Mexico,"Basic reporting
The study is of general importance for livestock production, health and welfare. The study examines the predictive capacity of the THI in relation to potential heat stress by using the time series method (ARIMA MODEL). They have predicted THI using four previous days climatic data of previous four years. Accuracy of model prediction for THI has not discussed in detail ?. Results and discussion part is weak in the manuscript. Predictive capacity of model has to be discussed in detail.

Experimental design
Experimental design is okay

Validity of the findings
Validity of the findings is not explained properly.
Furthermore, the data demonstrates an annual increase in the number of days the THI indicates a risk of heat stress. This line is written in abstract, but not discussed in the results and discussion part.

Clearly state what is the gap, hypothesis of study?

Accuracy of model prediction for THI has not discussed in detail ?. Predictive capacity of model with results of model has to be discussed in detail.

Discussion part is not written well. Some points in the Paper has been written very technically, difficult for the readers to comprehend the interpretation.

Generally, it is a very well established fact that Increase in THI can be stressful for the animals. Repeatedly these points in the paper have been written. While the specific ARIMA model utility and its accuracy has not been written at all.

Additional comments
The paper is recommended for major revision.
Many mistakes in reference writing through out the text.",Authentic,"The environment in which an animal is situated can have a profound impact on its health, welfare, and productivity. This phenomenon is particularly evident in the case of dairy cattle, then, in order to quantify the impact of ambient temperature (°C) and the relative humidity (%) on dairy cattle, the temperature-humidity index (THI) is employed as a metric. This indicator enables the practical estimation of the stress imposed on cattle by ambient temperature and humidity. A seasonal autoregressive integrated moving average (SARIMA) (4,1,0)(0,1,0)365 model was estimated using daily data from the maximum daily THI of 4 years (2016–2019) of the Comarca Lagunera, an arid region of central-northern Mexico. The resulting model indicated that the THI of any given day in the area can be estimated based on the THI values of the previous four days. Furthermore, the data demonstrate an annual increase in the number of days the THI indicates a risk of heat stress. It is essential to continue building predictive models to develop effective strategies to mitigate the adverse effects of heat stress in dairy cattle (and other species) in the region."
Turbidivision: a machine vision application for estimating turbidity from underwater images,"Basic reporting
In this article, the authors present a model they developed to estimate turbidity from underwater images. Their findings are robust, with an extensive set of data, good model performance, and well-explained methods. This paper's content is relevant to the field of water monitoring. However, there is substantial room for improvement in the paper's form. Many sentences are ambiguous, redundant, and do not follow scientific writing standards. Therefore, I do not recommend publishing the article as it is, but I encourage the authors to carefully revisit the form, as I believe that their findings are very promising. In this review, I used the structure provided by the PeerJ editors, and used a red/orange/green colors to evaluate each point. Each evaluation is followed by a detailed explanation.

RED: The article must be written in English and must use clear, unambiguous, technically correct text. The article must conform to professional standards of courtesy and expression.

Explanation: see comments in the document. I primarily concentrated on the abstract and the introduction to provide detailed comments, but the criticism is applicable to the whole document.

ORANGE: The article should include sufficient introduction and background to demonstrate how the work fits into the broader field of knowledge. Relevant prior literature should be appropriately referenced.

Explanation: The authors defined in the first paragraph of the introduction the relevance of turbidity in water quality assessment. However, despite using in the article statements such as line 47 “High turbidity can have a negative impact on aquatic life” or line 53 “overly turbid water is unsuitable for consumption by humans”, they did not provide any information on what is considered a high/normal/low turbidity value in the context of natural waters and drinking waters. This is a problem because the reader is not able to assess whether the measurement range (0-55NTU) is relevant in this context.

GREEN: The structure of the article should conform to an acceptable format of ‘standard sections’ (see our Instructions for Authors for our suggested format). Significant departures in structure should be made only if they significantly improve clarity or conform to a discipline-specific custom.

Comment: the article is brief, well-structured and information are presented where the reader expect them.

ORANGE: The submission should be ‘self-contained,’ should represent an appropriate ‘unit of publication’, and should include all results relevant to the hypothesis.

Explanation: The authors used the argument that historical dataset of underwater images could be analyzed with their method (line 75: “historical data from underwater images often lacks […]” or line 96 “gaining insights into historical data”). I agree that this is a very strong argument for the relevance of their model. Yet they did not provide any evidence that such data are available. I suggest citing existing dataset, and even (if possible) analyzing some of historical data with their algorithm, which would with little effort greatly improve the strength of the article.

Experimental design
The submission should clearly define the research question, which must be relevant and meaningful. GREEN: The knowledge gap being investigated should be identified, and statements should be made as to how the study contributes to filling that gap.

Comment: the introduction was well structured and highlights the importance of turbidity for water monitoring, as well as the limitations of currently available techniques for turbidity measurement, leading to the research gaps.

ORANGE: The investigation must have been conducted rigorously and to a high technical standard. The research must have been conducted in conformity with the prevailing ethical standards in the field.

Explanation:
There is, at difference instance, a lack of justification to explain why the authors choose certain methods over others:
• Line 122: “For 38% of all photos, we included the 4.2cm Secchi”. Why did you do that? What is the impact of the Secchi disk on the model performance? Did you try to train the model with images without Secchi disk? (This is relevant because for future use of the model most users will not have access to a Secchi disk).
• Line 148: “We used Ultralytics YOLOv8 classification”. Why did you selected this method? Did you try other approaches? Why do you believe that this method is suited for this task?
• Line 136: “the images were broken into 11 groups”. Why eleven? Why not 5? Did you optimite this number, or is it for practical reasons?

ORANGE: Methods should be described with sufficient information to be reproducible by another investigator.

Explanation: In general, the description about the data collection lacks information, which hinders reproducibility:
• Line 106: “whether the water was flowing”. Under which criteria was the water flowing? Did you use a flowmeter? Did you use this information in the analysis?
• What about the illumination? Did you use sunlight? Did you use the camera flash? Did you collect information about it?
• Line 115: “Images were collected by taking individual photographs […] and by saving a selection of frames from a video”. Why to acquisitions methods ? How many images are taken from each mode?

Validity of the findings
ORANGE: The data on which the conclusions are based must be provided or made available in an acceptable discipline-specific repository. The data should be robust, statistically sound, and controlled.

Explanation: I am concerned about the validity of the findings because some information about the dataset are missing. The authors used images takes from different sources, which is improving the range of application of their model. My concern is: is it possible that the model is not actually predicting turbidity, but the origin of the image?
Here is a possible scenario: all samples from rivers have a turbidity between 0 and 5 NTU, all samples from the lab experiment have a turbidity between 10 and 15 NTU, etc. In that scenario, the classification model could be predicting the origin of the image, which is itself correlated with turbidity. The problem would be that if one provide a new image to the model, it will not be able to predict turbidity accurately.
Solution: 1) provide information, for each sub-parts of the dataset, about the range of turbidity. What about, in Figure X_REF, using a color-code for each dot representing the origin of the sample? 2) provide information about the train-validate-test splitting of the data, regarding whether images from each source is present in each one of them. 3) Even better: keep a whole section of the dataset, for instance river images, from the training and validating set. If the model still performs well on this totally new images, it means it learned how to estimate turbidity from any images. This is important because future users of your app need to verify that the model will still perform if they take a picture in a totally different context.

ORANGE: The conclusions should be appropriately stated, should be connected to the original question investigated, and should be limited to those supported by the results. In particular, claims of a causative relationship should be supported by a well-controlled experimental intervention. Correlation is not causation.

Comment: see comment above.

Additional comments
I have two additional remarks/questions about the result analysis.

First, why did you present the model accuracy in percentage (above 2.5 FNU)? In my opinion, the main message of this article is that with a properly trained model, any camera can give a +/- 5 FNU estimation of the turbidity, no matter the range. By giving accuracy in %, you are confusing the reader who could believe that the accuracy increases with increasing turbidity (heteroscedasticity), which is apparently not the case (I am using Figure 2, but a residual plot would enable the reader to judge heteroscedasticity more clearly).

Second, why did you analyze the results differently below and above 2.5FNU? You can consider whether this is necessary, or if it is not overcomplicating things. In my opinion, this is not very relevant, as a natural water below 2.5 FNU can already be considered as very clean, not matter if the turbidity is 1.9 or 1.8 FNU. Again, in my understanding, your approach is not aiming at being as accurate as possible, but instead to be accurate enough to provide a cheap, accessible and quick way to estimate turbidity.",Authentic,"The measurement of turbidity serves as a key indicator of water quality and purity, crucial for informing decisions related to industrial, ecological, and public health applications. As existing processes require both additional expenses and steps to be taken during data collection relative to photography, we seek to generate accurate estimations of turbidity from underwater images. Such a process could give new insight to historical image datasets and provide an alternative to measuring turbidity when lower accuracy is acceptable, such as in citizen science and education applications. We used a two-step approach to a machine vision model, creating an image classification model trained on image data and their corresponding turbidity values recorded from a turbidimeter that is then used to generate continuous values through multiple linear regression. To create a robust model, we collected data for model training from a combination of in situ field sites and lab mesocosms across suspended sediment and colorimetric profiles, with and without a Secchi disk for visual standard, and binned images into 11 classes 0–55 Formazin Nephelometric Units (FNU). Our resulting classification model is highly accurate with 100% of predictions within one class of the expected class, and 84% of predictions matching the expected class. Regression results provide a continuous value that is accurate to ±0.7 FNU of true values below 2.5 FNU and ±33% between 2.5 and 55 FNU; values that are less accurate than conventional turbidimeters but comparable to field-based test kits frequently used in classroom and citizen science applications. To make the model widely accessible, we have implemented it as a free and open-source user-friendly web, computer, and Google Play application that enables anyone with a modern device to make use of the tool, the model, or our repository of training images for data collection or future model development."
Towards sustainable coastal management: aerial imagery and deep learning for high-resolution Sargassum mapping,"Basic reporting
All comments have been added in detail to the last section.

Experimental design
All comments have been added in detail to the last section.

Validity of the findings
All comments have been added in detail to the last section.

Additional comments
Review Report for ""Towards sustainable coastal management: Aerial imagery and deep learning for high-resolution Sargassum mapping"" in PeerJ

1. Within the scope of the study, classification and semantic segmentation processes were carried out using deep learning to detect Sargassum from aerial images.

2. In the Introduction section, what Sargassum is, its importance, its difficulties and basically semantic segmentation are mentioned by giving Figure-1. In this section, it is recommended to add a literature table consisting of columns such as dataset, model used, results, advantages/disadvantages, etc., in order to present the literature more clearly. Thus, the current topic and the literature can be compared better. In addition, it is recommended that the difference of the study from the literature and its main contributions to the literature be added more clearly at the end of the Introduction section.

3. In the Materials and Methods section, the semantic segmentation and dataset used in the study are basically mentioned. In the semantic segmentation section, first the u-net architecture, then the Pix2pix deep learning, a conditional generative adversarial network based on u-net, and the customizations made are explained. Although there are many different deep learning models that can be used for semantic segmentation when the literature is examined, it should be explained more clearly why the Pix2pix model is preferred. In the study of this model, the developed originality point, algorithm and hyperparameters used need to be explained in detail.

4. 15,268 aerial images with a 3-class structure consisting of ""Sargassum, Sand and Other"" classes were used as the dataset. The dataset used in the study is sufficient in quantity, size and quality. It has been stated that the dataset distribution is randomly divided into 80% training and 20% testing. The results obtained in classification and segmentation problems are very dependent on the dataset. For this reason, it is very important how the dataset distribution is made and how the training, validation and/or testing sections are determined. In order to analyze the results more accurately, it would be better to perform cross-validation instead of random distribution of the dataset. Based on this, it should be stated more clearly why cross-validation is not preferred and/or the dataset distribution ratio (80:20) and the basis on which the images are selected.

5. There are serious deficiencies in the evaluation metrics regarding classification and semantic segmentation in the results section. For example, metrics such as ROC curve, AUC score, precision-recall curve for both the triple class (Sargassum, Sand and Other) and binary class (Sargassum and Other) that are required to analyze the classification results correctly are not included.

As a result, although the study is important in terms of the dataset used and the problem addressed, it is recommended to pay attention to the parts explained in detail above.",Authentic,"The massive arrival of pelagic Sargassum on the coasts of several countries of the Atlantic Ocean began in 2011 and to date continues to generate social and environmental challenges for the region. Therefore, knowing the distribution and quantity of Sargassum in the ocean, coasts, and beaches is necessary to understand the phenomenon and develop protocols for its management, use, and final disposal. In this context, the present study proposes a methodology to calculate the area Sargassum occupies on beaches in square meters, based on the semantic segmentation of aerial images using the pix2pix architecture. For training and testing the algorithm, a unique dataset was built from scratch, consisting of 15,268 aerial images segmented into three classes. The images correspond to beaches in the cities of Mahahual and Puerto Morelos, located in Quintana Roo, Mexico. To analyze the results the fβ-score metric was used. The results for the Sargassum class indicate that there is a balance between false positives and false negatives, with a slight bias towards false negatives, which means that the algorithm tends to underestimate the Sargassum pixels in the images. To know the confidence intervals within which the algorithm performs better, the results of the f0.5-score metric were resampled by bootstrapping considering all classes and considering only the Sargassum class. From the above, we found that the algorithm offers better performance when segmenting Sargassum images on the sand. From the results, maps showing the Sargassum coverage area along the beach were designed to complement the previous ones and provide insight into the field of study."
"Comprehensive analysis of bioplastics: life cycle assessment, waste management, biodiversity impact, and sustainable mitigation strategies","Basic reporting
Clear writing and good structure. Sufficient references used correctly in context. Figures included are relevant, contributing to a self-contained study.

Experimental design
This is an original contribution, with a research question well defined; the methodology is detailed and well explained.

Validity of the findings
The discussion has been rewritten to present the main findings clearly. The information presented is of great interest, highlighting resilient corals that grow in urban sites, yet having a stable microbial community. The dominance of Endozoicomonas is also observed in these urban reefs.

Additional comments
The authors have done a great job in the resubmitted manuscript. Suggestions by reviewers have been considered and now the ms. is more concise, better structured, easy to read and follow, and highlights the main findings of the study.
I think the manuscript is ready for acceptance.",Generic,"Coral resilience varies across species, with some exhibiting remarkable stability and adaptability, often mediated by their associated microbiomes. Given the species-specific nature of coral-microbiome interactions, investigating the microbiomes of urban-adapted corals provides critical insights into the health, dynamics, and functioning of coral holobionts. In this study, we examined the microbiome of Madracis auretenra, a Caribbean coral from Santa Marta, Colombia, across contrasting environmental conditions. Over two years, we compared the microbiomes of healthy and stressed coral colonies from two distinct reef habitats—urban and protected—using 16S rRNA gene sequencing (V4 region) to assess microbial diversity. Our findings revealed microbial richness and diversity were primarily influenced by seasonal and local factors rather than host-specific traits such as interaction with algae, health status, or microhabitat. These variations were not substantial enough to disrupt the overall microbial community structure, which remained stable across temporal and spatial scales. Dominant taxa included Endozoicomonas, along with Vibrionaceae and Rhodobacteraceae, which form dense ecological interaction networks. Notably, nutrient and oxygen levels emerged as key drivers of microbiome fluctuations, yet Vibrionaceae populations exhibited exceptional temporal stability. These findings highlight the presence of a well-structured and resilient coral microbiome with minimal seasonal variability, even in urban-influenced environments. We propose that the dominance of Endozoicomonas and the stability of Vibrionaceae populations play a pivotal role in maintaining microbiome balance, ultimately contributing to the ecological resilience of M. auretenra in dynamic reef habitats."
Endozoicomonas dominance and Vibrionaceae stability underpin resilience in urban coral Madracis auretenra,"Basic reporting
The manuscript has been carefully reviewed and meets the scientific and editorial standards of PeerJ. The suggested revisions have been successfully incorporated, resulting in a clear and well-structured document with solid methodological and bibliographic support. The tables and figures have been improved for better clarity, enhancing the presentation of the results. Additionally, the revised title more accurately reflects the study’s content and scope.

The writing is professional, clear, and appropriate for a scientific journal, with no major grammatical errors. The manuscript includes relevant and up-to-date references, following PeerJ’s guidelines. The figures and tables are well-organized, and the availability of raw data in public databases ensures compliance with the journal’s requirements.

Experimental design
The research question is clear and relevant, with an improved introduction that follows previous recommendations and highlights the importance of studying microbiomes in urban corals. The methodology has been refined for greater clarity and reproducibility, providing a more precise description of data processing and the controls implemented to prevent contamination and sampling biases.",Authentic,"Coral resilience varies across species, with some exhibiting remarkable stability and adaptability, often mediated by their associated microbiomes. Given the species-specific nature of coral-microbiome interactions, investigating the microbiomes of urban-adapted corals provides critical insights into the health, dynamics, and functioning of coral holobionts. In this study, we examined the microbiome of Madracis auretenra, a Caribbean coral from Santa Marta, Colombia, across contrasting environmental conditions. Over two years, we compared the microbiomes of healthy and stressed coral colonies from two distinct reef habitats—urban and protected—using 16S rRNA gene sequencing (V4 region) to assess microbial diversity. Our findings revealed microbial richness and diversity were primarily influenced by seasonal and local factors rather than host-specific traits such as interaction with algae, health status, or microhabitat. These variations were not substantial enough to disrupt the overall microbial community structure, which remained stable across temporal and spatial scales. Dominant taxa included Endozoicomonas, along with Vibrionaceae and Rhodobacteraceae, which form dense ecological interaction networks. Notably, nutrient and oxygen levels emerged as key drivers of microbiome fluctuations, yet Vibrionaceae populations exhibited exceptional temporal stability. These findings highlight the presence of a well-structured and resilient coral microbiome with minimal seasonal variability, even in urban-influenced environments. We propose that the dominance of Endozoicomonas and the stability of Vibrionaceae populations play a pivotal role in maintaining microbiome balance, ultimately contributing to the ecological resilience of M. auretenra in dynamic reef habitats."
Advancing molecular macrobenthos biodiversity monitoring: a comparison between Oxford Nanopore and Illumina based metabarcoding and metagenomics,"Additional comments
This manuscript by Doorenspleet et al. is a timely comparison among three molecular methods and the morphological approach for molecular assessment of marine benthic metazoan biodiversity. Their methods are sound and their results are clear and conclusive, and they will be of interest for many ecological researchers that are currently wondering which molecular method is best for biodiversity assessment of marine eukaryotic communities, and possibly other ecosystems, since their results are probably translatable to other eukaryotic communities such as freshwater or terrestrial soils.
The manuscript is in general, well written, and provided that some minor corrections are addressed, which I detail below, I think that it can be publishable in PeerJ.

Minor corrections:

-Abstract:
L36: Correct the length of the fragment to ""The 313 bp COI Leray region""
L47: Remove duplicated words: ""standardized monitoring method.""

-Introduction:
L87 and L89: Correct ""Illumina MiSeq"" to ""Illumina technologies"". I do not think that ""Illumina MiSeq is currently the standard platform, since many laboratories have moved to Illumina NovaSeq and they are not using the MiSeq anymore. ""Illumina technologies are currently the standard platform"" and ""In comparison to Illumina technologies, the Oxford Nanopore…""
L116: Correct to: ""paired-end Illumina MiSeq metabarcoding""

-Materials and methods
L137: What was the full volume (size) of the Van Veen grab? Please specify.
L144: Anthozoa is currently considered a subphylum (McFadden et al., 2022), with two classes: Hexacorallia and Octocorallia. If anthozoans were not further classified in this study, then you should correct the word ""class"" to ""subphylum"". If they were classified either as Hexacorallia or Octocorallia, then it is fine to keep the rank ""class"", even though it can be a little misleading in this context. [McFadden, C. S., van Ofwegen, L. P., & Quattrini, A. M. (2022). Revisionary systematics of Octocorallia (Cnidaria: Anthozoa) guided by phylogenomics. Bulletin of the Society of Systematic Biologists, 1(3), 1–79.]
L162: Correct the details of the incubation instructions: ""were incubated overnight at 56 °C in the power-beads tube of the DNeasy PowerSoil Kit (QIAGEN), supplemented with 10 µL of proteinase K (20 mg/ml).""
L169. With the current description, it is ambiguous and difficult to know which primers were used for the amplification. I can see from Van den Bulcke et al. (2021) that the original Leray primer set were not used, but a modified version, produced by replacing the deoxy-inosines (I) by totally degenerated bases (N). These are not the primers from Leray et al. and you should specify this here clearly. Specially, since you used the KAPA HiFi Taq polymerase, which does not work properly with deoxy-inosines. So, please rewrite these sentences.
L268: Change ""normalized"" to ""transformed"" in ""Therefore, the data were not rarefied but transformed using a log10 transformation"".

-Results:
L290: ""3,060,417,120 data"" is ambiguous. Change to ""shotgun reads""
L349: Correct the name: ""Crepidula fornicata""
L354: Correct the name: ""Nephtys cirrosa""
L369: Delete the first ""the"" from ""the both the""
L375: Correct the name ""Scolelepis bonnieri""

-Discussion:
L466: Remove ""rRNA"" from ""contained only COI sequences"" or explain it more clearly, in the case that the database contained sequences from COI and other rRNA markers.
L486: Correct ""the mitogenomes of mock community were available""
L522: Correct the name: ""the Anthozoa Cylista""

-Data Availability:
L567-569: Improve the syntax of the data availability statement. Remove the first ""are available"", remove the s from ""Custom""",Authentic,"DNA-based methods and developments of sequencing technologies are integral to macrobenthos biodiversity studies, and their implementation as standardized monitoring methods is approaching. Evaluating the efficacy and reliability of these technological developments is crucial for macrobenthos biodiversity assessments. In this study, we compared three DNA-based techniques for assessing the diversity of bulk macrobenthos samples from the Belgian North Sea. Specifically, we compared amplicon sequencing using Illumina MiSeq and portable real-time sequencing of Oxford Nanopore versus shotgun sequencing using Illumina NovaSeq sequencing. The 313 bp mitochondrial cytochrome c oxidase subunit I (COI) metabarcoding fragment served as the target region for the metabarcoding analysis. Our results indicate that Oxford Nanopore and MiSeq metabarcoding had similar performances in terms of alpha and beta diversity, revealing highly similar location-specific community compositions. The NovaSeq metagenomics method also resulted in similar alpha diversity, but slightly different community compositions compared to the metabarcoding approach. Despite these differences, location-specific community compositions were maintained across all platforms. Notably, read counts from the NovaSeq metagenomic analysis showed the weakest correlation to size corrected morphological abundance and there were mismatches between morphological identification and all DNA based findings which are likely caused by a combination of factors such as primer efficiency and an incomplete reference database. Our findings underscore the critical importance of database completeness prior to implementing DNA-based techniques as standardized monitoring method, especially for metagenomics. Nevertheless, our findings emphasize that Oxford Nanopore metabarcoding proves to be a viable alternative to the conventional Illumina MiSeq metabarcoding platform for macrobenthos biodiversity monitoring."
"Diversity of lanternfish (Myctophidae) larvae along the Ninety East Ridge, Indian Ocean","Validity of the findings
Manuscript Review: ""Lanternfish Larvae Distribution and Population Dynamics in the Ninety East Ridge, Indian Ocean""
Summary: The manuscript is an interesting and comprehensive study on the genetic diversity and distribution patterns of lanternfish larvae from the Ninety East Ridge, identified using COI barcoding to 38 species. The study compared the effects of different net types on sampling, explored the genetic differentiation of species across water masses, and looked into population expansion patterns for key species, C. warmingii and N. valdiviae. The findings of the study provide valuable information on the role of water masses in shaping genetic diversity and evolutionary lineages in lanternfish species.
Strengths:
The integration of molecular techniques, such as COI barcoding, with ecological sampling provides a robust and thorough approach to studying larval distribution patterns and population structure. Both genetic and ecological data are combined to strengthen the conclusions of the study.
In-depth analysis on genetic patterns may be made on the construction of a phylogenetic tree, haplotype networks, besides AMOVA analyses, giving an effective interpretation of the structure of lanternfish populations. To this extent, the finding about water masses presenting barriers to certain species' gene flow adds new dimensions to such ecological factors associated with genetic differences in the Indian Ocean.
Focus on Population Dynamics: Analysis of population expansion in C. warmingii and N. valdiviae using neutrality tests, mismatch distribution, and EBSP analyses provides informative information on the evolutionary history of these species. Variability in the timing of expansions across species adds nuance to our understanding of their population dynamics.
These studies may have clear implications with respect to the conservation of lanternfish populations, both in the context of climate change and marine protected areas. The study would also be useful in providing a foundation for future monitoring efforts and devising conservation strategies.
Improvements:
Hypothesis Testing for SEI-29: The finding that the aggregation of haplotypes at the SEI-29 station is due to its seamount location on an isolated seamount is intriguing. However, further sampling from surrounding areas-perhaps especially south-west of the station-would strengthen the argument. In fact, it would be helpful to discuss possible alternative explanations for this observation, such as oceanographic current patterns or local ecological conditions.
Discussion of Potential Ecological Implications: The manuscript might further develop the ecological significance of the observed genetic divergence and population structure of lanternfish larvae. For example, how will such divergence between C. warmingii and N. valdiviae affect the roles of each in the marine food web or vulnerability to changes such as those related to climate change?

Additional comments
Minor Comments:
More clearly stating the study's key research questions and objectives in the introduction may help present those study's main questions and objectives more coherently. Overall, the background is pretty well presented; however, it is hard to immediately see what the ultimate research questions are.",Authentic,"Since the 19th century, the impact of seamounts on the distribution of plankton has been a topic of considerable interest. The influence of seamounts on the biogeographic patterns of marine organisms is complex, with some aspects still under debate. It is generally accepted that seamounts can drive the upwelling of nutrient-rich deep waters. Tidal amplification, flow acceleration, and internal waves can further enhance vertical mixing, leading to increased primary productivity near seamounts. Seamounts may also act as barriers to the migration of marine organisms, affecting gene flow. Research on Pacific seamounts suggests these features might serve as “stepping stones” for the dispersal of marine species across the ocean. However, investigations of seamounts in the eastern Indian Ocean remain limited. Focusing on the Ninety East Ridge region in the eastern Indian Ocean, this study collected zooplankton samples using horizontal (surface) and vertical (0–200 m) plankton nets and measured temperature and salinity profiles with a conductivity, temperature, and depth (CTD) sensor. A total of 544 fish larvae were identified, including 260 lanternfish larvae, representing 38 species across 12 genera, determined through COI DNA barcoding. Phylogenetic trees and haplotype networks were constructed to analyze genetic distances and population structures of lanternfish species. Among the samples, intra-specific genetic distances ranged from 0% to 2.99%, while inter-specific distances ranged from 1.88% to 25.71%. Except for Notolychnus valdiviae (Brauer, 1904), the maximum intra-specific distances were lower than the minimum inter-specific distances for all species. Haplotype analysis of nine species revealed significant variations in haplotype number, structure, and spatial distribution. Specifically, Ceratoscopelus warmingii (Lütken, 1892) and N. valdiviae exhibited a notable north-south divergence pattern, consistent with the temperature and salinity distribution of the region’s water masses. This conclusion was supported by analysis of molecular variance analysis, suggesting that larval stages of certain lanternfish species may struggle to cross boundaries between water masses. However, the remaining species showed no significant north-south distribution differences, possibly due to their adaptive capabilities, vertical migration patterns, or the duration of their planktonic larval stages. These findings suggest that seamounts and water mass distribution have varying implications for lanternfish species, potentially influencing gene flow and horizontal distribution patterns, which could contribute to speciation. Global climate change-induced alterations in ocean currents may profoundly impact the genetic diversity of fish species. This study provides new insights into the diversity of lanternfish in the Ninety East Ridge region and offers valuable data for understanding the biogeography of seamounts."
Susceptibility of lymnaeid snails to Fasciola hepatica and Fasciola gigantica (Digenea: Fasciolidae): a systematic review and meta-analysis,"Basic reporting
The manuscript is written in good, passable English, although I have proposed rephrasing of some sentences in a few instances (see attached pdf). Relevant literature is cited sufficiently to cover the study content. However, 26 extra references are listed that do not appear in text citations. I'm not sure if this can be related to errors in the reference manager software (as is sometimes the case), or the authors simply failed to thoroughly check the list. This is so glaring.

The review article is well structured and the results are relevant, bringing out an important picture on the global prevalence of fascioliasis from experimental studies and natural infections. The results adequately satisfy the research hypothesis.

Experimental design
The review study is well designed using standard, well-justified methods. and the study is within the scope and aims of the journal. The meta-analysis is technically sound and all data collection was standardised optimally, despite difficulties with the variability of the literature assessed.

Validity of the findings
The data was meaningfully presented and analysed. Tables 1 and 2 can benefit by adding columns showing the prevalence for each parasite species, as well as the overall prevalence for each method.

Additional comments
I have added a few comments in the attached annotated pdf.",Authentic,"Background
Fasciolosis is a food-borne disease that causes major economic losses, globally. This zoonotic disease is caused by Fasciola hepatica and Fasciola gigantica species which employ freshwater snails from the family Lymnaeidae as their intermediate hosts. Thus, a key aspect of understanding the epidemiology of the disease lies in understanding the transmission ecology of the parasite. Therefore, this systematic review and meta-analysis were conducted to assess the experimental susceptibility and prevalence of natural infections of F. hepatica and F. gigantica in lymnaeid snails.

Methods
Relevant peer-reviewed articles published in the past 20 years (2004–2023) were searched and appraised. Prevalence and infection rate estimates were based on 41 studies that met the inclusion criteria.

Results
Five thousand five hundred and seventy-five (5,575) lymnaeid snails were subjected to experimental infections and 44,002 were screened for natural infections. The overall pooled infection rate was higher in experimental infections 50% (95% CI [42–58%]) compared to natural infections of field-collected snails 6% (95% CI [0–22%]). The highest pooled infection rate was recorded in South America at 64% (95% CI [48–78%]) for experimental infections while the lowest was recorded for natural infections at 2% (95% CI [0–6%]) in Europe and 2% (95% CI [0–17%]) in Asia. In experimental studies, F. gigantica recorded the highest pooled prevalence at 73% (95% CI [61–84%] compared to F. hepatica which recorded 47% (95% CI [38–56%]). For natural infections, however, F. hepatica had the highest prevalence (12% (95% CI [0–30%]) while the lowest was noted for naturally infected F. gigantica at 2% (95% CI [0–18%]). Based on the snail species, the highest pooled prevalence was recorded for Pseudosuccinea columella infected with F. hepatica and F. gigantica at 47% (95% CI [33–61%]) while the lowest was recorded for F. hepatica naturally infected Galba truncatula at 4% (95% CI [0–10%]). Natural Fasciola spp. infections in intermediate snail hosts decreased in prevalence while experimental infections have increased in prevalence over the past 20 years.

Conclusions
While there seems to be a strong intermediate host specificity between the two Fasciola spp., experimental infection results showed that G. truncatula and R. natalensis are susceptible to F. hepatica and F. gigantica, respectively."
Investigating passive eDNA samplers and submergence times for marine surveillance,"Basic reporting
Very clear and concise writing with good coverage of all relevant literature. Raw data is shard on SRA. Hypotheses could be better defined with rationalizations e.g was the reduction in ASV richness over time expected?

Experimental design
Very good experimental design to address the research questions.

Validity of the findings
The contrary results between the ddPCR and metabarcoding could be explored further that would make this article more interesting. It would also be interesting to know what aspects of the study the authors would encourage replication for, are both materials and submergence times equally important as independent variables in future studies? Should future studies use both ddPCR/qPCR and metabarcoding for their assessments ? What might be the risks of only using one metric as seen in this study.

Additional comments
I have reviewed the manuscript titled “Investigating passive eDNA samplers and submergence times for marine surveillance.” The manuscript builds on valid research questions raised by previous studies: How is the efficacy of passive samplers impacted by submergence times, and do the sampling materials influence these trends? These questions were investigated via empirical testing with ddPCR for the detection of four invasive species, as well as 18S rRNA metabarcoding. This study is clear and well-written. I have only suggested minor edits in the text.

However, the data from ddPCR and metabarcoding seem to show opposite trends. This is quite interesting and should be highlighted more clearly in the text and discussed in greater detail to reduce the potential for confirmation bias as readers evaluate this highly interesting and relevant study. I believe that with these concerns addressed, this study is fit for publication in PeerJ.

Specific Comments:
Line 85: The sentence cites two studies, but a third study’s results are elaborated on in the following sentence to back up the claim.
Line 127: This part needs to be explained better: “and additionally the water squeezed from the Whirl-PakÆ Speci-Sponges.”
Line 236: NIS needs to be defined.
Line 242: Was the sample data rarefied? It says that they were rarefied in line 247, but no threshold is mentioned.
Line 322: Did NIS detection increase over time with passive sampling? This seems confusing, as line 303 mentions that NIS decreased with time. Perhaps this needs more clarification.
Line 397: Does this refer to eDNA yields or copy number of targeted NIS?
Methodological Concerns:
ddPCR methods/results: Assay kinetics and LOD/LOQ must be reported based on standardized guidelines. See:
Klymus, Katy E., et al. ""Reporting the limits of detection and quantification for environmental DNA assays."" Environmental DNA 2.3 (2020): 271-282.
Figures:
Figure 3: The regressions are not linear. Is the relationship between eDNA accumulation and time expected to be non-linear/exponential? What is the mechanistic rationale for choosing the given regression method (GLM with exponential link function) for this application?
Figure 4B: Do these scatterplots also include the NIS detected by the ddPCR assays? Were they co-detected with both analytical methods but showed different trends with submergence time?
The ASV richness seems to decline with submergence time, while the ddPCR detection of invasive species eDNA seems to increase in most cases (Figures 3, 4). Authors should discuss this in greater detail. Could this all be attributed to degradation and biofouling? It would also be interesting to see the ASV richness of the active filtration samples alongside these plots.
Figure 5B: What are the white spaces in Figure 5B? Relative abundance plots should not have white spaces. Is it all B. leachii? Are there non-invasive taxa represented here?
Figure 6: Interesting! The beta community composition at longer submergence time points is more different from the initial time point. This might be due to reduced ASV richness over time, as shown in Figure 4. However, the increasing beta dissimilarity, as shown in Figure 6, could be easily misinterpreted as a function of increased diversity captured over submergence time due to intuitive hypotheses and signals from Figure 4. I would encourage the authors to use a different visualization to convey this information or add additional details to avoid misinterpretation.",Authentic,"Passive environmental DNA (eDNA) samplers offer a cost-effective and scalable approach to marine biodiversity monitoring, potentially aiding detections of non-indigenous species. This study explored the efficiency of passive eDNA samplers to detect a variety of globally problematic marine invasive species in field conditions: Sabella spallanzanii, Styela clava, Bugula neritina and Undaria pinnatifida. Four passive sampler substrates, nylon filters, positively charged nylon discs, nylon mesh, and artificial sponges, were tested across six submergence times, ranging from 10 to 720 min, against standard filtration-based approaches. Our results demonstrated that passive samplers could achieve comparable or even higher eDNA yields than traditional active filtration methods, indicating their potential for biosecurity surveillance. Species-specific droplet-digital PCR (ddPCR) assays provided sensitive and quantifiable eDNA signals, though assay validation remains crucial to avoid false negatives. Significant variation in eDNA signal detection highlighted the importance of considering both material selection and submersion time, depending on the targeted organisms. Furthermore, 18S rRNA metabarcoding was undertaken to assess how the overall detected biodiversity might interfere with species-specific detections. Certain sessile organisms, such as ascidians and polychaetes, dominated early representation on the passive filters but did not interfere with species-specific detection. By optimizing material selection, submersion time, and assay validation, passive eDNA sampling can enhance the sensitivity and reliability of eDNA-based monitoring, contributing to improved marine biosecurity and conservation efforts."
Future climate-driven habitat loss and range shift of the Critically Endangered whitefin swellshark (Cephaloscyllium albipinnum),"Basic reporting
This study titled ""Projected Habitat Loss and Range Shift of the Critically Endangered Whitefin Swellshark (Cephaloscyllium albipinnum) in response to climate change"" is well designed and the modeling is important to see whether swellshark will affect from climate change or not. The studies for such critical animals are valuable and it is obvious that they will fill an important gap in the literature.

Experimental design
I have a main question.
Why did not authors use Bio-ORACLE v3.0 (Assis et al., 2024)?This version was produced from the CMIP6 Earth system models, and for the future conditions, they have a lot of layers. The different layers can be shown different results for the model.

Assis, J., Fernández Bejarano, S.J., Salazar, V.W., Schepers, L., Gouvêa, L., Fragkopoulou, E., Leclercq, F., Vanhoorne, B., Tyberghein, L., Serrão, E.A., Verbruggen, H., De Clerck, O. (2024) Bio-ORACLE v3.0. Pushing marine data layers to the CMIP6 Earth system models of climate change research. Global Ecology and Biogeography. DOI: 10.1111/geb.13813.

Other comments:
Minor comments:
In Occurrence data section on line 113.
How many did you obtain occurrence data from GBIF? and after all filtration process, how many data have you continued to model? This status should be mentioned here.

On line 138
There is a new version for Bio-ORACLE.

On line 165
Do you have a citation for this threshold value?

On line 195
Why didn't you include other feature classes?

On line 221
Please rewrite ""Were areas with.......""

Validity of the findings
These findings are enough.",Authentic,"Climate change is driving many species to shift their geographical ranges poleward to maintain their environmental niche. However, for endemic species with restricted ranges, like the Critically Endangered whitefin swellshark (Cephaloscyllium albipinnum), endemic to southeastern Australia, such dispersal may be limited. Nevertheless, there is a poor understanding of how C. albipinnum might spatially adjust its distribution in response to climate change or whether suitable refugia exist for this species in the future. Therefore, to address this gap, this study utilised maximum entropy (MaxEnt) modelling to determine the potential distribution of suitable habitat for C. albipinnum under present-day (2010–2020) climate conditions and for future conditions, under six shared socioeconomic pathways (SSP1-1.9, SSP1-2.6, SSP2-4.5, SSP3-7.0, SSP4-6.0 and SSP5-8.5) for the middle (2040–2050) and end (2090–2100) of the century. Under present-day conditions (2010–2020), our model predicted a core distribution of potentially suitable habitat for C. albipinnum within the Great Australian Bight (GAB), with benthic primary productivity and surface ocean temperature identified as key distribution drivers. However, under all SSP scenarios, future projections indicated an expected range shift of at least 72 km, up to 1,087 km in an east-southeast direction towards Tasmania (TAS). In all future climate scenarios (except SSP1-1.9 by 2100), suitable habitat is expected to decline, especially in the high-emission scenario (SSP5-8.5), which anticipates a loss of over 70% of suitable habitat. Consequently, all future climate scenarios (except SSP1-1.9 by 2100) projected a decrease in suitable habitat within a currently designated marine protected area (MPA). These losses ranged from 0.6% under SSP1-1.9 by 2050 to a substantial 89.7% loss in coverage under SSP5-8.5 by 2100, leaving just 2.5% of suitable habitat remaining within MPAs. With C. albipinnum already facing a high risk of extinction, these findings underscore its vulnerability to future climate change. Our results highlight the urgency of implementing adaptive conservation measures and management strategies that consider the impacts of climate change on this species."
Association of red blood cell distribution width-platelet ratio with mortality after coronary artery bypass grafting,"Basic reporting
unambiguous professional English is used throughout.
Literature references, and sufficient field background/context are provided.
Professional article structure, figures, tables. Raw data shared.
The manuscrit is Self-contained with relevant results to hypotheses.

Experimental design
The researchh question i well-definedd, relevant & meaningful. It is stated how research fills an identified knowledge gap
Original primary research within the Aims and Scope of the journal.
Rigorous investigation performed to a high technical & ethical standard.
Methods described with sufficient detail & information to replicate.

Validity of the findings
Impact and novelty not assessed. Meaningful replication encouraged where rationale & benefit to literature is clearly stated
All underlying data have been provided; they are robust, statistically sound, & controlled
Conclusions are well stated, linked to original research question & limited to supporting results

Additional comments
I want to thank the authors for agreeing with the suggestions made by the reviewers and for including them in the manuscript, which I believe will greatly interest the entire scientific community concerned with this topic.",Generic,"Background
This study aims to explore the association between red blood cell distribution width-platelet ratio (RPR) and mortality in patients after coronary artery bypass grafting (CABG).

Methods
Data on patients who underwent CABG from January 1, 2021, to July 31, 2022, were retrospectively collected. The locally weighted scatter plot smoothing (Lowess) method was utilized to display the crude association between RPR and in-hospital mortality. The areas under the receiver operating characteristic curves (AUC) were used to assess the discrimination. The cut-off value (0.107) of RPR was calculated using the Youden index method. The primary outcome was in-hospital mortality.

Results
In total, 1,258 patients were included. The Lowess curve showed an approximate positive linear relationship between RPR and in-hospital mortality. In the multivariable logistic regression model, RPR was an independent risk factor (OR 1.493, 95% CI [1.119–1.992] per standard deviation (SD) increase, p = 0.006) for in-hospital mortality after CABG. RPR (AUC 0.716, 95% CI [0.617–0.814]) demonstrated greater discrimination than RDW (AUC 0.578, 95% CI [0.477–0.680], p = 0.002). The cut-off value (0.107) of RPR was calculated for further analysis, and groups were further divided into the high RPR group (≥ 0.107) and the low RPR group (< 0.107). In the multivariable logistic regression model, high RPR (≥ 0.107) correlated with elevated risks of in-hospital mortality (OR 6.097, 95% CI [2.308–16.104], p < 0.001) and one-year mortality (OR 6.395, 95% CI [2.610–15.666], p < 0.001) after adjusting for all included covariates. Subgroup analyses revealed that high RPR consistently had increased risks of in-hospital mortality and one-year mortality. Besides, patients with low RPR show better one-year survival than those with high RPR.

Conclusion
Preoperative high RPR could serve as an independent risk predictor for in-hospital mortality and one-year mortality, which can be utilized to assess the prognosis of patients and further provide guidance for the treatment in patients following CABG."
Mehran vs. Mehran2 pre-procedure: which score better predicts risk of contrast-induced acute kidney injury in patients with acute coronary syndrome?,"Additional comments
In the index report, the authors explore the relationship and predictive ability of risk of contrast-induced acute kidney injury in patients with acute coronary syndrome using Mehran vs. Mehran2 scores. They demonstrated that The original Mehran score was more consistently discriminative for predicting CI-AKI risk in ACS patients undergoing PCI compared to the Mehran 2 Pre-procedural score. The analysis is timely, clinically relevant and of great use to physicians, and cardiologists. Tables and figures are used effectively to present complex data. The study employs robust statistical techniques, including ROC curve analysis and Poisson regression, to assess predictive capacity and validate findings. The comparison between scores is well-structured. Key metrics such as sensitivity, specificity, and AUC-ROC are systematically reported.

I have minor comments:-
1) The manuscript contains repetitive phrases and occasional complex sentence structures, which could hinder readability.
2) : Simplify sentences and eliminate redundancies. For example, replace ""the performance of the Mehran 2 Pre-procedural score was slightly inferior"" with ""The Mehran 2 Pre-procedural score underperformed slightly.""
3) The figures, such as the ROC curve, lack sufficient detail in their captions. Table labels are not consistently formatted, and legends can be improved for clarity.
4) Revise captions to be self-explanatory. Ensure consistent formatting and include details like ""confidence intervals"" or ""statistical tests used.""
5) The introduction provides an adequate background but could expand on the rationale for focusing on ACS patients exclusively.
6) Explain the clinical implications of excluding stable angina patients, particularly how this affects the broader applicability of findings.
7) The study population is relatively small and limited to a single center in Brazil, which may limit generalizability. Highlight this limitation more prominently and discuss its impact on external validity. Suggest multicenter studies for future research.
8) The study uses AUC-ROC as a key metric but does not explore other complementary measures of predictive power, such as reclassification indices. Include alternative metrics (e.g., net reclassification improvement) to provide a more comprehensive assessment of score performance.
9) The manuscript adequately explains most statistical tests but lacks details on model assumptions for Poisson regression. State how assumptions (e.g., no overdispersion) were tested and whether the Poisson model was compared with alternatives like negative binomial regression.
10) The study notes differences in findings compared to prior research but does not deeply analyze potential causes. Discuss these differences in the context of population characteristics, such as ethnicity or comorbidities, and their effect on score validity.
11) Highlight scenarios where the Mehran2 score's superior specificity could prevent unnecessary interventions.
12) Discuss the relevance of the original Mehran score's higher sensitivity for early detection in at-risk populations
13) Suggest a hybrid approach: using the original Mehran score for ACS patients at higher procedural risk and the Mehran2 score for broader, pre-procedural stratification.
14) Add a side-by-side comparison of score performance across patient subgroups to visually represent nuances in applicability",Authentic,"Background
Contrast-induced acute kidney injury (CI-AKI) is a significant concern during percutaneous coronary intervention (PCI) procedures. The novel Mehran 2 pre-procedural risk score, an updated version of the original Mehran score, shows promise as a predictive tool. However, its effectiveness specifically in acute coronary syndrome (ACS) patients requires further investigation. This study aims to evaluate the performance of Mehran 2 pre-procedure risk score compared to original score in predicting CI-AKI risk in acute coronary syndrome patients undergoing PCI.

Material and Methods
A prospective cohort study was conducted with patients with ACS undergoing PCI, who were followed up for 90 days (December 2019–February 2021). The Mehran 2 CI-AKI risk score with pre-procedure data was compared with the original Mehran score. Receiver operating characteristic (ROC) curve and area under the ROC curve (AUC-ROC) were used to evaluate the discriminative capacity.

Results
192 patients were analyzed and 33% (n = 64) developed CI-AKI. CI-AKI outcome was associated with advanced age, arterial hypertension, chronic kidney disease, troponin T, hemodynamic instability, serum hemoglobin, serum creatinine, and higher both Mehran scores. Both scores demonstrated good agreement. The original Mehran score demonstrated superior CI-AKI stratification with higher sensitivity (85.94%) and specificity (60.16%) compared to the Mehran 2 pre-procedural score (sensitivity 50%, specificity 75%). Significant differences were observed in the discriminative performance between both scores.

Conclusion
Sociodemographic, clinical, and laboratory variables were associated with CI-AKI. The original Mehran score demonstrated more consistent discriminative capacity for predicting CI-AKI risk in ACS patients undergoing PCI compared to the Mehran 2 pre-procedural score."
Understanding uremic cardiomyopathy: from pathogenesis to diagnosis and the horizon of therapeutic innovations,"Basic reporting
The introduction effectively sets the stage by defining UC as a serious complication of chronic kidney disease (CKD) and distinguishing it from other forms of cardiomyopathy. The emphasis on diastolic dysfunction and left ventricular hypertrophy (LVH) as primary features is well articulated, supported by relevant citations. The mention of UC's association with cardiac fibrosis and sudden cardiac death underscores its clinical significance, particularly in end-stage renal disease (ESRD) patients.

Experimental design
The study design is thorough, detailing a systematic literature search across multiple databases. The inclusion and exclusion criteria are clearly defined, ensuring the credibility of the selected studies. This section could benefit from a brief explanation of how the findings were synthesized, which would provide further clarity on the review process.

Validity of the findings
The article is well-researched and informative, addressing a critical area of concern in nephrology and cardiology. It successfully communicates the importance of understanding UC for improving patient outcomes in those with CKD.

Additional comments
Suggestions for Improvement
1. The article could benefit from a more detailed discussion on potential therapeutic strategies and interventions being explored for UC.
2. Including recent statistics or projections about the prevalence of UC could enhance the urgency of understanding and addressing this condition.
3. A clearer delineation of the implications for clinical practice, particularly in managing patients with CKD and UC, would add practical value for clinicians.",Authentic,"Uremic cardiomyopathy (UC) is a significant cardiovascular complication in individuals with end-stage renal disease. This review aims to explore the multifaceted landscape of UC, including the key pathophysiological mechanisms, diagnostic challenges, and current therapeutic approaches. The prevalence of cardiac hypertrophy, as a hallmark of UC, is highlighted and some new insights to its intricate pathogenesis, involving uremic toxins, oxidative stress, and inflammatory responses is elucidated. Diagnostic complexities, including the absence of specific biomarkers, are discussed, and the need for advanced imaging modalities and emerging diagnostic strategies are emphasized. Current therapeutic interventions, although lacking specificity, are addressed, paving its way to the potential future directions in targeted therapies. The review concludes new insights into the critical importance of ongoing research and technological advancements which will enhance early detection, precision treatment, and ultimately improve outcomes for individuals with UC."
Effect of atrial fibrosis on clot burden score and physicochemical properties of thrombus in patients with ischaemic stroke occurring in non-valvular atrial fibrillation,"Additional comments
In the index report, the authors highlight the impact of the degree of atrial fibrosis on the clot burden score (CBS) and physicochemical properties in patients with acute ischaemic stroke (AIS) due to non-valvular atrial fibrillation (NVAF). The manuscript is timely, clinically relevant and of use to physicians, neurologists and cardiologist. I have minor concerns which need to be addressed: -
1) In the abstract, Please mention the study was retrospective in nature, remove the name of hospital from the abstract and the main text. Expand PTFV1 in abstract. Clearly indicate for authors that CBS 0-6 signifies higher thrombus burden, otherwise one may find it confusing and contradictory. Remove the statidtcal details at most places in the abstract and keep it simple, concise and relevant. Your abstract in the current state gives it a monotonous imporession
2) Expand AIS in introduction and add that AF is a risk factor for Coronary artery disease and ischemia and also dementia besides stroke and HF. For instance, one prospective observational study reported dementia in almost 37% of all NVAF patients. include these details and also provide the relevant recent references. Dementia in turn, is partly linked to atrial fibroses as seen in certain studies
3) The retrospective nature limits the generalizability of the study findings as selction bias cannot be completely accounted for. No formal sample size calculation available for determining the power of the study to draw these conclusions
4) Remove the details of (TOAST) classification from methods
5) How was PTFV1 calculated. Provide details and diagram of ECG along with the same for the readers understanding and to allow for repeatability of your findings in future by other studies.
6) Another major limitation is that one cannot assume that GAL-3 and TGF B will always be synonymous with atrial fibrosis. These markers are non-specific and in general would indicate excess inflammation and fibrosis in the body and not specifically localizing to the atrium. The only definitive evidence would come from atrial biopsy possible at the time of some other cardiac surgery (although it is not a realistic one). Perhaps using LA strain in combination with these biomarkers would make more sense and give a definitive alternative to biopsy for quantifying atrial fibroses
7) Mention mean age in results up to 2 decimals with standard deviation
8) The first line of discussion is a repeat. Please avoid
9) Please check this statement ‘Stroke caused by AF is usually fatal or results in severe disability.’ And give valid reference to it if it is appropriate
10) No need to include details of all the biomarkers like BNP, NLR etc. just stick to the most relevant ones like GAL-3 and TGF B will and make the discussion short, concise and more relevant to your research.
11) Another inflammatory marker besides the ones discussed and researched by the authors is initial or baseline Hs-CRP which in fact of late has been shown to independent predictor of clinical outcome in NVAF. Perhaps the authors can include 1-2 recent papers highlighting the same in the references for the readers
12) I would suggest the authors to further mention in discussion (although the same has not been studied in the index paper) that atrial fibroses has a strong link with underlying ischemia in the presence or absence of epicardial stenosis and this in turn is one of the strongest predictors of clinical outcomes including CAD, MI, stroke, HF, recurrence after ablation etc. accorindgly, I will suggest addition of some of the most relevant literature surrounding this published in the last 2 years.
13) The conclusions should be specific, focused and discussing the salient findings and key messages from your study rather than previously known facts. Make it concise and straightforward",Authentic,"Background
To investigate the effect of the degree of atrial fibrosis on the clot burden score (CBS) and physicochemical properties in patients with acute ischaemic stroke (AIS) due to non-valvular atrial fibrillation (NVAF).

Methods
A total of 117 patients with AIS in NVAF attending the Department of Cardiovascular Medicine and the Cerebrovascular Diagnostic and Treatment Centre between August 2021 and May 2024 were included in the study. Baseline clinical data, biochemical indexes, and imaging data of the patients were collected, and the patients were divided into 93 cases of the CBS (score of 0–6) group and 24 cases of the CBS (score of 7–10) group according to the CBS. CBS (score of 0–6) signifies higher clot burden. The enzyme-linked immunosorbent assay was used to measure the concentration of galactaglutinin-3 (gal-3) and transforming growth factor (TGF-β1) in the serum of the patients, and the PTFV1 were collected by 12-lead electrocardiogram, and the differences in the degree of atrial fibrosis between different groups and the risk factors of CBS (score of 0–6) were analysed. To analyse the effect of atrial fibrosis on the collateral circulation of stroke, the patients were divided into 31 cases with good collateral circulation (grade 3–4) and 86 cases with poor collateral circulation (grade 0–2) according to the digital subtraction angiography (DSA) images. The cerebral thrombus was collected from 60 AIS patients who underwent mechanical thrombectomy. The content of erythrocyte, fibrin/platelets and leukocytes in the thrombus was analysed by Mathew’s scarlet blue staining, and the density of thrombus was measured by computed tomography (CT).

Results
A total of 117 patients were included in this study, and the proportion of hypertensive patients, proportion of chronic atrial fibrillation (CAF), B-type natriuretic peptide (BNP), neutrophil/lymphocyte ratio (NLR), D-dimer, uric acid concentration, proportion of patients with PTFV1 < −0.03 mm s, gal-3, and TGF-β1 were higher in the CBS (score of 0–6) group as compared to the CBS (score of 7–10) group (P-value < 0.05). Hypertension, proportion of CAF, homocysteine, NLR, D-dimer, uric acid, PTFV1 < −0.03 mm s, gal-3, and TGF-β1, were risk factors for the development of high CBS in atrial fibrillation (AF), and hypertension and CAF were the most important factors for the occurrence of AF in the independent risk factors for stroke combined with high clot burden. gal-3 and TGF-β1 were risk factors for poor collateral circulation, atrial fibrosis indexes were not associated with thrombus pathological composition and thrombus density.

Conclusions
Atrial fibrosis increases clot burden in patients with AIS due to NVAF but does not significantly correlate with the physicochemical properties and density of the thrombus."
Berberine-induced browning and energy metabolism: mechanisms and implications,"Additional comments
1，This study first describes the metabolism of berberine, the basic classification and functional status of white and brown adipose tissue, followed by the mechanisms of browning and brown adipose tissue activation. Finally, it discusses the effects and pathways of berberine in brown adipose tissue activation, listing two key factors AMPK and GDF15, as well as a series of packaging methods and applicability of berberine. Finally, the conclusion is drawn. The review has a certain level of organization, but lacks innovation, Lack of eye-catching viewpoints and lacks the latest research progress. The research content is relatively scattered and the themes are not focused enough.

2, The abstract first mentioned the panic of obesity, increased energy consumption, reduced appetite, and the importance of brown adipose tissue. To stimulate the positive effects of berberine by consuming energy. However, in the Introduction (Line 34) of the main text, the source, composition, and efficacy of berberine are mentioned first. Then it was mentioned that there is a potential effect of weight loss, as it promotes glucose and lipid metabolism, inhibits and regulates microorganisms. Here, it does not obediently introduce the browning of adipose tissue, but stating that berberine can induce browning and thermogenesis of adipose tissue (Line 52-53), which seems a bit abrupt. The reason of linking brown fat to the discovery of thermogenic function of berberine is rather far-fetched.

3, The title is about the mechanism and implications of berberine induced browning of adipose tissue, but the overall of berberine metabolism and packaging methods is not specifically targeted at adipose tissue or brown adipose tissue, but applied to the whole body. Therefore, it is suggested that the author revise it slightly wider, such as energy metabolism of skeletal muscle, myocardium, fat, brain, etc., and develop some targeted packaging materials to delay metabolism, which can have a more meaningful effect after absorption. The author may consider refining the title or reorganizing the logical structure of the paper writing.

4, But in terms of mechanism, which is the key content. The author listed many methods and proteins that activate brown adipose tissue. Table 2 lists the doses and studied pathways of berberine intake in cells, rats, mice, and humans. The dose range is somewhat broad and the time period is long-term. Is there a summary of the lowest effective dose or other signaling pathways besides AMPK and GDF15? What dosage works through which pathways and what regulatory mechanisms are involved.

Actually, the mechanism is not very clear, and most of the literature is only observation of phenomena, not gene knockout followed by berberine supplementation to determine the target. As referenced in Wang et al. 2021; Yang et al. (2017) found that GDF15 binds to its receptor, increases UCP1 expression levels, promotes browning, and subsequently reduces body weight.

5, Some descriptions are not precise enough and are rather vague, L343-344, how much better is the bioavailability of berberine-SLN than free berberine? Please write it down, then it will look more convincing.

6, Is the nanotechnology methods of berberine specifically designed for brown adipose tissue? Or they are fit for the whole body.

7, There was also a lot of statement about the classification of adipose tissue, the sources and activation of brown fats, but in fact, many of them are unrelated to berberine directly.

8, When it comes to the regulation of brown adipose tissue by berberine through the AMPK and GDF15 pathways, there is a lot of talk about the regulation of AMPK, but it is actually not related to berberine. At last, I recommend the authors can carefully sort out the key points and logic of this study.",Authentic,"Obesity has become a global pandemic. The approaches researched to prevent it include decreasing energy intake and/or enhancing energy expenditure. Therefore, research on brown adipose tissue is of great importance. Brown adipose tissue is characterized by its high mitochondrial content. Mitochondrial uncoupling protein 1 (UCP1) releases energy as heat instead of chemical energy. Thermogenesis increases energy expenditure. Berberine, a phytochemical widely used in Asian countries, has positive effects on body weight control. While the precise mechanisms behind this effect remain unclear, the adenosine monophosphate-activated protein kinase (AMPK) pathway is known to play a crucial role. Berberine activates AMPK through phosphorylation, significantly impacting brown adipose tissue by enhancing lipolytic activity and increasing the expression of UCP1, peroxisome proliferator-activated receptor γ-co-activator-1α (PGC1α), and PR domain containing 16 (PRDM16). While investigating the mechanism of action of berberine, both the AMPK pathway is being examined in more detail and alternative pathways are being explored. One such pathway is growth differentiation factor 15 (GDF15), known for its appetite-suppressing effect. Berberine’s low stability and bioavailability, which are the main obstacles to its clinical use, have been improved through the development of nanotechnological methods. This review examines the potential mechanisms of berberine on browning and summarizes the methods developed to enhance its effect."
"The association between fibroblast growth factor 21 with diabetes retinopathy among type 2 diabetes mellitus patients: a systematic review, meta-analysis, and meta-regression","Basic reporting
The manuscript submitted 103488v1 explores the association between fibroblast growth factor 21 (FGF-21) with diabetes retinopathy (DR) among type 2 diabetes mellitus (T2DM) patients. Authors illustrate that DR is a leading cause of vision disorders worldwide. FGF-21plays a crucial role in blood sugar regulation and have reported to be correlated with DR incidence and severity and they have concluded that higher level of FGF-21 is associated with higher DR and STDR incidence among T2DM patients. This study supports its use for further eye exams and therapies associated particularly with DR. The association between FGF-21 levels and DR will provide a way to clinicians and researchers to get insight into a novel pathway for future DR research, emphasizing its relevance as a biomarker for monitoring and predicting diabetic complications in type 2 diabetes patient. Clear and unambiguous English is used throughout the manuscript but the manuscript should be carefully revised to correct numerous grammatical issues. Sufficient data is provided to support the findings.

Experimental design
The study is well designed and the findings are interesting and have scientific values. Experimental design is impressive.

Material and Methods
1. Line 154- klotho association with diabetes and retinopathy, needs to be define in introduction.
2. Line 174- authors have not included TG levels as independent variable also dependent variables are not mentioned.
3. Line 183- I2, 2 should be written as subscript as it is creating confusion.
4. Line 193 pooled ORS, What is the significance of doing sensitivity analysis in present review)

Validity of the findings
Results

1. Line 237, what does it mean by the present study demonstrate a significant inverse association between FGF-21 levels and retinopathy diabetes incidence. It is suggested to give more explanation.

Discussion

1. Line 308-313 in discussion is in repetition to line 84 to 88 in introduction, again 313-315 is in repetition.
2. Clearly explain the role of FGF-21 in glucose metabolism and insulin resistance and how increase FGF-21 can affect glucose metabolism. The relation is not clear here, how increase hyperglycemia is related to increase FGF-21 levels and how increase FGF-21 can impair glucose metabolism. The role of FGF-21 with hyperglycemia and DR need more clear consideration.
3. Line 317-325 requires further explanation.
4. FGF-21 leads towards increase gluconeogenesis, so how can FGF-21 aim to repair microvascular damage in retinopathy.
5. More clarification regarding the cutoff values of FGF-21 is required to use it as a biomarker. Are the authors agreed with Jin et al., cutoff value 554.69 pg/ml.

Conclusion
In conclusions, authors have suggested that FGF-21 levels can be used to explore new pathway for future for DR treatment. How kindly explain.

Additional comments
The manuscript should be carefully revised to correct numerous grammatical issues. Additionally, the authors should address the specific issues listed below:
Abstract:
1. Line 25, Diabetic retinopathy (DR) is a leading cause of vision worldwide. It is a mistake, correct this sentence, word problem/ disorder is missing here.
2. Various abbreviation are used without defining them first such as T2DM, STDR and LDL. It is suggested to define them first and then use abbreviations throughout the manuscript.

Introduction:
1. Use of abbreviation is inconsistent, at some places the abbreviation of type 2 diabetes mellitus is used as T2DM and at some places it is written as Type 2 DM (Line 56). Again line 68 and 71, full form of diabetic retinopathy is used instead of abbreviation though it is already defined. Line no. 87 VEGF is not defined here. More consideration is required in this regard.
2. Line 59-61, it is suggested to either break the sentence or rephrase it for better understanding.
3. It would be worthwhile to give a brief detail regarding how oxidative stress can lead to diabetic retinopathy.
4. How proliferative and non-proliferative diabetic retinopathy are different from each other.
5. Line no. 88-90, how this is connected with T2DM, what is the purpose of adding this sentence.
6. Text is not justified throughout the manuscript.
7. It would be beneficial to explore the role of different other pathways associating T2DM and DR.",Authentic,"Background
Diabetic retinopathy (DR), a leading cause of vision loss worldwide, is a common complication of type 2 diabetes mellitus (T2DM) driven by chronic hyperglycemia and microvascular damage. Fibroblast growth factor 21 (FGF21) is crucial in blood sugar regulation and has been linked to DR incidence and severity. While some studies suggest that FGF21 levels may contribute to the DR incidence, others propose a protective role. This discrepancy necessitates further analysis, prompting this study to evaluate the association between FGF21 levels and DR incidence and severity in T2DM patients.

Methods
A systematic search was conducted through MEDLINE, Web of Science, Scopus, and Embase up to May 2024 for studies evaluating the association between FGF21 and DR incidence and severity. A random-effect model meta-analysis was performed to calculate the pooled standardized mean difference (SMD) and 95% confidence intervals (CI). A univariate meta-regression was performed to analyze factors influencing pooled size estimates. All statistical analyses were performed using STATA 17 software.

Result
This systematic review and meta-analysis of 5,852 participants revealed that FGF21 was positively correlated with DR (SMD 3.11; 95% CI [0.92–5.30], p = 0.005) and sight-threatening DR (STDR) incidence (SMD 3.61; 95% CI [0.82–6.41], p = 0.01). There was no significant difference in FGF21 levels in DR vs STDR (p = 0.79). Subgroup analysis revealed a significant difference in DR incidence between LDL groups, with higher DR incidence in the group with low-density lipoprotein (LDL) levels >100 (P < 0.00001). Meta-regression revealed no variables significantly influenced the pooled size estimates.

Conclusion
A higher level of FGF21 was associated with higher DR and STDR incidence among T2DM patients, highlighting its potential utilization as a biomarker for DR detection and enabling the exploration of FGF21-based treatment strategies. However, variables independently predicting DR among patients with elevated FGF21 levels shall be explored further."
Reliability and utility of blood glucose levels in the periodontal pockets of patients with type 2 diabetes mellitus: a cross-sectional study,"Basic reporting
The authors have successfully addressed all the concerns raised in the initial review of their manuscript titled ""Reliability and Utility of Blood Glucose Levels in the Periodontal Pockets of Patients with Type 2 Diabetes Mellitus: A Cross-Sectional Study."" The revisions have improved the clarity, organization, and scientific stringency of the study. The methods and results are now well-presented, with appropriate approach, and the discussion effectively contextualizes the findings within the existing literature. Ethical considerations have been clearly outlined, and the language has been polished for better readability. Overall, the manuscript is now suitable for publication.

Experimental design
There are no specifical comments for this section.

Validity of the findings
I have no further questions.",Authentic,"Background
Several studies have measured gingival blood glucose (GBG) levels, but few have confirmed systematic bias using Bland–Altman analysis. This study compared the effectiveness of GBG levels with that of fingertip blood glucose (FTBG) levels using Bland–Altman and receiver operating characteristic (ROC) analyses.

Methods
A total of 15 healthy volunteers and 15 patients with type 2 diabetes were selected according to inclusion and exclusion criteria. Each group comprised eight male and seven female participants. The GBG and FTBG levels were measured using a self-monitoring blood glucose device after periodontal examination. Pearson’s product‒moment correlation and simple linear regression analyses were performed. In addition, Bland‒Altman analysis was also performed to assess the degree of agreement between the two methods. ROC analysis was conducted to determine the sensitivity, specificity, and cutoff values for patients with diabetes. The area under the ROC curve (AUC) was used to identify significant differences.

Results
The mean GBG and FTBG levels were 120 ± 44.8 mg/dL and 137 ± 45.1, respectively, for the whole sample. The mean GBG and FTBG levels were 145 ± 47.2 mg/dL and 163 ± 49.1, respectively, in the diabetes group. The mean GBG and FTBG levels in the nondiabetes group were 95.3 ± 25.2 and 111 ± 18.8, respectively. Patients with diabetes were more likely to have a probing pocket depth (PPD) of ≥4 mm at the sampled site. Pearson’s product‒moment correlation and simple linear regression analyses revealed a significant correlation between the GBG and FTBG measurements. Bland–Altman analysis revealed that GBG and FTBG measurements differed significantly among all participants; however, no significant differences were observed among the patients with diabetes (mean difference (MD) ± standard deviation (SD) = −18.1 ± 34.2, 95% confidence interval (CI) [−37.0 to 0.88]) or among the participants with a PPD of ≥4 mm (MD ± SD = −15.2 ± 30.4, 95% CI [−30.8 to 0.43]). The sensitivity, specificity, and cutoff values of the GBG measurements for detecting diabetes were 80%, 93%, and 123.5 mg/dL, respectively. The sensitivity, specificity, and cutoff values of the FTBG measurements for detecting diabetes were 73%, 87%, and 134.0 mg/dL, respectively. No significant differences were observed between the AUCs (0.078, 95% CI [−0.006 to 0.161]).

Conclusions
The GBG measurements aligned with the FTBG measurements in the patients with diabetes and among the participants with a PPD of ≥4 mm. Patients with diabetes were more likely to have a PPD of ≥4 mm at the sampled site, GBG levels can be used to screen for type 2 diabetes in dental clinics."
Reliability and utility of blood glucose levels in the periodontal pockets of patients with type 2 diabetes mellitus: a cross-sectional study,"Basic reporting
Overall, the article uses clear and professional English, provides sufficient background information and literature references, has a well-organized structure, and shares raw data. However, there are a few areas that need improvement:

Language and Grammar:
Some sentences could be simplified for clarity. For example, the abstract contains some complex sentence structures that can be made more concise.
For instance, “Several studies have reported the measurement of gingival blood glucose (GBG) levels; however, few studies have confirmed the presence of systematic bias using the Bland-Altman analysis.” could be revised to: “Several studies have measured gingival blood glucose (GBG) levels, but few have confirmed systematic bias using Bland-Altman analysis.”
Literature References:
It is suggested to include more recent studies in the background section to ensure the timeliness and relevance of the literature.
For example, when discussing the measurement of GBG, adding references to recent publications would enhance the completeness of the background information.

Experimental design
The experimental design is robust, and the methods are described in detail, but the following areas could be improved:

Sample Size:
The sample size is relatively small (15 participants per group). It is recommended to discuss the impact of the sample size on the study’s findings and to mention in the discussion section whether further research with larger samples is needed to validate the results.
For example: “The sample size of 15 participants per group might limit the generalizability of the findings. Future studies with larger sample sizes are necessary to validate these results.”
Methodological Details:
Some methodological details could be further clarified. For instance, the criteria for selecting participants or the specific procedures for blood glucose measurement should be described in more detail to enhance replicability.
For example: “The inclusion and exclusion criteria for participant selection should be specified more clearly. Additionally, a step-by-step description of the blood glucose measurement procedure would help ensure the study can be accurately replicated.”

Validity of the findings
The study’s findings are robust and statistically sound, but the following points should be considered:

Statistical Analysis:
While the statistical methods used are appropriate, it would be helpful to provide more detailed explanations of the statistical tests and their results in the text.
For example: “A more detailed explanation of the Bland-Altman analysis results, including the interpretation of the bias and limits of agreement, would provide a clearer understanding of the findings.”
Conclusions and Implications:
The conclusions are well-stated and supported by the results, but discussing the broader implications of the findings and potential limitations in more detail would strengthen the article.
For example: “Discussing the potential clinical implications of using gingival blood glucose measurements in dental settings and any limitations related to the study’s scope or methodology would enhance the discussion section.”

Additional comments
The study is commendable for its thorough investigation and clear presentation. The findings are significant for clinical applications in screening type 2 diabetes in dental settings. However, minor revisions are needed to address the above points.",Authentic,"Background
Several studies have measured gingival blood glucose (GBG) levels, but few have confirmed systematic bias using Bland–Altman analysis. This study compared the effectiveness of GBG levels with that of fingertip blood glucose (FTBG) levels using Bland–Altman and receiver operating characteristic (ROC) analyses.

Methods
A total of 15 healthy volunteers and 15 patients with type 2 diabetes were selected according to inclusion and exclusion criteria. Each group comprised eight male and seven female participants. The GBG and FTBG levels were measured using a self-monitoring blood glucose device after periodontal examination. Pearson’s product‒moment correlation and simple linear regression analyses were performed. In addition, Bland‒Altman analysis was also performed to assess the degree of agreement between the two methods. ROC analysis was conducted to determine the sensitivity, specificity, and cutoff values for patients with diabetes. The area under the ROC curve (AUC) was used to identify significant differences.

Results
The mean GBG and FTBG levels were 120 ± 44.8 mg/dL and 137 ± 45.1, respectively, for the whole sample. The mean GBG and FTBG levels were 145 ± 47.2 mg/dL and 163 ± 49.1, respectively, in the diabetes group. The mean GBG and FTBG levels in the nondiabetes group were 95.3 ± 25.2 and 111 ± 18.8, respectively. Patients with diabetes were more likely to have a probing pocket depth (PPD) of ≥4 mm at the sampled site. Pearson’s product‒moment correlation and simple linear regression analyses revealed a significant correlation between the GBG and FTBG measurements. Bland–Altman analysis revealed that GBG and FTBG measurements differed significantly among all participants; however, no significant differences were observed among the patients with diabetes (mean difference (MD) ± standard deviation (SD) = −18.1 ± 34.2, 95% confidence interval (CI) [−37.0 to 0.88]) or among the participants with a PPD of ≥4 mm (MD ± SD = −15.2 ± 30.4, 95% CI [−30.8 to 0.43]). The sensitivity, specificity, and cutoff values of the GBG measurements for detecting diabetes were 80%, 93%, and 123.5 mg/dL, respectively. The sensitivity, specificity, and cutoff values of the FTBG measurements for detecting diabetes were 73%, 87%, and 134.0 mg/dL, respectively. No significant differences were observed between the AUCs (0.078, 95% CI [−0.006 to 0.161]).

Conclusions
The GBG measurements aligned with the FTBG measurements in the patients with diabetes and among the participants with a PPD of ≥4 mm. Patients with diabetes were more likely to have a PPD of ≥4 mm at the sampled site, GBG levels can be used to screen for type 2 diabetes in dental clinics."
"Association of thyroid hormones with the severity of chronic kidney disease: a cross-sectional observational study at Tabuk, Saudi Arabia","Basic reporting
Correlation Between Thyroid Hormone Levels and Renal Disease Severity in Chronic Kidney Disease Patients"" aims to investigate the relationship between thyroid hormone levels (TSH, T3, T4) and markers of renal disease severity (creatinine, urea, PTH) in patients with chronic kidney disease (CKD). The study employs a cross-sectional observational design involving 86 participants from King Fahad Hospital in Tabuk, Saudi Arabia. The research is structured well, with clear objectives outlined in the abstract and introduction sections. Key concepts are introduced effectively, providing a comprehensive background to contextualize the study's significance.

Experimental design
The study utilizes a prospective cross-sectional observational framework, which is appropriate for exploring associations between variables but limits the ability to establish causality. The cohort selection from a renal clinic and adherence to ethical guidelines are strengths of the study design. The methodology details the use of biochemical assays conducted on Roche Cobas E411 analyzers for thyroid hormones and renal markers, ensuring standardized measurements. However, the study lacks details on potential confounders or variables controlled for in the analysis, which could affect the interpretation of results.

Validity of the findings
The findings suggest a weak correlation between thyroid hormone levels and renal disease severity markers. Specifically, no significant correlations were found between creatinine/urea and thyroid hormones, contrasting with a strong positive correlation between PTH and creatinine. The use of Pearson correlation coefficients to quantify associations is appropriate, although the significance and clinical relevance of some correlations, such as T3 with creatinine, are questionable due to weak coefficients and lack of statistical significance. The strong correlation between PTH and creatinine highlights the study's contribution to understanding secondary hyperparathyroidism in CKD.

Additional comments
A few of the suggestions are listed.
Specify briefly why understanding the relationship between CKD and thyroid dysfunction is important from a clinical perspective. For instance, mention the prevalence or impact of thyroid dysfunction in CKD patients.
Consider rephrasing to explicitly state the primary aim of the study. For example, ""This study aims to assess the correlation between thyroid hormone levels (TSH, T3, T4) and markers of renal disease severity in CKD patients.""
Include a brief sentence explaining why these specific biochemical parameters and this cohort size were chosen. This adds context to the study design.
Instead of listing Pearson correlation coefficients, summarize the key findings in relation to the study objective. For instance, ""Thyroid hormone levels showed weak correlations with markers of renal disease severity, while a significant positive correlation was found between PTH and serum creatinine.
Emphasize the implications of the findings for clinical practice and future research. For example, ""While thyroid hormone levels appear to have limited impact on renal disease markers in CKD, the strong correlation between PTH and creatinine suggests potential implications for therapeutic strategies.""
The study provides valuable insights into the complex interplay between thyroid function and renal disease markers in CKD patients. While the findings contribute to the existing literature, several methodological improvements and considerations for future research could strengthen the validity and applicability of the results",Authentic,"Background
The interplay between chronic kidney disease (CKD) and thyroid dysfunction is becoming more evident in the biomedical community. However, the intricacies of their relationship warrant deeper investigation to understand the clinical implications fully.

Objective
This study aims to systematically evaluate the correlation between thyroid hormone levels, including thyroid-stimulating hormone (TSH), triiodothyronine (T3), and thyroxine (T4), and markers of renal disease severity. These markers include serum creatinine, urea, and parathyroid hormone (PTH) levels in individuals diagnosed with CK).

Methods
We conducted a cross-sectional observational study involving a cohort of 86 participants with CKD recruited from the renal clinic at King Fahad Hospital in Tabuk. Biochemical parameters, encompassing plasma electrolytes and thyroid hormone concentrations, were quantitatively assessed. These measurements were performed with the aid of a Roche Cobas E411 analyzer. The Pearson correlation coefficient was employed to delineate the strength and direction of the associations between the thyroid function markers and renal disease indicators.

Results
The statistical analysis highlighted a generally weak correlation between the concentrations of thyroid hormones and the indicators of renal disease severity, with Pearson correlation coefficients between −0.319 and 0.815. Critically, no significant correlation was found between creatinine and thyroid hormones (TSH, T3, T4), nor was any substantial correlation between urea and thyroid hormones. Conversely, a robust positive correlation was noted between the levels of parathyroid hormone and serum creatinine (r = 0.718, p < 0.001).

Conclusion
The data suggests that thyroid hormone levels have a minimal correlation with the severity of renal disease markers. In contrast, the pronounced correlation between PTH and creatinine underscores the importance of considering PTH as a significant factor in managing and therapeutic intervention of CKD complications. These initial findings catalyze further research to thoroughly investigate the pathophysiological relationships and potential therapeutic targets concerning thyroid dysfunction in patients with renal impairment."
Diabetes self-care and its associated factors among type 2 diabetes mellitus with chronic kidney disease patients in the East Coast of Peninsular Malaysia,"Basic reporting
This manuscript has been written well, and there are no significant issues with English or grammar.
The conclusion (in the abstract) can be reframed and contextualized.
All abbreviations, such as DKD and ESKD, must be spelled out when used for the first time on a page.
The study objectives can be made explicit with clear hypotheses presented at the beginning. Subsequently, a reference to statistically and non-statistically significant relationships can be made later.
The manuscript needs to be carefully evaluated for missing citations. For example – “Numerous research studies and recommendations in the field of diabetes support these methods. Numerous diabetes self-care initiatives have been documented in the literature but with varying degrees of success.” Reference to those studies must be provided.
I have included some other suggestions which can improve the manuscript.

Experimental design
The study could be informative and valuable if it includes the literature gap, indicating its rationale. However, the study has included sufficient literature to demonstrate the relationships between different variables to support the logical implications for the study’s overall goal. The overall goal should be rewritten. It says, “This study aims to determine the mean diabetes self-care score and its associated factors among type 2 diabetes mellitus patients with chronic kidney disease.” What does “the mean diabetes self-care score” indicate? It can be reframed. Also, what are those associated factors?

The result section should include more than just information about the tables. It should be a narrative description of the results with reference to hypotheses and appropriate tables for further information. Authors should make reference to their research questions/hypotheses.
Information in all tables must be interpreted, and key information should be presented in a descriptive manner for readers, such as what are the key characteristics of the research subjects and what the mean score of self-care actually represents. It should be more than just “moderate self-care.”
Authors claimed in line 310 “ There was no previous study done to determine a direct association between diabetes self-care and depression.” Please check the following study found through Google Scholar and they have used the same self-care scale. Authors need to do a literature review to contextualize the result. There should also be a rationale for difference or convergence with previous findings.

Tohid H, Papo M, Ahmad S, Sumeh AS, Jamil TR, Hamzah Z. Self-care activities among patients with Type 2 Diabetes Mellitus in Penampang, Sabah and its association with depression, anxiety and stress. Malaysian Journal of Public Health Medicine. 2019 Jan 1;19(1):117-25.
Additionally, in the same paragraph, there could be some citation issues.

Validity of the findings
The validity of the result must be reevaluated in light of a new literature review. How the findings contribute back to the literature must be presented succinctly.",Authentic,"Introduction
Diabetes self-care among diabetic patients is crucial as it determines how patients care for their illness in their daily routine for better diabetes control. This study aims to calculate the average score for diabetes self-care among patients with type 2 diabetes mellitus and chronic kidney disease and to identify factors that are associated with this score.

Materials and Methods
This cross-sectional study enrols patients over 18 years old with type 2 diabetes mellitus and chronic renal disease with an eGFR of less than 60 mL/min/1.73 m2 in a tertiary hospital in Malaysia. The Malay version of the Summary of Diabetic Self-Care Activities (SDSCA) was used to assess diabetes self-care, the Malay version of the diabetes-related distress questionnaire (DDS-17) was used to assess diabetes distress, and the Malay version of the Patient Health Questionnaire-9 (PHQ-9) was used to assess depression. Data analysis was performed using both simple and multiple linear regression models to determine the associations between variables.

Result
One hundred and seventy-six eligible patients were recruited for this study. The mean score for diabetes self-care is 3.62. The eGFR (p = 0.002) and diabetes distress (p = 0.004) are the significant associated factors for diabetes self-care among type 2 diabetes mellitus patients with chronic kidney disease.

Conclusion
The mean score for diabetes self-care indicated a moderate level of self-care. The eGFR level and diabetes distress were important factors influencing diabetes self-care practices."
"Knowledge, attitude and purchasing behavior of Saudi mothers towards food additives and dietary pattern of preschool children","Basic reporting
I have reviewed the manuscript titled "" Knowledge, attitude and purchasing behavior of
Saudi mothers towards food additives and dietary pattern of preschool children."" The study was interesting; however, certain areas require improvement, my detailed comments and suggestions are provided below.
For authors I should recommend that’s, please enhance the introduction by providing more recent references to underline the significance of the study.
Clearly state the research gap that this study aims to fill.
For review litrature purpose, expand on the adverse effects of specific food additives with more detailed examples from recent studies.
You mentioned that the study aims to evaluate the knowledge, attitude, and purchasing behavior toward food additives among mothers in the western region of Saudi Arabia. Why was this specific region chosen for the study? Are there particular characteristics or trends in this region that justify its selection?
The introduction lists several food additives and mentions over 10,000 substances classified as food additives. Does your study focus on specific food additives or categories of additives? If so, which ones and why were they chosen?
The introduction suggests that educating mothers about healthy food choices can positively influence their children's diets. Can you elaborate on the specific educational interventions or strategies that have been shown to be effective in previous studies? How does your study aim to contribute to this body of knowledge?
Is there evidence to suggest that regulatory awareness impacts purchasing decisions?
Ensure all references are formatted according to PeerJ guidelines.
Add more recent references to support the introduction and discussion sections.

Experimental design
The study uses a cross-sectional design with an online survey, which is appropriate for the research question. However, I have some concerns as detailed below,
You utilized a nonprobability convenience sample of 385 mothers. Could you provide more detailed justification for the sample size and explain why a convenience sample was chosen over other sampling methods? How might this sampling method impact the generalizability of your findings?
Mention any potential biases introduced by this sampling method and how they were addressed?
The Food Frequency Questionnaire (FFQ) was adopted from a previous study. Could you provide more details on the process of adapting the FFQ to your study? Were there any modifications made to tailor it to the specific context of your research?

Validity of the findings
You excluded 51 responses from participants living outside the western region of Saudi Arabia. Can you provide more details on how the exclusion criteria were determined? Were there any notable differences in the excluded responses that could impact the study's findings?
While 52.1% of participants acquired information about food additives from social media/the internet, can you assess the accuracy and reliability of these sources? How do you think the source of information might affect the participants' knowledge and perceptions of food additives?
The results indicate that a significant percentage of children were underweight or obese, and many suffered from food allergies or dental caries. Can you discuss how these health indicators were measured and their potential implications for the study's findings on dietary patterns?
You found statistically significant differences in mothers' knowledge of food additives based on age, education level, occupation status, and economic status. Can you provide more insight into why these particular demographic factors influence knowledge levels? Are there any interventions or educational programs that could target these specific groups to improve knowledge?
There were 15 missing data points for weight and 94 for height. How did you handle these missing data in your analysis? Did you employ imputation methods, or were these cases excluded from specific analyses?
The results show that employed mothers had more positive attitudes towards food additives. Can you explore possible reasons for this finding? How might employment status influence mothers' attitudes and purchasing behaviour towards food products containing additives?

Comparative analysis with previous studies in the discussion section
The discussion mentions that higher socioeconomic status correlates with better knowledge and attitudes towards food additives. Could you provide more details on how socioeconomic factors specifically contribute to these differences? For example, does income level affect access to healthier food options or educational materials?
The discussion mentions that preschool children consume certain unhealthy food products daily. What specific interventions or policy changes would you recommend to improve dietary patterns among this age group? Could educational programs targeting both mothers and children be effective?
You acknowledge several limitations, including the observational nature of the study and response bias. Can you propose specific methodologies for future research that might address these limitations? For example, would longitudinal studies or randomized controlled trials provide more robust data?

Additional comments
In your ethical considerations section, you mention obtaining ethical approval and informed consent. Can you include the specific protocol reference number for the ethical approval? Additionally, were there any specific ethical challenges you faced during the study, and how were they addressed?",Authentic,"Background
There are over 506 children’s products containing one or more types of additives. Maternal awareness of these additives is essential for the health of preschool-aged children, as this period is vital for children’s growth and development. This study aims to assess the knowledge, attitudes, and purchasing behaviors related to food additives among mothers living in the western region of Saudi Arabia, as well as the dietary patterns of preschool children.

Method
A cross-sectional study was conducted using an online survey with a convenience sample of 521 mothers of preschool-aged children (3–5 years old). The survey gathered data on the child’s age, number of children, the youngest child’s weight and height, food intolerance, tooth decay, as well as the dietary patterns of preschool children. It also assessed the mother’s knowledge, attitude, and purchasing behaviors related to food additives.

Results
The study found that 46.6% of mothers demonstrated good knowledge of food additives, while 56.0% demonstrated fair attitudes and 78.5% good purchasing behavior regarding additives. Additionally, the majority of mothers reported favorable dietary patterns for their preschool-aged children. “Biscuits and crackers” had the highest consumption frequency (4.98 ± 1.50), with 36.7% of children consuming them once daily, while “Soft beverages” had the lowest consumption frequency (2.73 ± 2.04), with 46.6% of children never consuming them. Statistically significant differences were identified between mothers’ knowledge and their age, education level, occupation status, and economic status (p < 0.05). ANOVA results also indicated a statistically significant difference between mothers’ attitudes and occupation status (p < 0.05). Furthermore, there were significant positive correlations between mothers’ knowledge of food additives and their attitudes (r = 0.293) and purchasing behaviors (r = 0.284) related to additives.

Conclusion
The findings suggest that mothers possess a relatively good level of knowledge of food additives and hold fair attitudes toward them, tending to result in healthier purchasing behaviors and dietary practices for their preschool-aged children. To increase awareness, nutrition intervention programs are required across various socio-economic groups of mothers in the western region of Saudi Arabia. These programs can significantly contribute to promoting healthier dietary practices for preschool-aged children and improving overall family health and well-being."
DMSA-Net: a deformable multiscale adaptive classroom behavior recognition network,"Basic reporting
The paper is well-structured and easy to read. The narrative progresses logically: the introduction outlines the problem, the second section reviews related work on the topic, and subsequent sections describe the methodology and present the results.

Minor comments:
1. When numbering equations in LaTeX, it is recommended to use the ""{equation}"" environment to ensure consistent formatting. In the current document, equations are manually numbered with a space following the formula.
2. Figures 1, 2, 3, 4, 5, 6, 9, and 10 are presented in relatively low resolution. Pixelation is visible, and in some cases (e.g., Figures 9 and 10), it is difficult to discern the objects detected by the model.
3. Figure 8 appears to be a screenshot, as indicated by the presence of a gray element in the lower-left corner.
4. It is unclear why the abbreviation for ""Multiscale Attention Feature Pyramid Structure"" is ""NSAFPS"" rather than ""MSAFPS."" The reasoning behind the use of ""N"" instead of ""M"" should be clarified.
5. Line 32 contains a possible typographical error: ""prowess"" should likely be ""process.""
6. Many instances of incorrect capitalization appear throughout the text. For example:
- Articles are capitalized unnecessarily, such as on line 23, several times between lines 262 and 263, and in other parts of the document.
- ""We"" is incorrectly capitalized on line 30.
7. Conversely, some terms and sentences should be capitalized but are not:
- Line 58: ""RoI"" should be ""ROI.""
- Figure captions (e.g., for Figures 6, 9, 10, and 11) begin with lowercase letters but should start with uppercase.
- Line 268: The sentence starts with a lowercase letter, or there is a period is used in place of a comma.
Similar issues are present throughout the document and require thorough proofreading.
8. Most references to figures in the text appear after the corresponding figure. It would be more logical to reference the figure before presenting it.
9. Figures 3 and 6 are included in the article, but there are no references to them in the text.
10. On line 62, multiple citations are listed separately. It would be better to combine them using a single command, e.g., ""\cite{ref-book8, ref-book9, ref-book10, ref-book11, ref-book12, ref-book13, ref-book14}"" instead of writing each citation individually.
11. There are several instances where no space follows a citation or a parenthetical explanation, such as in ""NSAFPS(Multiscale Attention Feature Pyramid Structure)"" on line 28 or ""conduct\cite{ref-book1}"" on line 39. In contrast, there are cases where unnecessary spaces precede punctuation marks, such as on line 39 and between lines 262 and 263. To address these inconsistencies, it requires thorough proofreading.
12. The term ""Multiscale"" is inconsistently written as ""Multi-scale"" in some places. Consistent spelling should be used throughout the document.
13. For numbered lists, it is recommended to use the ""enumerate"" environment in LaTeX for clarity and formatting consistency (e.g., lines 108, 113, and 118).
14. Line 170 contains a typographical error: ""BoumdingBox"" should be ""BoundingBox."" or even with space between bounding and box.
15. Line 248: ""BackBone"" should be ""Backbone.""
16. Abbreviations should be defined upon their first occurrence. For example, ""EMA"" is first expanded on line 306, but it appears earlier on line 293 without explanation. Similarly, the origin of the ""E"" in ""Multiscale Attention"" (abbreviated as ""MA"") is unclear. This issue is repeated for other abbreviations and should be addressed.
17. In Figure 6, uppercase letters are used for labels, but the same labels appear in lowercase in the description. Furthermore, the labels on the figure and in the image description differ significantly and should be aligned.
18. Formulas 6–8 appear to require reordering for clarity. For example, Formula 6 introduces $L_{WIOUv1}$, which has not been previously defined, and combines it with a formula calculating $\gamma$. Probably these two formulas should be separated in two lines. Formula 7 uses $R_{WIOU}$, which is described in Formula 8. This arrangement forces the reader to search backward to understand the notation.
19. Lines 512–513: Are you certain that the reference is to Figure 8? It is better to use the ""ref{}"" command in the LaTeX document for referencing figures and tables instead of manual references.
20. No links to the table 7 in the text.

Experimental design
After a review of the article, the following questions remain:
1. What is the execution time of the models? Is the proposed method intended for real-time analysis of student behavior during lectures?
2. Student behavior can change throughout a lecture. However, the experiments were conducted on static images. How is the proposed algorithm intended to work when applied to lecture video recordings? Specifically, how should the model's output be interpreted if, at one moment, a student is attentively listening, then becomes distracted by their phone, and later resumes paying attention?
3. How will the proposed approach perform in scenarios involving varying lighting conditions in the classroom and differing input photo/video quality?

Validity of the findings
1. Comparing the results of the proposed algorithm, authors conducted experiments with other well-known model. However, results obtained by other researchers on the SCB-Dataset3-S dataset were not included. It would be valuable to see a comparison with the results of other researchers. These results can be added to the table 7.
2. Figure 11 presents the model's output. Why were many faces not detected?
3. Will the code be available on GitHub or another platform for researchers to reproduce the results presented in the article?

Additional comments
A significant contribution of this work is the publication of a new dataset containing images of students in a classroom exhibiting various behavior patterns: hand-raising, reading, writing, using a phone, bowing their head, etc.",Authentic,"In the intelligent transformation of education, accurate recognition of students’ classroom behavior has become one of the key technologies for enhancing the quality of instruction and the efficacy of learning. However, in the recognition of target behavior in real classroom scenarios, due to the use of wide-angle or panoramic images for image acquisition, students in the back row are far away from monitoring devices, and their subtle body movements such as the small opening and closing of the mouth (to determine whether they are speaking), fine finger operations (to distinguish between reading books or operating mobile phones) are difficult to recognize. Moreover, there are occlusions and scale differences in the front and back rankings, which can easily cause confusion and interference with target features in the detection process, greatly limiting the accurate recognition ability of existing visual algorithms for classroom behavior. This article proposes a deformable multiscale adaptive classroom behavior recognition network. To improve the network’s capacity to model minute behavioral phenomena, the backbone section introduces a deformable self-attention dattention module, dynamically modifying the receptive field’s geometry to enhance the model’s concentration on the region of interest. To improve the network’s capacity for feature extraction and integration of behavior occlusion and classroom behavior at different scales, a proposal has been put forward the Multiscale Attention Feature Pyramid Structure (MSAFPS), to achieve multi-level feature aggregation after multiscale feature fusion, reducing the impact of mutual occlusion and scale differences in classroom behavior between front and back rows. In the detect section, we adopt the Wise Intersection Over Union (Wise-IoU) loss as our loss criterion, augmenting the evaluation framework with richer contextual cues to broaden its scope and elevate the network’s detection prowess. Extensive experimentation reveals that our proposed method outperforms rival algorithms on two widely adopted benchmark datasets: SCB-Dataset3-S (the Student Classroom Behavior Dataset–https://github.com/Whiffe/SCB-dataset) and we created object detection dataset DataMountainSCB (https://github.com/Chunyu-Dong/DataFountainSCB1) containing six types of behaviors."
UMEDNet: a multimodal approach for emotion detection in the Urdu language,"Basic reporting
This paper presents UMEDNet, a multi-modal emotion prediction model for Urdu, and the UMED Corpus, a key resource for emotion classification in low-resource languages. The model effectively classifies emotions using video, speech, and text data.

1. In the ""Related Works"" section, it would be beneficial to provide a more comprehensive discussion of the challenges faced by low-resource languages in both emotion prediction and multi-modal models. Specifically, the paper could elaborate on the limitations posed by the scarcity of publicly available datasets and pre-trained models, which are major obstacles in these fields. It would also be valuable to explore existing solutions, such as cross-lingual transfer learning, and offer a comparison between these approaches and multi-modal studies. Additionally, including relevant literature on similar studies conducted in languages such as Tamil or Haitian Creole would further strengthen the context and breadth of the research.
2. In the ""Corpus Collection"" section, further elaboration on the data quality and diversity would enhance the comprehensiveness of the discussion. Specifically, it would be valuable to address the balance between formal and informal language (or written vs. spoken language) in the collected data. Additionally, the inclusion of demographic factors, such as age, gender, and other relevant variables, would provide a more nuanced understanding of the corpus' representativeness. Furthermore, a discussion on the overall size of the dataset would be beneficial, as this impacts the generalizability and robustness of the findings.
3. In the ""Data Annotation"" section, it would be useful to outline the measures taken to ensure the quality and reliability of the annotated data. Specifically, clarifying the procedures followed to guarantee that the annotated video meets high standards, often referred to as ""gold"" annotations, would strengthen the validity of the dataset. This could include details on inter-annotator agreements, quality control processes, and any validation methods employed.",Authentic,"Emotion detection is a critical component of interaction between human and computer systems, more especially affective computing, and health screening. Integrating video, speech, and text information provides better coverage of the basic and derived affective states with improved estimation of verbal and non-verbal behavior. However, there is a lack of systematic preferences and models for the detection of emotions in low-resource languages such as Urdu. To this effect, we propose Urdu Multimodal Emotion Detection Network (UMEDNet), a new emotion detection model for Urdu that works with video, speech, and text inputs for a better understanding of emotion. To support our proposed UMEDNet, we created the Urdu Multimodal Emotion Detection (UMED) corpus, which is a seventeen-hour annotated corpus of five basic emotions. To the best of our knowledge, the current study provides the first corpus for detecting emotion in the context of multimodal emotion detection for the Urdu language and is extensible for extended research. UMEDNet leverages state-of-the-art techniques for feature extraction across modalities; for extracting facial features from video, both Multi-task Cascaded Convolutional Networks (MTCNN) and FaceNet were used with fine-tuned Wav2Vec2 for speech features and XLM-Roberta for text. These features are then projected into common latent spaces to enable the effective fusion of multimodal data and to enhance the accuracy of emotion prediction. The model demonstrates strong performance, achieving an overall accuracy of 85.27%, while precision, recall, and F1 scores, are all approximately equivalent. In the end, we analyzed the impact of UMEDNet and found that our model integrates data on different modalities and leads to better performance."
TASCI: transformers for aspect-based sentiment analysis with contextual intent integration,"Basic reporting
The manuscript is written in clear, professional English, with precise terminology aligning with the fields of sentiment analysis and machine learning. The introduction effectively provides context for sentiment analysis, aspect-based sentiment analysis (ABSA), and intent-based sentiment analysis (IBSA), though it could benefit from explicitly linking the identified research gap to the objectives of the proposed TASCI model. The figures and tables are relevant, well-labeled, and contribute significantly to the understanding of the model's performance, though figure resolution should be checked for publication standards. The manuscript adheres to a logical structure, flowing seamlessly from the introduction to the methodology, results, and conclusions. The raw data and supplementary materials, including datasets and source code, are made available, meeting data transparency requirements, though ensuring all supplementary details, such as hyperparameters and configurations, are included will enhance reproducibility. The results are self-contained and tied to the hypotheses, with metrics like Accuracy, Precision, Recall, and F1-Score clearly presented; however, definitions of these metrics should be added for broader accessibility. The ablation study is thorough, validating the contributions of various model components. Additionally, the manuscript includes sufficient explanations of technical concepts, such as self-attention and GRU mechanisms, making it approachable for readers across different expertise levels. Expanding the conclusion to outline future research directions, such as the broader applicability of TASCI in other NLP tasks, would further strengthen the manuscript. Overall, the study adheres to PeerJ standards and effectively communicates its findings.

Experimental design
The paper makes a significant contribution to the field of context-aware sentiment analysis. The research question is clearly defined and aims to fill the knowledge gap in the literature. The TASCI model provides an innovative approach that includes contextual intent integration, which is lacking in traditional sentiment analysis methods. The results obtained on the restaurant, laptop and Twitter datasets demonstrate the accuracy and effectiveness of the proposed model. The originality of the research and the proposed solution are of significant value in the fields of NLP and machine learning.

In the method section, the components of the model are explained in detail and clearly, which increases the reproducibility. The integration of modern techniques such as the self-attention mechanism, GRU-based intent analysis and transformer-based sentiment classification supports the robustness and validity of the method. The benchmark datasets and performance metrics used increase the reliability of the analysis. However, a clearer specification of ethical standards regarding the use of data could strengthen the integrity of the study.

The results clearly demonstrate the superiority of the proposed model. The accuracy and Macro-F1 scores of TASCI show that it outperforms other existing models. However, more analytical and visual details of the results can deepen the reader's understanding. The strengths of the model include filling a gap in the literature, a detailed explanation of the method, and its success on different datasets. However, discussion of the limitations of the model and more detailed consideration of some of the shortcomings can broaden the scope of the study.
Overall, the article provides an innovative and effective solution to an important problem in the field of NLP and, with the proposed TASCI model, goes beyond existing methods in the literature. However, ethical standards, visual support and more detailed discussion of limitations would further strengthen the study.

Validity of the findings
The impact and novelty of the research have not been explicitly assessed within the manuscript. While the proposed TASCI model presents a clear contribution to the field by integrating intent analysis with aspect-based sentiment analysis, further elaboration on how this approach significantly advances the state-of-the-art or addresses practical challenges in real-world applications could enhance the impact. Meaningful replication of the study is encouraged, as the rationale for the model and its integration with existing methodologies is well-stated. The benefit of TASCI to the literature is evident, particularly in its ability to outperform prior models on benchmark datasets; however, its broader implications could be articulated more thoroughly.

All underlying data have been provided and are robust, statistically sound, and well-controlled. The use of benchmark datasets (Restaurant, Laptop, and Twitter) ensures that the results are comparable and relevant to the field of natural language processing. Additionally, the inclusion of detailed performance metrics (Accuracy and Macro-F1) for each dataset enhances the reliability of the findings.

The conclusions are well-stated and appropriately linked to the original research question. They are limited to supporting results, effectively summarizing the contributions and the superiority of TASCI in aspect-based sentiment classification. The conclusions align with the evidence presented and do not overreach, ensuring a balanced and evidence-based interpretation of the findings.",Authentic,"In this article, we present a novel Transformer-Based Aspect-Level Sentiment Classification with Intent (TASCI) model, designed to enhance sentiment analysis by integrating aspect-level sentiment classification with intent analysis. Traditional sentiment analysis methods often overlook the nuanced relationship between the intent behind a statement and the sentiment expressed toward specific aspects of an entity. TASCI addresses this gap by first extracting aspects using a self-attention mechanism and then employing a Transformer-based model to infer the speaker’s intent from preceding sentences. This dual approach allows TASCI to contextualize sentiment analysis, providing a more accurate reflection of user opinions. We validate TASCI’s performance on three benchmark datasets: Restaurant, Laptop, and Twitter, achieving state-of-the-art results with an accuracy of 89.10% and a macro-F1 score of 83.38% on the Restaurant dataset, 84.81% accuracy and 78.63% macro-F1 score on the Laptop dataset, and 79.08% accuracy and 77.27% macro-F1 score on the Twitter dataset. These results demonstrate that incorporating intent analysis significantly enhances the model’s ability to capture complex sentiment expressions across different domains, thereby setting a new standard for aspect-level sentiment classification."
Eternal-MAML: a meta-learning framework for cross-domain defect recognition,"Basic reporting
This paper tackles the challenge of limited training samples in industrial defect detection by proposing Eternal-MAML, an enhanced MAML framework. It addresses label arrangement issues, integrates Squeeze-and-Excitation and Residual blocks for stability, and shows an 8% accuracy improvement over state-of-the-art methods on the MVTec dataset. Ablation studies further validate the framework's components. This work offers a promising solution for improving meta-learning models in industrial applications.

Experimental design
The experimental design is rational, covering the main needs of current industrial inspection while also demonstrating the superiority of the method through recognized evaluation criteria. The problem setup in the experiments is generally reasonable, reflecting the innovation and advantage of the method in this aspect. However, the description of the experimental methods and parameter settings should be more detailed, thoroughly describing the specific setup and corresponding results during the experimental process. Moreover, the lack of data description makes it difficult to ensure the reproducibility of the results. It is suggested to make the souce code public available on github.

Validity of the findings
The conclusions drawn in this paper effectively address the proposed issues and present viable solutions. Based on the experiments and their results, the method proposed in this paper exhibits good performance. However, the stability and generalizability of the method cannot be fully ascertained from the presentation in the paper. It is recommended to provide a more detailed description of the experimental data and an explanation of the corresponding experimental settings to enhance the persuasiveness and validity of the article.

Additional comments
The folloowing concerns should be addressed.
1.Clarification on the mechanism of vector W.
The manuscript mentions that the vector W improves the performance, but does not provide a clear explanation of how W guides the updates. I suggest adding a detailed description of the mechanism of W and its role in the model’s performance improvement. Providing a theoretical foundation or referencing similar studies would strengthen this section.
2.Unclear explanation in Lines 213-216?
The statement in lines 213-216 seems confusing and potentially misleading. It is unclear whether the issue arises from labeling of dataset, the MAML method, or a combination of both. I recommend revising this part to clarify the source of the problem. It would be helpful to provide more details and distinguish between these potential causes, supported by evidence from the text or relevant references.
3.Figure 1 does not explicitly show all modules.
The current version of Figure 1 does not clearly illustrate the SE module and residual connections. I suggest modifying the figure to explicitly depict these components or adding annotations that help the viewer intuitively understand the whole structure. This would enhance the clarity of the presentation.
4.Confusing definitions in Lines 268-274.
The definitions of “transfer learning” and “learning from scratch” presented in lines 268-274 are not entirely clear. Specifically, it is unusual to define transfer learning (TF) as training with a larger train set, and learning from scratch (LFS) as training with a smaller test set. Additionally, the explanation that LFS is initialized using pre-trained weights from ImageNet does not align with the traditional method of training from scratch. I suggest revising these definitions to align with standard usage in the literature and providing clearer justification for the choices made in the study.",Authentic,"Defect recognition tasks for industrial product suffer from a serious lack of samples, greatly limiting the generalizability of deep learning models. Addressing the imbalance of defective samples often involves leveraging pre-trained models for transfer learning. However, when these models, pre-trained on natural image datasets, are transferred to pixel-level defect recognition tasks, they frequently suffer from overfitting due to data scarcity. Furthermore, significant variations in the morphology, texture, and underlying causes of defects across different industrial products often lead to a degradation in performance, or even complete failure, when directly transferring a defect classification model trained on one type of product to another. The Model-Agnostic Meta-Learning (MAML) framework can learn a general representation of defects from multiple industrial defect recognition tasks and build a foundational model. Despite lacking sufficient training data, the MAML framework can still achieve effective knowledge transfer among cross-domain tasks. We noticed there exists serious label arrangement issues in MAML because of the random selection of recognition tasks, which seriously affects the performance of MAML model during both training and testing phase. This article proposes a novel MAML framework, termed as Eternal-MAML, which guides the update of the classifier module by learning a meta-vector that shares commonality across batch tasks in the inner loop, and addresses the overfitting phenomenon caused by label arrangement issues in testing phase for vanilla MAML. Additionally, the feature extractor in this framework combines the advantages of the Squeeze-and-Excitation module and Residual block to enhance training stability and improve the generalization accuracy of model transfer with the learned initialization parameters. In the simulation experiments, several datasets are applied to verified the cross-domain meta-learning performance of the proposed Eternal-MAML framework. The experimental results show that the proposed framework outperforms the state-of-the-art baselines in terms of average normalized accuracy. Finally, the ablation studies are conducted to examine how the primary components of the framework affect its overall performance. Code is available at https://github.com/zhg-SZPT/Eternal-MAML."
RMIS-Net: a fast medical image segmentation network based on multilayer perceptron,"Basic reporting
- The article's English is quite weak.
- The purpose of the article is unclear, additions should be made to the introduction section accordingly.
- The contributions of the article are listed. However, what purpose do these contributions provide? The contributions section should be detailed instead of giving one sentence each.
- The organization of the paper should be given at the end of the introduction section.
- ""Dice"" or ""DICE"", a single usage form should be preferred.
- What does RMIS stand for, its expansion should be given where it is first used.
-Explanations should be written in Figure 6. Text should be added to understand what each module is expressed in the figure.

Experimental design
- How were the C1, C2,..., C5 channel number values ​​determined in the proposed method.
- Before moving on to the subheading in section 4, information about the experimental conditions should be given. Under which conditions and with which parameters was the experiment performed?
- Why were horizontal flipping, rotation, and cutting applied as data augmentation methods? Why were different methods not preferred?

Validity of the findings
- What is the data partitioning ratio for training, testing, and validation according to the values ​​in Table 1?
- More detailed information should be provided about the metrics used.
- Can the channel number values ​​(Table 3) be varied?
- The conclusion section should be expanded.

Additional comments
The article titled ""RMIS-Net: A Fast Medical Image Segmentation Network Based on Multilayer Perceptron"" should be revised according to the following points.",Authentic,"Medical image segmentation, a pivotal component in diagnostic workflows and therapeutic decision-making, plays a critical role in clinical applications ranging from pathological diagnosis to surgical navigation and treatment evaluation. To address the persistent challenges of computational complexity and efficiency limitations in existing methods, we propose RMIS-Net—an innovative lightweight segmentation network with three core components: a convolutional layer for preliminary feature extraction, a shift-based fully connected layer for parameter-efficient spatial modeling, and a tokenized multilayer perceptron for global context capture. This architecture achieves significant parameter reduction while enhancing local feature representation through optimized shift operations. The network incorporates layer normalization and dropout regularization to ensure training stability, complemented by Gaussian error linear unit (GELU) activation functions for improved non-linear modeling. To further refine segmentation precision, we integrate residual connections for gradient flow optimization, a Dice loss function for class imbalance mitigation, and bilinear interpolation for accurate mask reconstruction. Comprehensive evaluations on two benchmark datasets (2018 Data Science Bowl for cellular structure segmentation and ISIC-2018 for lesion boundary delineation) demonstrate RMIS-Net’s superior performance, achieving state-of-the-art metrics including an average F1-score of 0.91 and mean intersection-over-union of 0.82. Remarkably, the proposed architecture requires only 0.03 s per image inference while achieving 27× parameter compression, 10× acceleration in inference speed, and 53× reduction in computational complexity compared to conventional approaches, establishing new benchmarks for efficient yet accurate medical image analysis."
SODU2-NET: a novel deep learning-based approach for salient object detection utilizing U-NET,"Basic reporting
This work proposes an DL-based model SODU2-NET for automated saliency detection using 92 residual blocks, and Atrous Spatial Pyramid Pooling (ASPP). It seems that the main contribution of this work lies in modified U-NET that adds the residual blocks inside the encoder architecture. Here, authors use multiple metrics to identify and compare the issue of SOD in old 107 models. However, the concept of the U-NET used for salient object detection in this paper have been used in many existing works, and the incorporation of these well-developed methods into the proposed work cannot be contribution to salient object detection. In addition, some suggestions for authors are as follows:
1. The introduction and related works must provide a critical evaluation of the models used for SOD in previous studies. The main contributions of this article and the differences with existing methods should be made clear in the Introduction and Related Work sections.
2. Authors are advised to present a critical discussion, not just a descriptive summary of the topic in literature review and other subsections.
3. Some of the figures are used from other literatures, which violating copy right rules. It would be better to re-create the figures and cite them properly.
4. Furthermore, the references should be arranged in the order of their appearance.

The contribution of the paper is not adequate. The whole paper needs significant improvement. The content of the paper is ambiguous. Introduction is not well written, and authors are unable to justify their contributions. Moreover, the results of the proposed work are not clear.

Experimental design
Experimental work is not enough to show advantages of the proposed work over existing techniques. It would better to discuss technical aspects of the proposed work. For example, What are the reasons that proposed work is giving better mean average precision, f1-score etc.?

Validity of the findings
the contribution of the paper is not adequate and it is not properly verified.",Authentic,"Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. To address this challenge posed by complex backgrounds in salient object detection is crucial for advancing the field. This article proposes a novel deep learning-based architecture called SODU2-NET (Salient object detection U2-Net) for salient object detection that utilizes the U-NET base structure. This model addresses a gap in previous work that focused primarily on complex backgrounds by employing a densely supervised encoder-decoder network. The proposed SODU2-NET employs sophisticated background subtraction techniques and utilizes advanced deep learning architectures that can discern relevant foreground information when dealing with complex backgrounds. Firstly, an enriched encoder block with full feature fusion (FFF) with atrous spatial pyramid pooling (ASPP) varying dilation rates to efficiently capture multi-scale contextual information, improving salient object detection in complex backgrounds and reducing the loss of information during down-sampling. Secondly the block includes an attention module that refines the decoder, is constructed to enhances the detection of salient objects in complex backgrounds by selectively focusing attention on relevant features. This allows the model to reconstruct detailed and contextually relevant information, which is essential to determining salient objects accurately. Finally, the architecture has been improved by adding a residual block at the encoder end, which is responsible for both saliency prediction and map refinement. The proposed network is designed to learn the transformation between input images and ground truth, enabling accurate segmentation of salient object regions with clear borders and accurate prediction of fine structures. SODU2-NET is demonstrated to have superior performance in five public datasets, including DUTS, SOD, DUT OMRON, HKU-IS, PASCAL-S, and a new real world dataset, the Changsha dataset. Based on a comparative assessment of the model FCN, Squeeze-net, Deep Lab, Mask R-CNN the proposed SODU2-NET is found and achieve an improvement of precision (6%), recall (5%) and accuracy (3%). Overall, approach shows promise for improving the accuracy and efficiency of salient object detection in a variety of settings."
"Fast2Vec, a modified model of FastText that enhances semantic analysis in topic evolution","Basic reporting
The document is written in professional English and provides a clear introduction and background. However, there are instances where sentence structure is overly complex, which may affect readability. Simplify some of the sentences to enhance clarity for a broader audience. For example, the sentence in the introduction, ""Without proper analysis, scientific data will remain meaningless raw information,"" could be rephrased as, ""Proper analysis is essential to transform raw scientific data into meaningful insights.""

Experimental design
The experimental design is described in detail, and the methods are replicable. However, there is limited explanation of how the dataset was cleaned and pre-processed before training models.
Improvement: Include a more detailed explanation of the data pre-processing steps, such as handling missing values, removing duplicates, or normalizing text, to provide better transparency and replicability.",Generic,"Background
Topic modeling approaches, such as latent Dirichlet allocation (LDA) and its successor, the dynamic topic model (DTM), are widely used to identify specific topics by extracting words with similar frequencies from documents. However, these topics often require manual interpretation, which poses challenges in constructing semantics topic evolution, mainly when topics contain negations, synonyms, or rare terms. Neural network-based word embeddings, such as Word2vec and FastText, have advanced semantic understanding but have their limitations. Word2Vec struggles with out-of-vocabulary (OOV) words, and FastText generates suboptimal embeddings for infrequent terms.

Methods
This study introduces Fast2Vec, a novel model that integrates the semantic capabilities of Word2Vec with the subword analysis strength of FastText to enhance semantic analysis in topic modeling. The model was evaluated using research abstracts from the Science and Technology Index (SINTA) journal database and validated using twelve public word similarity benchmarks, covering diverse semantic and syntactic dimensions. Evaluation metrics include Spearman and Pearson correlation coefficients to assess the alignment with human judgments.

Results
Experimental findings demonstrated that Fast2Vec outperforms or closely matches Word2Vec and FastText across most benchmark datasets, particularly in task requiring fine-grained semantic similarity. In OOV scenarios, Fast2Vec improved semantic similarity by 39.64% compared to Word2Vec, and 6.18% compared to FastText. Even in scenarios without OOV terms, Fast2Vec achieved a 7.82% improvement over FastText and a marginal 0.087% improvement over Word2Vec. Additionally, the model effectively categorized topics into four distinct evolution patterns (diffusion, shifting, moderate fluctuations, and stability), enabling a deeper understanding of evolution topic interests and their dynamic characteristics.

Conclusion
Fast2Vec presents a robust and generalizable word embedding framework for semantic-based topic modeling. By combining the contextual sensitivity of Word2Vec with the subword flexibility of FastText, Fast2Vec effectively addresses prior limitations in handling OOV terms and semantic variation and demonstrates strong potential for boarder applications in natural language processing tasks."
Enhancing phishing detection with dynamic optimization and character-level deep learning in cloud environments,"Basic reporting
1.The abstract should be improved. Your point is your own work that should be further highlighted.
2.Introduction seems to be incomplete. Please carefully check and supplement it.
3. More statistical methods are recommended to analyze the experimental results.
4. The article can be further enhanced by connecting the undergoing work with some existing literatures.
5. The numerical simulation verification is not convincing, and the actual engineering application example verification should be given.
6. There are a few typos and grammar errors in the manuscript.

Experimental design
As above

Validity of the findings
As above

Additional comments
As above",Generic,"As cloud computing becomes increasingly prevalent, the detection and prevention of phishing URL attacks are essential, particularly in the Internet of Vehicles (IoV) environment, to maintain service reliability. In such a scenario, an attacker could send misleading phishing links, potentially compromising the system’s functionality or, at worst, leading to a complete shutdown. To address these emerging threats, this study introduces a novel Dynamic Arithmetic Optimization Algorithm with Deep Learning-Driven Phishing URL Classification (DAOA-DLPC) model for cloud-enabled IoV infrastructure. The candidate’s research utilizes character-level embeddings instead of word embeddings, as the former can capture intricate URL patterns more effectively. These embeddings are integrated with a deep learning model, the Multi-Head Attention and Bidirectional Gated Recurrent Units (MHA-BiGRU). To improve precision, hyperparameter tuning has been done using DAOA. The proposed method offers a feasible solution for identifying the phishing URLs, and the method achieves computational efficiency through the attention mechanism and dynamic hyperparameter optimization. The need for this work comes from the observation that the traditional machine learning approaches are not effective in dynamic environments like phishing threat landscapes in a dynamic environment such as the one of phishing threats. The presented DLPC approach is capable of learning new forms of phishing attacks in real time and reduce false positives. The experimental results show that the proposed DAOA-DLPC model outperforms the other models with an accuracy of 98.85%, recall of 98.49%, and F1-score of 98.38% and can effectively detect safe and phishing URLs in dynamic environments. These results imply that the proposed model is useful in distinguishing between safe and unsafe URLs than the conventional models."
Enhancing healthcare data privacy and interoperability with federated learning,"Basic reporting
The source document is a paper undergoing peer review that discusses enhancing healthcare data privacy and interoperability with federated learning. The document includes an AI detection score of 29%, which suggests areas needing revision to ensure the content is original and adheres to academic standards.

Experimental design
The paper acknowledges that real data might be non-Independent and Identically Distributed (non-IID), which can significantly affect the results of CL and FL. To address this, the authors used the Synthetic Minority Over-sampling Technique (SMOTE) to synthesize data for classification tasks and Adaptive Synthetic Sampling (ADASYN) for regression problems. While using SMOTE and ADASYN is a good start, the authors should provide a more in-depth discussion on how these techniques mitigate the challenges posed by non-IID data in their specific context. They could also explore and compare other data synthesis techniques or partitioning strategies that are more robust to non-IID data.
The paper uses standard evaluation metrics such as Accuracy, F1-score, Kappa, and MCC for classification, and RMSE, MAE, R-squared, and MAPE for regression. The choice of evaluation metrics is appropriate, but the authors should justify their selection more explicitly. Additionally, they could consider including other metrics that are relevant to the specific healthcare applications they are addressing.
While comparing FL and CL is valuable, the authors could enhance their analysis by including additional baseline models or comparing against other federated learning algorithms. This would provide a more comprehensive understanding of the strengths and weaknesses of their proposed approach.
The experiments were simulated locally using only one dataset, and the data was divided into pairs of 60%/40%, 70%/30%, 80%/20%, and 90%/10% for training and testing.The authors should address the limitations of simulating the experiments locally. Running experiments on a distributed system with multiple devices would provide a more realistic evaluation of the performance of FL. They should also justify their choice of partition proportions and the number of clients used in the FL experiments.",Authentic,"This article explores the application of federated learning (FL) with the Fast Healthcare Interoperability Resources (FHIR) protocol to address the underutilization of the huge volumes of healthcare data generated by the digital health revolution, especially those from wearable sensors, due to privacy concerns and interoperability challenges. Despite advances in electronic medical records, mobile health applications, and wearable sensors, current digital health cannot fully exploit these data due to the lack of data analysis and exchange between heterogeneous systems. To address this gap, we present a novel converged platform combining FL and FHIR, which enables collaborative model training that preserves the privacy of wearable sensor data while promoting data standardization and interoperability. Unlike traditional centralized learning (CL) solutions that require data centralization, our platform uses local model learning, which naturally improves data privacy. Our empirical evaluation demonstrates that federated learning models perform as well as, or even numerically better than, centralized learning models in terms of classification accuracy, while also performing equally well in regression, as indicated by metrics such as accuracy, area under the curve (AUC), recall, and precision, among others, for classification, and mean absolute error (MAE), mean squared error (MSE), and root mean square error (RMSE) for regression. In addition, we developed an intuitive AutoML-powered web application that is FL and CL compatible to illustrate the feasibility of our platform for predictive modeling of physical activity and energy expenditure, while complying with FHIR data reporting standards. These results highlight the immense potential of our FHIR-integrated federated learning platform as a practical framework for future interoperable and privacy-preserving digital health ecosystems to optimize the use of connected health data."
Employing SAE-GRU deep learning for scalable botnet detection in smart city infrastructure,"Basic reporting
The authors have improved clarity by shortening complex sentences and reducing overly technical language​
A ""Significant Contributions"" section was added to highlight key contributions​
The structure conforms to PeerJ standards, and references are well-cited.
Figures are appropriately labeled, but Figure 4 could still benefit from a more descriptive caption.

Remaining Suggestions:
Ensure all figures have fully descriptive captions to improve readability.

Experimental design
The research question is well-defined, and the study now explicitly states how it addresses IoT security challenges​
The authors have clarified dataset choices (IoT-23 and MedBIoT) but should further elaborate on how these datasets represent real-world IoT botnet activity.
Methods are now described in more detail, including model pruning, weight quantization, and k-fold cross-validation

Validity of the findings
The authors have added details on cross-validation and performance metrics, strengthening the study’s statistical rigor​
.
The conclusions are now more closely linked to the research questions.
Feature importance techniques like SHAP values have been added to validate detection accuracy

Additional comments
The revised manuscript is significantly improved, with a clearer focus and stronger justifications for methodology choices",Authentic,"The proliferation of Internet of Things (IoT) devices in smart cities has revolutionized urban infrastructure while escalating the risk of botnet attacks that threaten essential services and public safety. This research addresses the critical need for intrusion detection and mitigation systems by introducing a novel hybrid deep learning model, Stacked Autoencoder–Gated Recurrent Unit (SAE-GRU), specifically designed for IoT networks in smart cities. The study targets the dual challenges of processing high-dimensional data and recognizing temporal patterns to identify and mitigate botnet activities in real time. The methodology integrates Stacked Autoencoders for reducing dimensionality and gated recurrent units for analyzing sequential data to ensure both accuracy and efficiency. An emulated smart city environment with diverse IoT devices and communication protocols provided a realistic testbed for evaluating the model. Results demonstrate significant improvements in detection performance with an average accuracy of 98.65 percent and consistently high precision and recall values. These findings enhance the understanding of IoT security by offering a scalable and resource-efficient solution for botnet detection. The functional investigation establishes a foundation for future research into adaptive security mechanisms that address emerging threats and highlights the practical potential of advanced deep learning techniques in safeguarding next-generation smart city ecosystems."
Comparative evaluation of approaches & tools for effective security testing of Web applications,"Basic reporting
The study mainly determines the effectiveness of SAST and DAST tools in finding OWASP Top
10:2021 and CWE Top 25:2023 vulnerabilities in web apps. For this purpose, 4 SAST and 5
DAST tools were selected to test 75 real-world web applications from technology, health and
education domains. The limitations of previous studies are highlighted in “Related Work” but it
is not mentioned that whether these limitations (some or all) are resolved in current study or not.

Experimental design
This work is specifically testing web applications but the RQs are just about the effectiveness of
DAST and SAST tools in finding OWASP Top 10 and CWE Top 25 vulnerabilities. Authors did
not mention about the web applications in any of the 4 RQs.
Need to add some details about “Report” column in Table 4. Which report it is representing?
Except 2, all SAST and DAST tools were deployed on Windows 10. Why these 2 tools were
deployed on Kali Linux. Any specific reason for using Linux for these tools?

Validity of the findings
In Results section, while answering “Research Question 1” authors have written “six out of nine
risk categories” while in “Research Question 2” it is “six out of ten”. Which one is correct?
In Table 8, the limitations of this work are not added.",Authentic,"It is generally accepted that adopting both static application security testing (SAST) and dynamic application security testing (DAST) approaches is vital for thorough and effective security testing. However, this suggestion has not been comprehensively evaluated, especially with regard to the individual risk categories mentioned in Open Web Application Security Project (OWASP) Top 10:2021 and common weakness enumeration (CWE) Top 25:2023 lists. Also, it is rare to find any evidence-based recommendations for effective tools for detecting vulnerabilities from a specific risk category or severity level. These shortcomings increase both the time and cost of systematic security testing when its need is heightened by increasingly frequent and preventable incidents. This study aims to fill these gaps by empirically testing seventy-five real-world Web applications using four SAST and five DAST tools. Only popular, free, and open-source tools were selected and each Web application was scanned using these nine tools. From the report generated by these tools, we considered two parameters to measure effectiveness: count and severity of the vulnerability found. We also mapped the vulnerabilities to OWASP Top 10:2021 and CWE Top 25:2023 lists. Our results show that using only DAST tools is the preferred option for four OWASP Top 10:2021 risk categories while using only SAST tools is preferred for only three risk categories. Either approach is effective for two of the OWASP Top 10:2021 risk categories. For CWE Top 25:2023 list, all three approaches were equally effective and found vulnerabilities belonging to three risk categories each. We also found that none of the tools were able to detect any vulnerability in one OWASP Top 10:2021 risk category and in eight CWE Top 25:2023 categories. This highlights a critical limitation of popular tools. The most effective DAST tool was OWASP Zed Attack Proxy (ZAP), especially for detecting vulnerabilities in broken access control, insecure design, and security misconfiguration risk categories. Yasca was the best-performing SAST tool, and outperformed all other tools at finding high-severity vulnerabilities. For medium-severity and low-severity levels, the DAST tools Iron Web application Advanced Security testing Platform (WASP) and Vega performed better than all the other tools. These findings reveal key insights, such as, the superiority of DAST tools for detecting certain types of vulnerabilities and the indispensability of SAST tools for detecting high-severity issues (due to detailed static code analysis). This study also addresses significant limitations in previous research by testing multiple real-world Web applications across diverse domains (technology, health, and education), enhancing generalization of the findings. Unlike studies that rely primarily on proprietary tools, our use of open-source SAST and DAST tools ensures better reproducibility and accessibility for organizations with limited budget."
Enhancing cybersecurity through autonomous knowledge graph construction by integrating heterogeneous data sources,"Basic reporting
- The English language should be improved so that an international audience can clearly understand your text. The current phrasing makes comprehension difficult. Some examples are
- l35: it's -> it has
- l38: Where small
- l51: there are different sources -> different source
- l90: ""lack of language understanding"" -> vague information
- l99: different types of formats -> different formats
- l104: Cite LPG
- l131: KG ""are"" highly useful
- l146: which types .... be tackled
- l160: did not have consistency or accuracy -> vague (clarify and cite)
- l171-174: many previous works mentioned but not cited
- l186: cite previous works
- l188: cite previous works
- l203: unclear whether it is 95% or 80% -> rephrase
- l221: high: vague (quantify)
- l234: Reasoning Function: vague (clarify)
- l469: named??
- l535: John managed to open -> John managed to open the door
- l682,687: Use the same representation of spaCy or Spacy
- l689: Proper names or proper nouns
- l713: CWE documents mentioned twice
- l763: somewhat long sentences -> vague (rephrase or give statistics)
- l817: manually examined -> not clear what this means
- l838: is it two or three, the table shows three measures
- l892 these lists cannot be directed answered -> clarify which lists
- l894: run the questions -> answer instead of run?
- l910: Json-based model -> model or dataset
- l912: edges not existed -> do not exist

- The Introduction and background are well written and show enough relevant articles which build up the context for the research work
- The structure conforms to the standards
- Figures are relevant and helped me understand the texts better. However, in some figures, it is hard to see the contents of the nodes of the graph (for eg, in Fig 25, relations in Fig 26,27, and Nodes in Fig 28). A table containing all node names should be provided and then referenced to the image

Experimental design
- The design of experiments and research questions are relevant and well-defined.
- Multiple experiments and investigations performed which validate the claims
- Most of the methods are clear. I have some clarifying questions which should be addressed in the revision to make the approach clear.
- A table for the different types of data should be provided for readers to have a clear understanding of the heterogeneous data sources. I suggest having a table with fields like source (dataset name), nature (structured or unstructured or both), volume of data points, fields (authors used or present in the data if structured))
- l545 -> What patterns created? Please clarify in detail
- For sections f (l550), g(l560), h(l574), i(l585), and j(l598) use examples from your data to illustrate instead or using general natural language examples so that the readers have clear understanding on the data used.
- It is not clear to me how the disambiguation is done since the entities usually extracted by pre-trained models are different that specific cyber-security entities.
- I suggest giving a table for evaluation (Line 871) on how many questions asked and how many answered to qualify the results. Use accuracy or other metrics to measure

Validity of the findings
- The overall approach is novel and has a potential impact, especially the KG extension part
- The created CKG will be helpful for future cyber-security
- The conclusion is well stated supporting the results and linked to the original research questions",Authentic,"Cybersecurity plays a critical role in today’s modern human society, and leveraging knowledge graphs can enhance cybersecurity and privacy in the cyberspace. By harnessing the heterogeneous and vast amount of information on potential attacks, organizations can improve their ability to proactively detect and mitigate any threat or damage to their online valuable resources. Integrating critical cyberattack information into a knowledge graph offers a significant boost to cybersecurity, safeguarding cyberspace from malicious activities. This information can be obtained from structured and unstructured data, with a particular focus on extracting valuable insights from unstructured text through natural language processing (NLP). By storing a wide range of cyber threat information in a semantic triples form which machines can interpret autonomously, cybersecurity experts gain improved visibility and are better equipped to identify and address cyber threats. However, constructing an efficient knowledge graph poses challenges. In our research, we construct a cybersecurity knowledge graph (CKG) autonomously using heterogeneous data sources. We further enhance the CKG by applying logical rules and employing graph analytic algorithms. To evaluate the effectiveness of our proposed CKG, we formulate a set of queries as questions to validate the logical rules. Ultimately, the CKG empowers experts to efficiently analyze data and gain comprehensive understanding of cyberattacks, thereby help minimize potential attack vectors."
The pivotal role of software defined networks to safeguard against cyber attacks: a comprehensive review,"Basic reporting
The manuscript is a well-structured and informative which address one of the most critical issues in modern cybersecurity. Clearly, the authors made good use of conciseness, making the material accessible to a wider audience. The introduction provides a solid foundation, clearly describing the problem and the proposed solution. The literature review is comprehensive, focuses on relevant research and demonstrates an in-depth understanding of the topic. The structure of the manuscript is consistent with academic standards, allowing the ideas to flow logically. The topic of network security, especially in software-defined networks, is of great interest to researchers, practitioners, and policymakers. This manuscript contributes to the ongoing discourse by providing new perspectives and valuable insights for further research.

Experimental design
Regarding the study design, the content of the manuscript is constant with the aim and scope of the journal. The authors achieved rigorous investigation of studies while adhering to excessive technical and ethical requirements. The literature review is comprehensive, properly-cited, and logically prepared into coherent paragraphs and subsections. The manuscript contributes to the sphere by means of inspecting a well-timed and applicable trouble, presenting treasured insight into the capability of software program-defined networks to enhance cybersecurity.

Validity of the findings
Once the findings are well explained, related to the motivations of the research, and based on supporting results, the paper would benefit from a thorough analysis of the impact and relevance of the findings If the contribution of the paper to the existing literature will be explicitly mentioned, which will enhance its relevance. In addition, identifying unresolved issues or future research directions will provide valuable directions for future studies in this area.

Additional comments
Here are some typos to improve manuscript quality:
• The introduction section is written in one long paragraph, it is better to divide it into two paragraphs and add a brief paragraph at the beginning of the introduction that gives the readers an idea about the importance of SDN in the field of networking. Also, I think there is no need to make a citation for the purpose of the paper in the fourth line of the introduction.
• It is preferable to change the title of Table3 to ""Existing work in SDN Cybersecurity field ""instead of ""Existing work in this field"".
• Although the language is clear, the grammar should be reviewed and verified.",Generic,"Software defined networks (SDNs) offer novel approaches to managing networks by separating the control plane from the data plane to enable programmable control over network resources effectively and dynamically. This framework supports monitoring of traffic flow and detection of threats while also enabling easy adaptation of network configurations, which is critical in safeguarding against cyber threats. However, this separation also brings forth security risks that cyber attackers may exploit. In this examination, the basic concepts of SDN are explained, pointing out their benefits compared to conventional networks and exploring the security issues that are part of SDN architectures. Different types of threats that focus on SDN layers are categorized and how they impact network security while suggesting different ways to address them. Furthermore, the review highlights issues and suggests potential research paths to enhance SDN security measures and ensure their effectiveness against ever-changing cyber dangers."
Graph neural networks embedded with domain knowledge for cyber threat intelligence entity and relationship mining,"Basic reporting
The English used in this paper is both professional and fluent, and extensive research has been conducted on related work. The structure of the article is well-organized, and the experimental results are detailed and consistent with the hypotheses. However, the first challenge it faces is questionable, as there are some flaws in the chart drawing and some formulas in the formal expression are not standardized enough.

Experimental design
The methods detailed in this paper provide sufficient information to replicate the experiments. However, the research questions are not appropriate. Regarding challenge (1), the author argues that current Named Entity Recognition (NER) methods overlook contextual information, whereas BERT, a well-established approach, takes such information into account and is considered a classic method for NER. For challenge (3), the Globalpointer algorithm is a one-stage method that can address these issues.

Validity of the findings
The experimental method applied in this paper is appropriate, the data presented is detailed, and it effectively addresses the problems raised by the author.

Additional comments
The issues that need to be addressed include:
（1）The proposed approach in the paper appears to be a combination of existing methods. For challenges 1 and 3, there are already established methods to address them, thus making the novelty ambiguous.
（2）In the abstract, the authors mention “Finally, the Globalpointer algorithm is employed to recognize relationships between threat entities and entities.” It’s unclear whether the relationship is identified only between threat and benign entities, or between all entities.
（3）In Table 4, the bold formatting should indicate the highest value for the same metric across different models for a specific category of entities. However, the bold formatting only highlights the highest value in this column, which includes multiple metrics and may make the different metrics non-comparable.
（4）There are some typos and unclear points. For instance, in Equation (8), should “f_MIP” be ”f_MLP”? In line 334 of Algorithm 1, is it correct that X_{new_b}<=（X_b-μ）/sqr(σ)? In line 359 of Algorithm2, what’s the meaning of axis = -2? In the 357 of Algorithm2, “ operatio”->”operation”? Additionally, there misses an equation operation in Equation (13).
（5）Some redundant abbreviations, such as Cyber Threat Intelligence (CTI) in the Introduction section.
（6）Some figures are too large and should be appropriately adjusted, such as Fig. 2, Fig. 10-12.",Authentic,"The escalating frequency and severity of cyber-attacks have presented formidable challenges to the safeguarding of cyberspace. Named Entity Recognition (NER) technology is utilized for the rapid identification of threat entities and their relationships within cyber threat intelligence, enabling security researchers to be promptly informed of the occurrence of cyber threats, thereby enhancing the efficiency of security defense and analysis. However, current models for identifying network threat entities and extracting relationships suffer from limitations such as the inadequate representation of textual semantic information, insufficient granularity in threat entity recognition, and errors in relationship extraction propagation. To address these issues, this article proposes a novel model for Network Threat Entity Recognition and Relationship Extraction (CtiErRe). Additionally, it redefines seven network threat entities and two types of relationships between threat entities. Specifically, first, domain knowledge is collected to build a domain knowledge graph, which is then embedded using graph convolutional networks (GCN) to enhance the feature representation of threat intelligence text. Next, the features from domain knowledge graph embedding and those generated by the bidirectional encoder representations from transformers (BERT) model are fused using the Layernorm algorithm. Finally, the fused features are processed using the GlobalPointer algorithm to generate both the threat entity type matrix and the threat entity relation type matrix, thereby enabling the identification of threat entities and their relationships. To validate our proposed model, we conducted extensive experiments, and the results demonstrate its superiority over existing models. Our model performs remarkably in threat entity recognition tasks, with accuracy and F1 scores reaching 92.13% and 93.11%, respectively. In the relationship extraction task, our model achieves accuracy and F1 scores of 91.45% and 92.45%, respectively."
Detecting malicious code variants using convolutional neural network (CNN) with transfer learning,"Basic reporting
It is well-written, with a clear and professional tone. However, some minor grammatical inconsistencies should be reviewed for refinement.

The authors provide an adequate background on malicious code detection and the role of CNN with transfer learning. However, additional references to recent works (e.g., Li et al., 2023; Khan et al., 2024) on adversarial robustness in malware detection could strengthen the discussion.

It mentions datasets used.

Experimental design
-It contributes to CNN-based malware detection using transfer learning, aligning with the journal’s scope.
- Clearly stated. However, the hypothesis testing would be strengthened by comparing against non-deep-learning approaches (e.g., traditional feature-based classifiers).

Validity of the findings
The presented accuracy and loss values are reasonable, but statistical validation (e.g., confidence intervals, significance tests) should be included.

Additional comments
The paper provides a meaningful contribution to the field of malware detection using deep learning. Addressing the recommended improvements will strengthen its impact and reproducibility.",Generic,"Malware presents a significant threat to computer networks and devices that lack robust defense mechanisms, despite the widespread use of anti-malware solutions. The rapid growth of the Internet has led to an increase in malicious code attacks, making them one of the most critical challenges in network security. Accurate identification and classification of malware variants are crucial for preventing data theft, security breaches, and other cyber risks. However, existing malware detection methods are often inefficient or inaccurate. Prior research has explored converting malicious code into grayscale images, but these approaches are often computationally intensive, especially in binary form. To address these challenges, we propose the Malware Variants Detection System (MVDS), a novel technique that transforms malicious code into color images, enhancing malware detection capabilities compared to traditional methods. Our approach leverages the richer information in color images to achieve higher classification accuracy than grayscale-based methods. We further improve the detection process by employing transfer learning to automatically identify and classify malware images based on their distinctive features. Empirical results demonstrate that MVDS achieves 97.98% accuracy with high detection speed, highlighting its potential for practical implementation in strengthening network security."
Federated learning with LSTM for intrusion detection in IoT-based wireless sensor networks: a multi-dataset analysis,"Basic reporting
The writing is clear and professional, effectively addressing all previous concerns. The background and context in IoT wireless sensor networks and intrusion detection systems are well-established. The article is well-structured, with sufficient literature references, and the datasets and preprocessing steps ensure reproducibility.

Experimental design
This paper presents original research that fits within the journal's Aims and Scope. The research question is clearly defined and relevant, addressing a significant knowledge gap in IoT security. The methods are described in sufficient detail for replication.

Validity of the findings
The impact and novelty of the findings have been adequately assessed in this revision. The authors have included baseline models for comparison and showed the effectiveness of the proposed model. The conclusions drawn are well stated and directly linked to the original research question, remaining within the bounds of the supporting results. Overall, the findings are robust and statistically sound.

Additional comments
Overall, the paper is well-written and presents a novel approach to intrusion detection. The authors have successfully addressed all previous comments, and I have no further concerns.",Generic,"Intrusion detection in Internet of Things (IoT)-based wireless sensor networks (WSNs) is essential due to their widespread use and inherent vulnerability to security breaches. Traditional centralized intrusion detection systems (IDS) face significant challenges in data privacy, computational efficiency, and scalability, particularly in resource-constrained IoT environments. This study aims to create and assess a federated learning (FL) framework that integrates with long short-term memory (LSTM) networks for efficient intrusion detection in IoT-based WSNs. We design the framework to enhance detection accuracy, minimize false positive rates (FPR), and ensure data privacy, while maintaining system scalability. Using an FL approach, multiple IoT nodes collaboratively train a global LSTM model without exchanging raw data, thereby addressing privacy concerns and improving detection capabilities. The proposed model was tested on three widely used datasets: WSN-DS, CIC-IDS-2017, and UNSW-NB15. The evaluation metrics for its performance included accuracy, F1 score, FPR, and root mean square error (RMSE). We evaluated the performance of the FL-based LSTM model against traditional centralized models, finding significant improvements in intrusion detection. The FL-based LSTM model achieved higher accuracy and a lower FPR across all datasets than centralized models. It effectively managed sequential data in WSNs, ensuring data privacy while maintaining competitive performance, particularly in complex attack scenarios. FL and LSTM networks work well together to make a strong way to find intrusions in IoT-based WSNs, which improves both privacy and detection. This study underscores the potential of FL-based systems to address key challenges in IoT security, including data privacy, scalability, and performance, making the proposed framework suitable for real-world IoT applications."
Efficient unified architecture for post-quantum cryptography: combining Dilithium and Kyber,"Basic reporting
The author primarily researched the integrated hardware architecture of the post-quantum cryptographic digital signature scheme Dilithium and the KEM scheme Kyber. In the paper, the author proposed optimizations in modular multipliers, data flow, and scheduling. Finally, the design's implementation results and comparisons on FPGA were presented.

The article is written in fluent and clear language, making it easy to understand. The research terminology is used accurately, and the study includes a thorough literature review of the research background. The detailed description of the specific components of both algorithms is commendable.

Experimental design
The author claims to have proposed various optimization techniques and specific implementation methods to support the hardware design. However, I believe there are still several issues that need to be clarified:

1. Dual-port RAM is a common design in Kyber and Dilithium hardware implementations. The memory bit-width and data computation width should align accordingly. The author should specify the parallelism of the polynomial computation units corresponding to the bandwidth and how it meets the design requirements.

2. The butterfly unit is the fundamental computation structure in NTT. The structure claimed by the author employs extensive pipelining but shows no significant difference from conventional butterfly units in its specific implementation. The author should clarify how this structure ensures compatibility with both Kyber and Dilithium simultaneously.

3. The polynomial sampling unit is equally important in the overall hardware implementation. However, the author only briefly introduces the basic sampling process without presenting the related hardware implementation results, such as the hardware designs of Keccak and SHA-3, as well as the different sampling structures for Kyber and Dilithium.

4. The overlapping of multi-sampling and NTT computation is a commonly used technique in Kyber and Dilithium hardware designs. The author should elaborate on the advantages of the proposed design compared to prior designs in the literature.

Validity of the findings
In the implementation results and comparisons, the author used two different FPGAs for diverse data comparisons and drew some conclusions. However, I still have questions that need clarification:

1. In Figure 5, the author indicates that areas in different colors represent different components, and the compress component occupies a significant portion of the area. However, the compression process in Kyber only constitutes a small part, with low hardware resource requirements. The author should provide a reasonable explanation for this discrepancy.

2. The author's design demonstrates certain advantages in frequency, showing improved frequency performance compared to other designs. The author should specify which key optimizations and techniques contributed to this improvement in frequency performance.",Authentic,"As the ongoing standardization process of post-quantum schemes yields initial outcomes, it becomes increasingly important to not only optimize standalone implementations but also explore the potential of combining multiple schemes into a single, unified architecture. In this article, we investigate the combination of two National Institute of Standards and Technology (NIST)-selected schemes: the Dilithium digital signature scheme and the Kyber key encapsulation mechanism. We propose a novel set of optimization techniques for a unified hardware implementation of these leading post-quantum schemes, achieving a balanced approach between area efficiency and high performance. Our design demonstrates superior resource efficiency and performance compared to previously reported unified architecture (DOI 10.1109/TCSI.2022.3219555), also achieving results that are better than, or comparable, to those of standalone implementations. The efficient and combined implementation of lattice-based digital signatures and key establishment methods can be deployed for establishing secure sessions in high-speed communication networks at servers and gateways. Moreover, the unique and compact design that requires small hardware resources can be directly used in small and cost-effective field programmable gate array (FPGA) platforms that can be used as security co-processors for embedded devices and in the Internet of Things."
Modeling and implementation of a real-time digital twin for the Stewart platform with real-time trajectory computation,"Basic reporting
Professional English Usage:
The manuscript maintains a professional tone and is generally well-written. However, a few minor grammatical errors and awkward phrasings remain. For example, in the introduction, the phrase ""the trustworthiness of a digital twin is a critical factor influencing its effectiveness"" could be restructured for better clarity.

Literature References and Background:
The authors have done an excellent job providing relevant literature and contextual background for their research. The inclusion of new citations and restructuring of explanations for key terms such as ""Jacobian matrix"" and ""inverse kinematics"" improves accessibility for a broader audience.

Figures and Data Presentation:
The authors have addressed my comments regarding figure descriptions. The clarification of color coding in Figure 5 and the addition of axis units in Figures 16-18 enhance clarity. The additional explanation of Figure 22 outlining the three-layer resilience architecture strengthens the manuscript's technical depth.

Self-Containment and Hypothesis Alignment:
The manuscript is self-contained and aligns well with the stated research objectives. However, the conclusions could still benefit from a more explicit discussion on how the study’s findings align with the hypothesis.

Experimental design
Originality and Scope:
The study aligns well with the journal's aims and scope, offering an innovative approach to integrating resilience into digital twins for Stewart platforms.

Research Question and Knowledge Gap:
The research question is well-defined, and the manuscript effectively highlights how it contributes to filling a knowledge gap. The discussion on resilience in digital twins and the integration of adaptive feedback mechanisms strengthens the manuscript’s relevance.

Methodological Rigor and Replicability:
The revised manuscript provides additional details on how data flows between the physical and virtual components. The clarification regarding network communication protocols (TCP/IP) and the use of structured databases for storing and refining model parameters improves transparency and replicability. The explanation of MQTT implementation for real-time communication is a valuable addition.

Validity of the findings
Data and Statistical Soundness:
The authors have responded to concerns regarding validation but should further strengthen their presentation of results demonstrating resilience. The paper primarily focuses on building a digital twin but does not provide extensive quantitative results showing how resilience is improved.

Conclusions and Justification:
While the discussion on resilience mechanisms has been expanded, additional experimental results showcasing the effectiveness of these mechanisms would further solidify the claims. The proposed future integration of neural networks for enhanced resilience is promising, but more evidence from simulations or tests would strengthen the conclusions.

Additional comments
The explanation of how disruptions were introduced (e.g., deactivating Stewart platform actuators) is much clearer now. However, numerical results demonstrating system recovery times or performance improvements would be beneficial.

The discussion section includes future work on integrating AI techniques, such as graph convolutional neural networks, to enhance resilience. While valuable, a brief feasibility assessment or preliminary results would provide additional support for this proposed direction.",Authentic,"The concept of a digital twin is increasingly acknowledged as an innovative and promising tool with significant potential in various end-use applications. At the heart of digital twin technology is the acquisition of real-time data from physical entities. However, the occurrence of disturbances necessitates the incorporation of resilience features within the digital twin architecture. The primary objective of this article is to develop resilient digital twins specifically for the Stewart platform. This work focuses on constructing the virtual component of the digital twin using MATLAB/Simulink and subsequently integrating this virtual model with its physical counterpart to establish a comprehensive digital twin system. Unlike other models, this system includes a motion trajectory computation module. This module is designed to receive signals from physical entities and convert them into motion trajectory data for input into the model, thereby aiming to accurately reflect the state of the physical entities under disruptive conditions. This functionality significantly enhances the reliability of the system beyond that of traditional digital twin systems. Furthermore, the article explores novel strategies and a framework for enhancing the resilience of the Stewart platform to disturbances."
Intersection collision prediction and prevention based on vehicle-to-vehicle (V2V) and cloud computing communication,"Basic reporting
The manuscript proposes a deep learning-based model for a specific prediction task. It discusses the model architecture, data preprocessing steps, and experimental results. The author claims significant improvements over existing methods and thoroughly analyses the model's performance. However, some aspects could be strengthened to enhance the work's clarity and robustness, particularly in areas related to model architecture, hyperparameter tuning, and evaluation metrics.
1. You mention using a ""deep learning network"" for prediction in Chapter 2, but it is unclear which specific network architecture was used. To enhance clarity, I recommend expanding on the architecture in the ""Model Architecture"" section by specifying the type of network used, detailing each layer's configuration activation functions, and providing a structure diagram. This will help readers understand the model design.

Experimental design
2. In Section 3, you mention standardization but do not provide enough detail on the methodology or parameter selection. I suggest expanding this section by specifying how standardization was performed using the mean and standard deviation of the training set, whether missing values were imputed, and how outliers were handled.
3. The model training section lacks any mention of hyperparameter tuning. I recommend discussing hyperparameter optimization in the ""Model Training"" section, describing the methods used (such as grid search or random search), and specifying the ranges of hyperparameters explored.

Validity of the findings
4. Currently, you only use accuracy as an evaluation metric, which may not fully capture the model's performance, especially in imbalanced datasets. I recommend adding other evaluation metrics such as F1-score, AUC, and Precision-Recall curves, especially when dealing with classification tasks, to provide a more comprehensive assessment of the model's performance.
5. The manuscript does not provide sufficient information about the experimental setup, hardware configurations, or random seeds, which makes reproducing the experiments difficult. I suggest adding this information in the ""Experimental Setup"" section, including details of the hardware platform used, the framework version, and the random seed to ensure reproducibility.
6. The manuscript does not compare with other models or traditional methods. To validate the superiority of the proposed model, I recommend adding comparison experiments with at least two traditional machine learning methods (such as SVM XGBoost) and presenting their performance in terms of accuracy, F1-score, etc.",Authentic,"In modern transportation systems, the management of traffic safety has become increasingly critical as both the number and complexity of vehicles continue to rise. These systems frequently encounter multiple challenges. Consequently, the effective assessment and management of collision risks in various scenarios within transportation systems are paramount to ensuring traffic safety and enhancing road utilization efficiency. In this paper, we tackle the issue of intelligent traffic collision prediction and propose a vehicle collision risk prediction model based on vehicle-to-vehicle (V2V) communication and the graph attention network (GAT). Initially, the framework gathers vehicle trajectory, speed, acceleration, and relative position information via V2V communication technology to construct a graph representation of the traffic environment. Subsequently, the GAT model extracts interaction features between vehicles and optimizes the vehicle driving strategy through deep reinforcement learning (DRL), thereby augmenting the model’s decision-making capabilities. Experimental results demonstrate that the framework achieves over 80% collision recognition accuracy concerning true warning rate on both public and real-world datasets. The metrics for false detection are thoroughly analyzed, revealing the efficacy and robustness of the proposed framework. This method introduces a novel technological approach to collision prediction in intelligent transportation systems and holds significant implications for enhancing traffic safety and decision-making efficiency."
Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios,"Basic reporting
All comments have been added in detail to the last section.

Experimental design
All comments have been added in detail to the last section.

Validity of the findings
All comments have been added in detail to the last section.

Additional comments
Review Report for PeerJ Computer Science
(Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios)

1. Within the scope of the study, a reinforcement learning based deep learning model was proposed to increase the quality of communication in complex communication environments and various channel estimation studies were carried out.

2. In the introduction, intelligent reflective surface technology and the importance of the subject with communication technologies were mentioned at a basic and sufficient level. In addition, the main contributions of the study were stated clearly and in bullet points.

3. In the Related works section, the literature related to the study was discussed in terms of both channel estimation based on deep learning methods and intelligent reflective surface and joint sensing channel estimation. Although the literature on the subject was mentioned in this section, a more in-depth analysis should be made especially in terms of deep learning methods. In this section, it is suggested to add a detailed literature table so that the proposed model can come to the forefront more.

4. In the Methodology section, both gated recurrent unit networks and convolutional neural networks based on deep learning were mentioned at a basic but sufficient level.

5. When the framework and details of the proposed CRPG-Net model are examined and compared with the literature, it is observed that it has a certain level of originality in this study.

6. Examining the types of metrics used in the study and the results obtained accordingly, it is understood that they are both at an acceptable level. In addition, comparing the obtained results with some models in the literature and demonstrating their superiority further increases the quality of the study.

As a result, the study has the potential to present an important deep learning-based model to the literature in terms of channel estimation model. However, attention should be paid to the above sections.",Authentic,"In contemporary wireless communication systems, channel estimation and optimization have become increasingly pivotal with the growing number and complexity of devices. Communication systems frequently encounter multiple challenges, such as multipath propagation, signal fading, and interference, which may result in the degradation of communication quality, a reduction in data transmission rates, and even communication interruptions. Therefore, effective estimation and optimization of channels in complex communication environments are of paramount importance to ensure communication quality and enhance system performance. In this article, we address the intelligent, reflective surface (IRS)-assisted channel estimation problem and propose an intelligent channel estimation model based on the fusion of convolutional neural network (CNN) and gated recurrent unit (GRU) row features, utilizing the reinforcement learning Deep Deterministic Policy Gradient (DDPG) strategy for Channel Reconstruction Prediction and Generation Network (CRPG-Net). The framework initially acquires the received signal by converting the guide-frequency symbols at the transmitter into time-domain sequences to be transmitted, and after propagating through the direct channel and the IRS reflection channel, processes the data at the receiver. Subsequently, the spatial and temporal features in the received signal are extracted using the CRPG-Net model, with the adaptive optimization capability of the model enhanced by deep reinforcement learning. The introduction of reinforcement learning enables the model to continuously optimize decisions in dynamic channel environments, improve the robustness of channel estimation, and quickly adjust the IRS reflection parameters when the channel state changes to adapt to complex communication conditions. Experimental results demonstrate that the framework achieves significant channel estimation accuracy and robustness across several public datasets and real test scenarios, with the channel estimation error markedly smaller than that of traditional least squares (LS) and linear minimum mean square error (LMMSE) methods. This method introduces innovative techniques for channel estimation in intelligent communication systems, playing a crucial role in enhancing communication quality and overall system performance."
Isolation and identification of endophytic fungi from Conyza blinii that exhibit antioxidant and antibacterial activities,"Basic reporting
The English language should be improved to ensure that an international audience can clearly understand your text. I suggest you have a colleague who is proficient in English and familiar with the subject matter review your manuscript, or contact a professional
editing service.
In the Introduction the authors stated ""This study might potentially facilitate the future use of bioactive compounds generated by C. blinii endophytic fungi in the food or medicinal sectors"". Detailed explanations for these perspectives should be provided.
The Discussion section needs more details. Relevant and recent studies on the isolated and selceted fungi should be cited and discussed. Particular attention must be given to the metabolites produced by these fungi.
The quality of the figures must be increased.
Different decimals are reported in Table 5 the authors should report the same number of decimals.

Experimental design
Why the auhtors decided to work on endophytic fungi of C. blinii? They stated ""At present, the researches on endophytic fungi of C. blinii are few and single."" What does it means? Is there a single research or few? The relative references are missing.

Validity of the findings
From table six in the supporting information it can be seen that the masses were also recorded in positive mode but only the results of negative mode were reported. What about all the other compounds produced by the fungi? Fusarium spp. are able to produce micotoxins and it is important to understand if the selected fungi are also producers of micotoxins. This will help to propose them as producers of compounds with potential medicinal value.
The retention times reported in Table 5 are not coincident with those reported in Supplementary Figure 3.
Were internal standards used for the quantification of the metabolites present in the different extracts?
Are the authors certain that all the metabolites were produced by fungi or are some of them the result of contamination? For example 4-dodecylbenzenesulfonic acid is a synthetic strong anionic surfactant and it's a key ingredient in many household and industrial detergents. Are the authors sure that in not a contaminat?
The conclusions need to be implemented by adding future perspectives on the practical application of these fungi, of thier extracts and of their pure metabolites.

Additional comments
Line 2: ""higher""? I suggest to delete it.
Line 22-23: Why it necesseray to report this sentence ""As a medicinal plant, Conyza blinii may contain a wealth of bioactive constituents""?
Line 49: ""Conyza blini"" should be ""Conyza blinii"".
Line 136: ethyl n-butanol???",Generic,"Background
As a medicinal plant, Conyza blinii is known to contain a wealth of bioactive constituents, including flavonoids, terpenes, and triterpenoid saponins, which contribute to its anti-inflammatory and anticancer properties. Endophytic fungi, which symbiotically inhabit plant tissues, are recognized for their ability to synthesize bioactive metabolites analogous to those of their hosts. However, the potential of C. blinii-associated endophytes remains underexplored. This study aims to isolate and characterize phenols-producing endophytic fungi from C. blinii, evaluate their biological activities, and analyze their chemical components to provide new insights for drug development.

Methods
During the study, 20 endophytic fungi were isolated from C. blinii. The Folin-Ciocalteu method was used to screen for strains capable of producing phenolic compounds. To assess their bioactivity, ethyl acetate extracts of different concentrations were tested for antibacterial and antioxidant activities. Antibacterial activity was evaluated using minimum inhibitory concentration (MIC) determinations, while antioxidant activity was assessed through 2,2-Diphenyl-1-picrylhydrazyl (DPPH) radical, 2,2′-Azinobis-(3-ethylbenzthiazoline-6-sulfonic acid) (ABTS) radical, hydroxyl radical, and superoxide anion radical scavenging assays. Additionally, liquid chromatography-mass spectrometry analysis was conducted to quantify the active components in the extracts.

Results
Among the isolated 20 endophytic fungi, four strains successfully produced phenolic compounds, with the highest total phenolic content of 77.17 ± 1.93 mg milligrams of gallic acid equivalents per gram of extract (GAE/g). All ethyl acetate extracts from the endophytic fungi exhibited good antibacterial and antioxidant properties. Notably, Fusarium circinatum demonstrated exceptional antioxidant activity, with scavenging rates for DPPH and ABTS radicals reaching 94.28 ± 0.042% and 96.60 ± 0.017%, respectively. The ethyl acetate extract of F. foetens showed remarkable antibacterial effects against Escherichia coli and Staphylococcus aureus, with MIC values as low as 0.5 mg/mL. Furthermore, liquid chromatography-mass spectrometry (LC-MS) analysis revealed that F. foetens could produce various high-value phenolic compounds, including tyrosol (626.1884 ng/mL) and homovanillic acid (369.15486 ng/mL), which hold potential pharmaceutical value.

Discussion
This study isolated 20 endophytic fungi from C. blinii, discovering that four strains, produced phenolic compounds with strong antioxidant and antimicrobial properties. Among them, F. circinatum exhibited the highest antioxidant activity. Additionally, the fungi produced bioactive metabolites with potential applications in health care, medicine, and agriculture. These findings highlight the potential of C. blinii endophytes for sustainable bioactive compound production."
Effects of gerbil disturbance on the ecological stoichiometric characteristics and nutrient uptake and utilization of H. ammodendron,"Basic reporting
1. Basic Reporting
The report is well written, but many grammatical errors need linguistic review.

Experimental design
2. Experimental design
Materials & Methods
- Study area
It is preferable to put coordinates in place to define the study area better.
- Experimental design and analysis process
Clear

Validity of the findings
4. Validity of the findings
clear and correct
5. Tables and figures
clear and effective

Additional comments
6. General comments
The whole paper needs an English review.
7. Confidential notes to the editor ammodendron.
None",Generic,"Rodent activity is an important factor that affects the growth and development of Haloxylon ammodendron. Studying the effect of rodent disturbance on plant ecological stoichiometric ratios helps evaluate the mechanism by which rodent disturbance affects plant growth and development. In this study, H. ammodendron, a dominant plant, and the gerbil, a typical rodent in the Gurbantunggut Desert, were selected as research objects. By measuring the biomass, root soil , and C: N: P ecostoichiometric ratios of the assimilated branches of H. ammodendron at different growth phases, the impact of great gerbil disturbance on the biomass, ecostoichiometric ratios, and nutrient uptake and use of H. ammodendron were investigated at different growth stages. The results showed that the gerbil disturbance increased the biomass of the aboveground part of the adult H. ammodendron. Gerbil disturbance also increased the soil N/P around the roots during the growth stage and the assimilation branch when the plants were middle-aged. In addition, this disturbance decreased the C/N value. The photosynthetic nitrogen use efficiency (PNUE) and photosynthetic phosphorus use efficiency (PPUE) of H. ammodendron during various growth periods decreased, and the absorption of total nitrogen (TN) in the soil decreased. However, soil total potassium (TK) absorption increased. The soil TN absorption capacity was weakened by gerbil disturbance. Meanwhile, the TK absorption capacity was enhanced, and the biomass of adult H. ammodendron increased. PNUE and PPUE of H. ammodendron were decreased by gerbil interference. In this study, the influence of gerbil disturbance on nutrient absorption by H. ammodendron and use of H. ammodendron was determined. This has provided a baseline for further studies on the coexistence mechanisms of gerbils and H. ammodendron."
"Biomass allocation, carbon content change and carbon stock distribution of Scots pine (Pinus sylvestris var. mongholica) plantation forests at different stand ages and densities in the sandy area of western Liaoning Province, China","Basic reporting
The article is very well prepared and contains the results studied in detail. Afforestation efforts and their success, especially in stressful areas, are of vital importance. Examining both the success and carbon sequestration potential of such afforestation will shed light on the applications. The article contains important results in these aspects and can be recommended for acceptance.

The tree specifically studied in the article is a variety and there are some errors in its Latin spelling. It will be sufficient to write the Latin name once and correctly. Its English name is stated as ""Pine"". Instead of this common name, the common name of the specific variety must be written. Throughout the text, it would be appropriate to continue with the specific common name only after specifying the common name (Latin name) in the Abstract section.

Experimental design
Throughout the article, there are references such as ""Chen Zheng et al. showed that the highest carbon stocks 79 are expected in northeastern China by 2060 (Chen et al., 2024)."" Instead of writing the references in this way, it would be more appropriate to write them as ""Chen Zheng et al. (2024) showed that the highest carbon stocks are expected in northeastern China.""

Research question is well defined, relevant & meaningful. However, the aim of the study may clearly been writen just after the hypotheses.

Validity of the findings
The results and analysis looks meaningful and include important comparisons from different age classes and land features.

Additional comments
This paper may be accepted after making some minor corrections",Authentic,"Scots pine (Pinus sylvestris var. mongholica) is one of the main afforestation species in the southeastern edge of the Horqin Sandy Land, which not only effectively prevents the expansion of the sandland, but also serves as an important carbon reservoir. Uncovering the biomass allocation, carbon content changes and carbon stock distribution among organs of Scots pine at different ages and densities can provide a theoretical basis for rational afforestation and management in the western Liaoning sandy area. In this study, the biomass and carbon content of four organs, namely, trunk, branch, leaf and root, were measured at different age classes (young stage, half-mature stage, near-mature stage, mature stage and over-mature stage forests) and densities, and the carbon stock of Scots pine plantations in the western Liaoning sandy area was estimated. The results showed that the biomass of all organs except leaves increased with the increase of stand age, but the rate of increase of each organ was not consistent. To resist wind and sand, the biomass was preferentially allocated to the trunk and roots, which was in line with the theory of allometry and optimal allocation. The carbon content of each organ of Scots pine increases and then decreases with the rise of forest age classes, and the root carbon content is the lowest in five forest ages, and the plant carbon is mainly stored in the aboveground part. The biomass of each organ in both near mature and mature forests increased with the decrease in density. Still, the root carbon content decreased with the decrease of density, and the PCA analysis showed that near mature and mature forests had better carbon sequestration capacity in low density. The carbon stock of Scots pine plantation forests in the sandy area of western Liaoning was mainly concentrated in Fuxin and Chaoyang cities, and the lowest carbon stock was found in Jinzhou. The age and density of the forest stand are important factors affecting the biomass and carbon content of Scots pine, therefore, when operating Scots pine plantation forests in the sandy areas of western Liaoning, different stand densities should be retained at different age stages, so that their biomass and carbon content can be sufficiently accumulated and distributed to improve the local environment."
Mass harvested per trunkload as a constraint to forage consumption by the African savanna elephant (Loxodonta africana),"Basic reporting
The writing is clear and unambiguous. The literature utilised is correct and sufficient. Structure is correct. Results link to hypotheses.

Experimental design
Experimental design is excellent. research questions are clear. Very rigorous investigation. Methods described well.

Validity of the findings
Findings are solid, robust and sound. Conclusions well stated.

Additional comments
In this paper, the authors used mechanistic models to estimate how mass harvested per trunkload of different vegetation types (e.g. grass, forbs, woody vegetation) varied between seasons, and males and breeding herds. They found that harvestable mass varied seasonally, and bulls harvested larger trunkloads compared to breeding herds across all vegetation types. In addition, elephants obtained trunkload masses of green grass that were >75 times larger than other grazers, and trunkload masses of woody vegetation that were 8 times larger than other browsers. This is not too surprising (but still very cool) as elephants are unique in this comparison in being able to use their trunks to obtain food and not having to rely on mouth width, teeth, tongues, or lips to take bites. In addition, I really liked the fact that the authors could quantify when the transitions would and should take place by determining when the harvestable mass of a preferred food source (e.g. green grass) declines to the point where it overlaps with the mass available next preferred food source (e.g. green browse). This provides a really nice example of how herbivores expand their diet breadth. Moreover, the similar patterns of forage selection between the authors’ models and typical elephants foraging patterns highlights elephants’ preference for green grass. Understanding this, the authors suggest that in areas with a high availability of green grass into the dry season would reduce elephant foraging and thus impacts on woody vegetation. Overall, I really enjoyed the paper. It was a real pleasure to read. It is well written, easy to follow, and timely. Moreover, the results will be helpful to anyone working on elephant foraging. Due to the high quality of the manuscript I cannot think of any adjustments that would improve the manuscript. This is the first time that this has happened, so congratulations to the authors!

Prof Adrian M Shrader
University of Pretoria",Authentic,"African elephants can convert woodland to shrubland or grassland. Moderate conversion observed at low elephant densities may improve conditions for other animals, while extensive transformation at high densities may reduce plant and animal diversity. The threshold density separating facilitation from habitat destruction varies spatially and is partly determined by food choice, which differs between adult bulls and members of breeding herds. When elephants consume herbaceous forage, woodland damage is low but this increases when woody plants are the primary food source. Consequently, an understanding of diet selection by elephants is important for forecasting the degree of vegetation conversion. One hypothesis is that elephants select forage that provides the highest rate of intake. The mass harvested per trunkload is a constraint to intake and therefore this study sought to determine if trunkload mass changes seasonally; varies across common forage types utilised by elephants; and differs between adult bulls and members of breeding herds.

Methods
Mechanistic models were used to estimate the mass harvested per trunkload of green grass, mixed green and dry grass, forbs, and leaves and bark from woody plants across a heterogenous, semi-arid savanna at a daily time step for one annual cycle. Separate models were constructed for adult bulls and members of breeding herds.

Results
Harvestable mass changed seasonally for herbaceous forage and for leaves from woody plants but was constant for canopy bark. The maximum average trunkload mass of green grass was >75 times heavier than the bite mass reported for other grazers while trunkloads of leaves from woody plants were only eight times heavier than the bite mass reported for other browsers. This is attributed to the advantage provided by the trunk, which increases harvestable mass beyond the constraint of mouth volume, particularly when feeding on grass. Herbaceous forage yielded heavier trunkloads than leaves and bark from woody plants during the wet season, but this was reversed in the dry season. Adult bulls harvested heavier trunkloads than members of breeding herds for all forage types except forbs; and adult bulls harvested disproportionately large trunkloads of grass and bark.

Conclusion
The strong correlation between the model outputs and well-established trends in the seasonal changes in elephants’ diet suggests that elephants are preferential foragers of the largest trunkload on offer. Consequently, they are grazers when suitable herbaceous forage is available, and browsers when it is scarce. Green grass provides adult bulls with disproportionately large trunkloads and, therefore, adult bulls are predicted to have a strong preference for green grass. Availability of suitable green grass during the dry season may therefore buffer woodlands from heavy impact by adult bulls. Consequently, where possible, protected areas with elephants should aim to include key grass resources."
Use of integrated population models for assessing density-dependence and juvenile survival in Northern Bobwhites (Colinus virginianus),"Basic reporting
This paper is well written. It includes sufficient and appropriate refences. Although there were ample supplemental figures and explanations provided, I did not see links to data or code, but I presume/hope the authors will provide those at a later date.

Experimental design
I have no concerns about experimental design. The study is especially impressive for having maintained consistent and rigorous data collection for so many years in a row.

Validity of the findings
There was nothing to make me question the validity of the findings.",Generic,"Management of wildlife populations is most effective with a thorough understanding of the interplay among vital rates, population growth, and density-dependent feedback; however, measuring all relevant vital rates and assessing density-dependence can prove challenging. Integrated population models have been proposed as a method to address these issues, as they allow for direct modeling of density-dependent pathways and inference on parameters without direct data. We developed integrated population models from a 25-year demography dataset of Northern Bobwhites (Colinus virginianus) from southern Georgia, USA, to assess the demographic drivers of population growth rates and to estimate the strength of multiple density-dependent processes simultaneously. Furthermore, we utilize a novel approach combining breeding productivity and post-breeding abundance and age-and-sex ratio data to infer juvenile survival. Population abundance was relatively stable for the first 14 years of the study but began growing after 2012, showing that bobwhite populations may be stable or exhibit positive population growth in areas of intensive management. Variation in breeding and non-breeding survival drove changes in population growth in a few years; however, population growth rates were most affected by productivity across the entire study duration. A similar pattern was observed for density-dependence, with relatively stronger negative effects of density on productivity than on survival. Our novel modeling approach required an informative prior but was successful at updating the prior distribution for juvenile survival. Our results show that integrated population models provide an attractive and flexible method for directly modeling all relevant density-dependent processes and for combining breeding and post-breeding data to estimate juvenile survival in the absence of direct data."
Buscando Luciérnagas: findings on Mexican fireflies from an 8-year virtual citizen science project,"Basic reporting
I have revised both the manuscript and the rebuttal letter. My intention was to see whether authors followed those reasonable suggestions so that the new version looks improved. My general impression is that authors did a great job in crafting their work both by accepting those criticisms that made sense as well as adding or removing information they realized was or was not needed.

Given the above, my opinion is that this paper looks great now and ready to be accepted.

Experimental design
This is report of data collected by citizens who watched fireflies (mainly in Mexico) and sent such information via Facebook. This approach is valid.

Validity of the findings
Validity is 100% as authors filtered and analyzed the sightings good enough to sustain their conclusions. I particularly like the data where fireflies are linked to land use as this is very novel. This should serve as an initiative to promote firefly conservation.",Authentic,"Fireflies are charismatic and conspicuous animals that often evoke childhood memories, which make firefly watching an emotional and even transformative experience. Citizen science projects have the potential to enhance transformative interactions with nature. Like many insects, firefly populations are declining due to land-use change, urbanization and watershed pollution, but ecological data for this group is scarce, particularly in Mexico. Virtual Citizen Science (VCS) initiatives can serve as a scientific instrument, yield reliable and relevant scientific data, and may also offer a platform to promote broader educational outcomes. We established a VCS project to document fireflies through a Facebook page named Buscando Luciernagas with the following hashtag in every post #veobrillar in 2015. After seven years we complied the gathered data and analyzed the results. We had 647 reports in total, with strong fluctuations from year to year that were correlated with the number of posts and publicity we made each year. The largest number of sightings (319) occurred in 2021, coinciding with a change in our reporting format. Most of the reports came from central Mexico (91.5%), but we had reports from eight states and also received some international reports from nine different countries. Fireflies were most frequently seen in habitats characterized as grasslands (35%) or forests (27%), followed by gardens (17%), vacant lots (9%) and parks (5%) but also paved areas and agricultural lands were reported (3% each). Most citizen scientists reported few fireflies, 1–5 individuals (31%) while only 11% reported more than 50 fireflies per sighting. Our study can serve as a preliminary approach to explore more focused research areas in the future. For example, in areas with no sightings, we could reach out to specific local people to corroborate that there are no fireflies in the region, or in areas with high sightings we could promote conservation measures. Notably, we found it intriguing to discover numerous sightings of fireflies in urban areas, which could offer a potential avenue for further research in urban ecology."
Tree regeneration and ontogenetic strategies of northern European hemiboreal forests: transitioning towards closer-to-nature forest management,"Basic reporting
The article by Raimundas Petrokas, Michael Manton and Darius Kavaliauskas is devoted to the review of ideas on discrete description of tree ontogeny and tree regeneration strategies for the development of closer-to-nature forest management strategies. This review is of broad interest and within the scope of the journal. The introduction presents in detail and adequately the topic of the review and its relevance for a wide range of specialists related to both forest theory and practice.

Experimental design
The presentation of results and discussion are greatly improved by the authors. Links between different parts of the review have been added. Information on existing concepts has been added and new sources discussed. The quality of the text has been improved so that it can be understood by specialists from different disciplines.

Validity of the findings
The conclusion and abstract are generally consistent with the content of the review, identifying opportunities for future directions in forest management in relation to more fully utilizing knowledge of tree species biology. The authors have improved the linkage of the conclusion to the discussion.

Additional comments
The article may be recommended for publication.",Generic,"Background
Tree ontogeny is the genetic trajectories of regenerative processes in trees, repeating in time and space, including both development and reproduction. Understanding the principles of tree ontogeny is a key priority in emulating natural ecological patterns and processes that fall within the calls for closer-to-nature forest management. By recognizing and respecting the growth and development of individual trees and forest stands, forest managers can implement strategies that align with the inherent dynamics of forest ecosystem. Therefore, this study aims to determine the ontogenetic characteristics of tree regeneration and growth in northern European hemiboreal forests.

Methodology
We applied a three-step process to review i) the ontogenetic characteristics of forest trees, ii) ontogenetic strategies of trees for stand-forming species, and iii) summarise the review findings of points i and ii to propose a conceptual framework for transitioning towards closer-to-nature management of hemiboreal forest trees. To achieve this, we applied the super-organism approach to forest development as a holistic progression towards the establishment of natural stand forming ecosystems.

Results
The review showed multiple aspects; first, there are unique growth and development characteristics of individual trees at the pre-generative and generative stages of ontogenesis under full and minimal light conditions. Second, there are four main modes of tree establishment, growth and development related to the light requirements of trees; they were described as ontogenetic strategies of stand-forming tree species: gap colonisers, gap successors, gap fillers and gap competitors. Third, the summary of our analysis of the ontogenetic characteristics of tree regeneration and growth in northern European hemiboreal forests shows that stand-forming species occupy multiple niche positions relative to forest dynamics modes.

Conclusions
This study demonstrates the importance of understanding tree ontogeny under the pretext of closer-to-nature forest management, and its potential towards formulating sustainable forest management that emulates the natural dynamics of forest structure. We suggest that scientists and foresters can adapt closer-to-nature management strategies, such as assisted natural regeneration of trees, to improve the vitality of tree communities and overall forest health. The presented approach prioritizes ecological integrity and forest resilience, promoting assisted natural regeneration, and fostering adaptability and connectivity among plant populations in hemiboreal tree communities."
Association of metallic elements with telomere length in children with autism spectrum disorder,"Basic reporting
I am satisfied with the answers provided by authors.

Experimental design
Authors explained restrictions concerning experimental design.
I accept their explanation.

Validity of the findings
Revised text is good and I accept authors explanation.

Additional comments
No additional comments",Generic,"Background
Imbalances in metal elements have been identified as a potential risk factor for autism spectrum disorder (ASD), and shortened telomere length (TL) is commonly observed in children with ASD. Metal elements may influence telomere homeostasis through oxidative stress, which could contribute to the pathogenesis of autism. However, studies examining the combined effects of metal elements on TL in children with ASD are limited. To fill the gaps in the current literature, this study aimed to investigate the relationship between six metallic elements: manganese (Mn), copper (Cu), zinc (Zn), calcium (Ca), magnesium (Mg), and iron (Fe), and TL in the whole blood of children with ASD.

Methods
A total of 83 children with ASD and 95 typically developing children were recruited. TL was measured using digital PCR, while metal concentrations were assessed using inductively coupled plasma mass spectrometry (ICP-MS). Linear regression analysis was first conducted to explore the correlations between metal elements and TL in both groups. Additionally, Bayesian Kernel Machine Regression (BKMR) was used to further examine the combined effects and potential interactions of these metals on TL in the ASD group.

Results
In the ASD group, Ca was found to have a protective effect on TL (β = 0.07, 95% CI [0.01–0.13], P = 0.027). In contrast, Mg showed a protective effect on TL in the control group (β = 0.10, 95% CI [0.01–0.18], P = 0.027). The BKMR model revealed a significant positive combined effect of the metal mixtures on TL in the ASD group, with Ca having the largest individual effect (PIP = 0.45). Further analysis indicated that increases in Zn and Mn concentrations from the 25th to the 75th percentile were negatively correlated with TL, while higher concentrations of Cu, Ca, Mg, and Fe were positively associated with TL. No significant interactions among the metals were observed.

Conclusions
This study suggests a potential link between metallic elements and TL in children with ASD, with Ca having the greatest effect. Our findings highlight the potential benefits of appropriate calcium supplementation as a protective strategy for lengthening telomeres in children with ASD, emphasizing the importance of early nutritional interventions to improve their overall health."
"The Italian version of the extended Barcelona Music Reward Questionnaire (eBMRQ): a validation study and association with age, gender, and musicianship","Basic reporting
The authors have a done thorough job in providing the missing details and have patiently explained and expanded the issues raised.

Experimental design
Nothing extra to report here, the authors have addressed the issues raised and fixed the minor omissions.

Validity of the findings
All the issues have been addressed and the choices about the factor interpretations and age groupings have been explained in a satisfactory way.

Additional comments
I thank the authors for their diligent and valuable work on this contribution music scholarship and it is great to see careful and systematic validation and expansion of the tools to other languages and cultural contexts.",Generic,"Background
Music is a primary source of pleasure for humans. Nevertheless, there is large interindividual variability in how individuals experience and derive pleasure from music and music-related activities. With this study we propose and validate the Italian version of the extended Barcelona Music Reward Questionnaire (eBMRQ), the most in-depth and comprehensive tool for investigating the diverse characterization of individual sensitivity to pleasure in music. In addition, we aim to investigate eBMRQ scores as a function of age, gender, and musicianship across Italian population.

Methods
For the validation process of the Italian eBMRQ, we first conducted forward and backward translation from the original English eBMRQ version. The new Italian version was then administered to 1,012 participants who were fluent in Italian from the north and the south of Italy through online surveys (age range 18–86 years old; M = 34.9, SD = 16.9, females 74%). Unrestricted confirmatory analysis was computed for both six-factor and single-factor models. The effect of gender, age, and musicianship on eBMRQ scores was analyzed through analysis of variance (ANOVA).

Results
The quality assessment of the factor solution indicated that the Italian eBMRQ demonstrated acceptable quality and reliability, making it a valid tool for assessing sensitivity to music reward. All factors were significantly correlated with each other, in line with previous adaptations of the BMRQ. Our findings indicate that females reported higher music reward sensitivity compared to males, except for Social Reward subscale. Moreover, individual reward sensitivity was significantly higher among musicians and amateurs compared to non-musicians, although this trend did not emerge for Sensory-motor and Mood Regulation subscales. Also, overall musical reward sensitivity was negatively associated with age.

Conclusions
The results obtained suggest the feasibility of applying the Italian version of eBMRQ as a reliable tool in the field of affective and clinical music-related research. Furthermore, the significant associations we have highlighted between eBMRQ scores, gender, age, and musicianship contribute to emphasizing the significant impact of individual factors on music reward sensitivity."
Identification of bacteria on Thai banknotes and coins using MALDI-TOF mass spectrometry and their phenotypic antimicrobial susceptibility profiles,"Basic reporting
In this study, the authors investigated bacterial contaminants on banknotes and coins using MALDI-TOF Mass Spectrometry and determined antimicrobial susceptibility testing (AST) in selected isolates. While the manuscript presents interesting data, several areas require clarification and improvement before further consideration.
1. The authors should specify the criteria for selecting the 150 banknotes and coins used in the study.
2. Additional samples from other sources, such as vegetable and grocery stores, should be included to provide more comprehensive insights into the prevalence of medically important bacterial pathogens.
3. I recommend using phosphate-buffered saline (PBS) instead of TSB to resuspend the swab.
4. Bacterial identification should be performed directly from the plates used for colony counting to enhance the accuracy of results.
5. The authors reported a higher contamination rate on banknotes than on coins, possibly due to differences in size or surface area. However, the comparison between banknotes and coins may not be justified. Instead, a comparison of contamination levels across different collection sites for both banknotes and coins would be more informative.
6. The study focuses solely on bacterial contamination. The authors should also examine fungal pathogens to provide a more comprehensive analysis.
7. Based on the results, the variation among bacterial contaminants appears minimal. Further clarification is needed to support this conclusion.
8. References should be included throughout the methodology section, and detailed information about the database used for bacterial identification should be provided.

Experimental design
The experimental design requires improvement.

Validity of the findings
Validity of the findings looks good to me

Additional comments
NA",Authentic,"Background
The existence and transmission of pathogenic and antibiotic-resistant bacteria through currency banknotes and coins poses a global public health risk. Banknotes and coins are handled by people in everyday life and have been identified as a universal medium for potentially microbial contamination.

Methods
To ascertain existence of medically important bacteria, a total of 300 samples including 150 banknotes and 150 coins were randomly collected at onsite retail fresh meat stores, i.e., pork and chicken, fish, and seafood stores, from nineteen fresh markets distributed across Bangkok, Thailand. An individual banknote or coin was entirely swabbed, and bacterial culture was carried out using tryptic soy agar (TSA), sheep blood agar (SBA) and MacConkey agar (Mac). A colony count was performed and bacterial species identification was conducted using matrix-assisted laser desorption/ionization (MALDI)-time of flight (TOF) mass spectrometry. Phenotypic antimicrobial susceptibility testing was carried out using the Kirby–Bauer disc diffusion methods.

Results
The results demonstrated that the bacterial contamination rate was higher on banknotes than on coins (93.33% vs. 30.00%) in all three store types. A substantial number of colonies of >3,000 colony forming units (CFU) was predominantly found in banknotes (70.00%), especially from fish store (83.3%); meanwhile, <1,000 CFU was observed in coin sample (76.67%). MALDI-TOF mass spectrometry could identify 107 bacterial species, most of them were Staphylococcus kloosii (14.02%, 15/107), Staphylococcus saprophyticus (12.15%, 13/107), and Macrococcus caseolyticus (8.41%, 9/107). The prevalence based on genera were Staphylococcus (36.45%, 39/107), Acinetobacter (20.56%, 22/107), and Macrococcus (10.28%, 11/107). Almost all Staphylococcus isolates had low susceptibility to penicillin (21%). Notably, Staphylococcus arlettae, Staphylococcus haemolyticus and M. caseolyticus were multidrug-resistant (MDR). It is notable that none of the staphylococci and macrococci isolates exhibited inducible clindamycin resistance (D-test negative). Escherichia coli and Pseudomonas putida isolates were carbapenem-resistant, and Acinetobacter baumannii isolates were MDR with showing carbapenem resistance.

Conclusion
Our data demonstrated a high prevalence of medically important bacteria presented on Thai currency, which may pose a potential risk to human health and food safety. Food vendors and consumers should be educated about the possible cross-contamination of bacteria between the environment, food item, and currency."
Selective enrichment of active bacterial taxa in the Microcystis associated microbiome during colony growth,"Basic reporting
1) I am not so convinced of the often-used terms “Microcystis and its microbiome” or “microbiome of Microcystis”. This is because Microcystis is a microbe, too, and it doesn’t really have a microbiome like the human gut or Daphnia for example… Personally I find “Microcystis associated microbiome” or something similar more fitting.
2) Taxonomy: L 171and onward – Why did you reclassify Burkholderiales as Beta? I think Silva is now based on the revised taxonomy where Beta don’t exist anymore. From silva: “With SILVA release 138 the Genome Taxonomy Database (GTDB) has been adopted. As a consequence of our efforts the following groups were prone to significant adaptations: Archaea, Enterobacterales, Deltaproteobacteria, Firmicutes, Clostridia. Betaproteobacteriales (formerly known as Betaproteobacteria) is now Burkholderiales, an order of Gammaproteobacteria. Epsilonproteobacteria vanishes within a new phylum Campilobacterota. Tenericutes are gone, they are now all part of Bacilli, inside Firmicutes. “ (for more info see https://www.arb-silva.de/documentation/faqs/)
I think just reclassifying a single group is really wrong. If the authors don’t agree with this new taxonomy I guess they should use an old taxonomy but then completely commit to that one since there are many groups with new taxonomic names in the text. Just changing one group is a bit arbitrary.
3) I could not evaluate data sharing since the sequences are not public

small suggestions for the text:
L 116 – filtered onto ?
L 121 – for RNA later I don’t think the ® is need, maybe it would be better to mention the producer.
L 136 – Toxic cell abundance
L 153 - onward, also here I would remove the ® and TM it seems a bit arbitrarily put just for some products.
L 159 – BP not pb
L 174 – the sequencing data is not public so I couldn’t check.
L 180 & 197 - there is some problem with ( -diversity)
Fig 5 – where are the unclassified genera? Are they in the <5% or removed from the graph?

Experimental design
L 138 – Please give more detailes on qPCR, such as machine, quantification system and standard curve consruction. Also how was specificity and inhibition evaluated.
L 141 – copy number per what?

Validity of the findings
no comment

Additional comments
The authors have analysed their data in an appropriate and very interesting way attributing changes in the community structure to different ecological processes. This study was clearly well planned to analyse exactly what they did. In my opinion the study is basically flawless and the results are very interesting. Graphs are comprehensive and aesthetically pleasing. The article is well written, and of appropriate length. It was a pleasure to review this work.",Authentic,"The toxic cyanobacterium Microcystis causes worldwide health concerns, being frequently found in freshwater and estuarine ecosystems. Under natural conditions, Microcystis spp. show a colonial lifestyle involving a phycosphere populated by a highly diverse associated microbiome. In a previous study, we have proposed that colony formation and growth may be achieved through mechanisms of multispecies bacterial biofilm formation. Starting with single-cells, specific bacteria would be recruited from the environment to attach and create a buoyant biofilm or colony. This progression from a few single cells to large colonies would encompass the growth of the Microcystis community and bloom formation. In order to test this, we applied 16S rDNA metabarcoding to evaluate the changes in bacterial community structure (gDNA) and its active portion (cDNA) between different sample sizes obtained from a Microcystis bloom. Bloom sample was sieved by size, from one or a few cells (U fraction) to large colonies (maximum linear dimension ≥ 150 µm; L fraction), including small (20–60 µm, S fraction) and medium size (60–150 µm, M fraction) colonies. We found that gDNA- and cDNA-based bacterial assemblages significantly differed mostly due to the presence of different taxa that became active among the different sizes. The compositional variations in the communities between the assessed sample sizes were mainly attributed to turnover. From U to M fractions the turnover was a result of selection processes, while between M and L fractions stochastic processes were likely responsible for the changes. The results suggest that colony formation and growth are a consequence of mechanisms accounting for recruitment and selection of specific bacterial groups, which activate or stop growing through the different phases of the biofilm formation. When the final phase (L fraction colonies) is reached the colonies start to disaggregate (bloom decay), few cells or single cells are released and they can start new biofilms when conditions are suitable (bloom development)."
Performance comparison of QuantiFERON-TB Gold In-Tube and QuantiFERON-TB Gold Plus in detecting Mycobacterium tuberculosis infection among HIV patients in China,"Basic reporting
Line 115: Mycobacterium Vaccae >> Mycobacterium vaccae
Line 124: Mycobacterium tuberculosis >> Mycobacterium tuberculosis italicize species names. Do these throughout the entire manuscript

Line 218: A review and meta-evaluation revealed a 1.3% increase in sensitivity for QFT-Plus compared to QFT-GIT. Authors need to specifically mention whether the review and meta-evaluation were done in China? Because they indicated that this is the first of such studies in China.

Line 278: In conclusion, the diagnosis performance of QFT-GIT and QFT-Plus across PLHIV with relatively higher CD4 count for Mycobacterium tuberculosis infection was comparable >> In conclusion, the diagnostic performance of QFT-GIT and QFT-Plus for detecting Mycobacterium tuberculosis infection among PLHIV with relatively higher CD4 counts was comparable.

Experimental design
The original primary research's aims follow within the journal's scope.
The research question is well-defined, relevant & meaningful. The authors stated how their research fills an identified knowledge gap. The authors demonstrated that they have performed rigorous investigations to a high technical and ethical standard.

The authors described methods with sufficient detail and information to replicate.

Validity of the findings
The authors demonstrate impact and novelty in this study.

Conclusions are well-stated and linked to the original research question.",Authentic,"Introduction
No direct comparative study assessing QuantiFERON-TB Gold In-Tube (QFT-GIT) and QuantiFERON-TB Gold Plus (QFT-Plus) for Mycobacterium tuberculosis infection among persons living with HIV (PLHIV) in China has been conducted.

Methods
Simultaneous QFT-GIT and QFT-Plus tests were conducted on PLHIV in a prison hospital. Positivity and negativity results from both assays were compared, and their diagnostic agreement was assessed.

Results
A total of 232 PLHIV individuals were included in this study. Among them, 57 patients (24.6%) and 56 patients (24.1%) were diagnosed with Mycobacterium tuberculosis infection based on QFT-GIT results and QFT-Plus, respectively. The overall agreement between the two assays was 98.3%, with a Cohen’s kappa value of 0.954. Consistency rates were observed between QFT-GIT plus, QFT-Plus TB1 and TB2 with QFT-GIT were 98.3%, 97.4% and 97.8%. The IFN-γ levels measured in QFT-GIT were found to surpass those in QFT-Plus TB1 (P = 0.04), while the difference compared to QFT-Plus TB2 exhibited a marginal trend (P = 0.134). Among the subgroup of 52 individuals who underwent dual QFT-GIT tests, a significant proportion of 23.1% (12 individuals) experienced a change in their QFT-GIT results, transitioning from a positive to a negative outcome.

Conclusions
The diagnostic performance of QFT-GIT and QFT-Plus for Mycobacterium tuberculosis infection among PLHIV with relatively higher CD4 counts was found to be comparable. Additionally, our investigation revealed that irrespective of the treatment regimen, whether it involved chemotherapy or immunotherapy, preventive Mycobacterium tuberculosis infection interventions among PLHIV consistently led to a reduction in IFN-γ levels."
A proof-of-concept point-of-care test for the serodiagnosis of human amebic liver abscess,"Basic reporting
I am not native speaker; therefore I am not considered myself an authority to correct the language.
The introduction and the backgrounds are short, but they contain the necessary information to introduce the need and description of the study.
It accomplished with the Peer J. standards
It only contains one figure. It is well described, and I consider is enough to describe the design of the lateral flow system.

The authors dont mention clearly the source of the EhHK9 strain

Experimental design
This work covers topics related to health science and I consider it covers the scope of the journal.
The authors highlight the relevance of the research as they describe the need of having the development of the CT platform necessary for the diagnostic of ALA, which is a neglected disease in undeveloped countries.
The author presents the complete results without hiding any of them that can compromise the final development and commercialization of the platform.
The methods describe in detail the components of the CT platform, with the specification to obtain replicates.

Validity of the findings
The study present original results, it is not replication or any other study.
The authors present the data that support the study. They report that the total number of samples analyzed are 253, out of them, 13 are positive to ALA. I consider this number is mall and from this mall number they don’t report false positives, and 40 negative samples that also don’t present false negatives.

When they present the false positives using clinical samples from other diseases, in most of the cases they are positive for ELISA and ICT platform. These results imply that the results are inherent to the ICT, as the cross reactivity is the most common problem in this type of samples.
Based on the above observations, I consider that the conclusions where they claim that CT platform is the sero diagnostics of ALA are risky. Even if it is correct for a sero diagnostics of amebiasis, it is necessary to contrast with the clinic information of the patient to discriminate amebiasis vs. ALA.

Additional comments
Based on the sample size and the results obtained, I suggest changing the title, even when it is innovative, I consider is still a proof of concept of the sero diagnostics platform.
By definition the Amebic liver abscess (ALA) is a severe extraintestinal manifestation caused when pathogenic trophozoites of E. histolytica disseminate to the liver, at this point the CT platform does not discriminate if the antibodies produced in the patients correspond to this type of infection or they correspond to another clinical manifestations of the disease. It is is necessary to include the clinical studies and the image analyses.",Authentic,"Background
Amebic liver abscess (ALA), caused by an extraintestinal invasion of the virulent protozoan Entamoeba histolytica, is important among parasitic causes of morbidity and mortality, especially in the tropics. Clinical symptoms, medical-imaging abnormalities of the liver and serological tests are normally made for supportive diagnosis. Serum-based enzyme-linked immunosorbent assay (ELISA) has been conventionally used for diagnosing ALA but is time-consuming and sophisticated equipment is required. Therefore, we sought to develop a new and rapid innovative point-of-care immunochromatographic test (ICT) that can use whole blood as an alternative to serum-based ELISA. An ICT tool using simulated whole-blood samples was developed for immunoglobulin G antibody detection, and its diagnostic efficiency was evaluated in comparison with serum-based ELISA.

Methods
Both methods were tested to assess their diagnostic performance using a total of 253 serum samples. These came from ALA patients (n = 13), healthy individuals (n = 40), and patients with other diseases (n = 200).

Results
Amebiasis-ICT exhibited 100% (95% confidential interval (CI) [75.3–100.0]) sensitivity and 97.1% (95% CI [94.1–98.8]) specificity, whereas ELISA gave the same sensitivity (100% 95% CI [75.3 –100.0]) and slightly lower specificity (95.8% 95% CI [92.5–98.0]). There were no significant differences in sensitivity and specificity between the two tests (Exact McNemar’s test; p > 0.05), with Cohen’s kappa agreement 96.44% (κ-value = 0.771, p < 0.001) indicating substantial agreement.

Conclusion
This ICT tool using simulated whole-blood samples has a high possibility of being used with real whole blood. Therefore, since there is no need to separate serum, this can be considered an innovative diagnostic tool to replace serum-based ELISA in clinics and field surveys in remote areas where medical facilities are limited."
Skin microbiota variation in Indian families,"Basic reporting
The introduction provides a solid background, but the rationale for choosing multi-generational families specifically could be further emphasized, especially regarding why this model is particularly valuable for studying skin microbiota.

Sharing of metadata related to the sequencing results could be beneficial for transparency

provide figures of better quality

Experimental design
A more explicit discussion of the limitations due to the sample size (i.e., families with only one grandparent or sibling) could be addressed earlier in the paper. This would manage expectations about the study's power and generalizability

Further detail on how potential biases during sample collection (e.g., temperature changes, collection timing) were minimized could add rigor to the methodology

Validity of the findings
How do these results contribute to global understanding, particularly given the Indian population's unique lifestyle factors?

While the authors mention the limitation of sample size, more robust discussion around the implications of not having more genetically diverse or unrelated family members within the cohort would be useful.

Additional comments
While the manuscript is well-structured and contributes to the understanding of skin microbiota in Indian families, addressing the above points regarding methodology clarity and better highlighting the findings would strengthen the paper further.",Authentic,"Background
In India, joint families often encompass members spanning multiple generations cohabiting in the same household, thereby sharing the same ethnicity, genetics, dietary habits, lifestyles, and other living conditions. Such an extended family provides a unique opportunity to evaluate the effect of genetics and other confounding factors like geographical location, diet and age on the skin microbiota within and between families across three generations.

Methods
The present study involved seventy-two individuals from fifteen families from two geographical regions of Maharashtra, India. The 16S rRNA sequencing of V3–V4 regions was performed and the generated taxonomic profiles were used for downstream analysis.

Results
Our study highlights a significant difference in community composition (beta diversity) between families (PERMANOVA; p = 0.001) and geographical locations (p = 0.001). We observed geographical location-wise differences in the relative abundances Staphylococcus in the families from Pune (Wilcoxon test, p = 0.007), and Bacillus in the Ahmednagar families (Wilcoxon test, p = 0.004). When within and between-family comparisons of skin microbiota composition were carried out between different generations (G1–G2, G2–G3, and G1–G3); we observed skin microbiota tended to be more similar within than between families but this difference was not significant.

Conclusion
This study underscores the diversity and commonalities in skin microbiota composition within and between families. Our result suggests that geographical location is significantly associated with the genus composition of skin microbiota, which is quantitatively unique for a family and likely explained by co-habitation."
"A novel microbial agent reduces soil paclobutrazol residue, enhances enzyme activities and increases Ophiopogon japonicus production","Basic reporting
Minor revisions have been requested for the manuscript. It is appropriate to publish after revision. Suggestions and corrections are indicated on the text.

Experimental design
good

Validity of the findings
good

Additional comments
no comment",Generic,"Background
Ophiopogon japonicus (O. japonicus) is a versatile plant valued for its medicinal, food, and ornamental properties. Its cultivation often involves the excessive use of paclobutrazol, leading to a series of environmental and agricultural problems such as soil contamination, nutrient depletion, and safety risks. However, there is currently no effective solution.

Methods
Based on a novel microbial agent, Micrococcus yunnanensis strain HY001 (MYSH), field experiments were conducted in the main production area of O. japonicus. Soil paclobutrazol residue, soil enzyme activities, and the yield and dry matter ratio of O. japonicus were measured. Hierarchical partitioning (HP) was used to identify the relative importance of different variables, and partial least squares path modeling (PLS-PM) was applied to elucidate the mechanisms underlying MYSH’s effects on soil health and crop production.

Results
MYSH significantly reduced soil paclobutrazol residue by 75.18% over five months, compared to a natural degradation rate of 50.72% over a year. Compared to the control group, the MYSH-treated group enhanced soil sucrase activity, soil urease activity, and soil alkaline phosphatase activity, with rates of 47.81%, 46.70%, and 216.66%, respectively. Additionally, MYSH improved O. japonicus productivity, with a 94.75% increase in yield and a 17.64% increase in dry matter ratio. HP revealed that MYSH was the primary factor affecting the yield and dry matter ratio of O. japonicus, with relative importance of 47.75% and 42.28%, respectively. The key mechanism was that MYSH degraded soil paclobutrazol residue, which in turn influenced soil sucrase activity, ultimately impacting the yield of O. japonicus (p < 0.05).

Conclusions
This study demonstrates the dual role of MYSH as both an environmental remediation agent and a crop productivity enhancer for the first time. By reducing paclobutrazol residue and enhancing soil health and crop production, MYSH shows great potential for broader application in sustainable agricultural practices. This research highlights the efficacy of microbial agents in addressing agrochemical contamination and promoting sustainable farming, providing a valuable contribution to the development of eco-friendly agricultural solutions."
Benchmarking of a time-saving and scalable protocol for the extraction of DNA from diverse viromes,"Basic reporting
I have thoroughly reviewed the current work and found no notable differences between it and a previously published study that employs a very similar experimental method and kit to investigate the human virome (https://pubmed.ncbi.nlm.nih.gov/35199035/). The research objectives and methodologies in both studies closely align, raising essential questions about the novelty and contribution of the current findings.

I recommend that the authors introduce and actively discuss their work compared to the previously published study. This discussion should include a detailed comparison of the methodologies used, highlighting similarities and identifying areas where they have built upon or diverged from the earlier research. Such a comparative analysis will help to contextualize their findings within the broader landscape of virome research, enhancing the significance of the current study and providing a clearer understanding of its contributions to the field. Addressing these points will strengthen the manuscript and offer readers a more comprehensive view of the subject matter.

Experimental design
no comment

Validity of the findings
no comment

Additional comments
no comment",Authentic,"The virome, composed of viruses inhabiting diverse ecosystems, significantly influences microbial community dynamics and host health. The phenol-chloroform DNA extraction protocol for viromes, though effective, is time-intensive and requires the use of multiple toxic chemicals. This study introduces a streamlined, scalable protocol for DNA extraction using a commercially-available kit as an alternative, assessing its performance against the phenol-chloroform method across human fecal, mouse fecal, and soil samples. No significant differences in virome diversity or community composition were seen between methods. Most viral operational taxonomic units (vOTUs) were common to both methods, with only a small percentage unique to either approach. Alpha- and beta-diversity analyses showed no significant impact of the extraction method on virome composition, confirming the kit’s efficacy and versatility on sample types beyond those officially supported by the manufacturer. While the kit approach offers benefits like reduced toxicity and increased throughput, it has limitations such as higher costs and potential issues reliably capturing low-abundance taxa. This protocol provides a viable option for large-scale virome studies, although the phenol-chloroform approach may still be preferable for specific sample types."
Effects of tropical fruit blends on fermentative and pigmentation aspects of probiotic native cultured goat milk,"Basic reporting
In the article titled ""Influence of tropical fruit blends on fermentative parameters and pigmentation in fermented goat milk with potentially probiotic native culture"", it is understood that the effects of 3 different regionally valuable fruits on some quality parameters in goat milk fermented with probiotic microorganisms in combination were studied. For this purpose, Total Phenolic Substance, pH and titratable acidity, color values ​​and the viability of L. plantarum and Streptococcus thermophilus were determined in the product. At the same time, fruit mixtures were determined using the experimental design method.
Although the aim of the study was good, the characteristics of the product could not be fully reflected due to the few quality analyzes performed. Examining the study in terms of total monomeric anthocyanin and in vitro antioxidant activity will add more meaning and enrich the study.

Keywords:
The number of keywords is not sufficient to represent the entire text. Therefore, keywords should be kept broad enough to reflect the entire study (5-6 words may be sufficient).

In Introduction:
“Among these foods, we have fruits (Hern·ndez-Carranza et al., ….” The subject “we” is used in this expression. Care should be taken not to use general expressions with personal subjects.

What is the purpose of combining three different fruits used in the study? It would be useful to mention this in the purpose section.

Experimental design
In Materials and Methods:
The structure of the fruits used in the study also contains anthocyanins, which are flavonoids with antioxidant properties. Total Monomeric Anthocyanin and Antioxidant capacity analyzes will also contribute to the study in terms of how stable the fermented goat milk remains in its structure and how it contributes to antioxidant activity.

Validity of the findings
The conclusion section should be further developed regarding the findings obtained and the advantages of the experimental design method applied.

All underlying data have been provided; they are robust, statistically sound, & controlled.",Authentic,"Background
Fruits are sources of bioactive compounds such as phenolics that bring health benefits to consumers. The addition of fruit products and microorganisms with probiotic potential in fermented goat milk can facilitate the acquisition of these benefits through diet. In this sense, the objective of this study was to evaluate the effect of incorporating a mixture of ingredients from jaboticaba (Myrciaria cauliflora), jambolana (Syzygium cumini), and mandacaru (Cereus jamacaru) fruits on fermentation parameters (pH, titratable acidity, viability of the native culture Lactiplantibacillus plantarum CNPC003 and the starter culture), associated with pigmentation (phenolic compound content and color) through experimental mixture design.

Methods
A simplex-centroid experimental design was conducted, comprised of seven trials totaling the addition of 30% of the fruit preparations in the final formulation of fermented milk and one control trial (without addition of preparations), with the response being the total phenolic content and the instrumental color parameter a*. Fermentations were carried out with the addition of the native culture Lactiplantibacillus plantarum CNPC003 and the starter culture Streptococcus thermophilus. Subsequently, analyses of pH, titratable acidity, viability of the native and starter cultures, total phenolic compound content, and the instrumental color parameter a* were performed.

Results
The final pH among trials ranged from 4.55 to 4.69, titratable acidity ranged from 0.59 to 0.64, the population of L. plantarum CNPC003 reached levels exceeding 8 log CFU/g, as did the population of Streptococcus thermophilus. The content of phenolic compounds was higher in trials T1, T5, and T7, as well as the color parameter (a*). The use of experimental mixture design contributed to the development of products with high viability of L. plantarum, high content of phenolic compounds, and a characteristic color of the added fruits, bringing benefits to consumer health."
"A new metriacanthosaurid theropod dinosaur from the Middle Jurassic of Yunnan Province, China","Basic reporting
The English has improved—nothing else to comment.

Validity of the findings
This has a significant impact due to the need for a good description of Metriacanthosaurids from China. Congrats again to the authors. The description has improved considerably.

Additional comments
After the revision, the manuscript has significantly improved. I think they follow most of the suggestions of both reviewers, and I think the manuscript should be accepted as is. Congratulations to the authors for their study!",Generic,"Metriacanthosaurid theropods represent a basal-branching lineage of tetanurans. Members of this clade are mainly medium to large-sized and lived in Laurasia during the Middle Jurassic to the Early Cretaceous. In this clade, Sinraptor dongi, Sinraptor hepingensis, and Yangchuanosarus shangyouensis from the Late Jurassic are well represented by the nearly complete specimens, but the incompleteness of Middle Jurassic taxa hinders our knowledge of the origin and early evolution of Metriacanthosauridae. This paper describes a new genus and species of metriacanthosaurids, Yuanmouraptor jinshajiangensis gen. et sp. nov, from the Middle Jurassic Zhanghe Formation of Yunnan Province, China. The new taxon is represented by a cranium and the anterior section of the vertebral column including the complete cervical series and the first dorsal vertebra. Yuanmouraptor jinshajiangensis can be diagnosed based on the following autapomorphies: the anterior process of postorbital sheet-shaped and keeping consistent depth; ventral ramus of postorbital bearing a laterally twisted trough running along its lateral surface; ventral surface of axial intercentrum parallel with that of axial centrum; discontinuity of inclination on anterodorsal margin of the third and fourth cervical vertebrae; strongly posteriorly elongated epipophyses of anterior cervical vertebrae; deeply excavated pneumatic foramina on the third cervical vertebra; sheet-shaped and subrectangular neural spines of posterior cervical vertebrae. Phylogenetic analysis recovers Yuanmouraptor as the most basal-branching member within Metriacanthosauridae and provides a new alternative phylogenetic topology of non-coelurosaurian tetanurans."
Using transformers and Bi-LSTM with sentence embeddings for prediction of openness human personality trait,"Basic reporting
Strength: 1) the study employs a wide range of machine learning (ML) and deep learning (DL) models, including shallow ML, ensemble models, and state-of-the-art architectures like Bi-LSTM and BERT. This comprehensive comparison provides a robust foundation for understanding model performance on personality prediction tasks. 2) Feature engineering is well-executed and compared, incorporating a mix of traditional (TF-IDF, POS tagging) and deep learning (word2vec, GloVe, and sentence embeddings) approaches. This breadth provides comprehensive comparison and understanding.

Weakness: 1) the manuscript provides detailed performance metrics for various models but lacks sufficient explanation or interpretation of the experimental results. This gap limits the reader's ability to understand why certain models perform better than others and what insights can be drawn from these results. 2) The manuscript does not clearly articulate the specific research gap it aims to address. While the study emphasizes applying ML and DL models for predicting the openness personality trait, it fails to specify how this work advances the field beyond existing literature or addresses unresolved questions.


1. The second contribution of this work is “Examination of diverse shallow ML models, ensemble models and advanced DL algorithms like LSTM and Bi-LSTM and transformer-based model BERT."", which seems less convincing given that models like LSTM and BERT are no longer considered cutting-edge. Incorporating Large Language Models (LLMs), such as GPT or LLaMA, as alternatives for embedding generation could provide a more contemporary approach.

2. The English language in this paper requires refinement to enhance clarity and ensure it is easily understood by an international audience. The current phrasing poses challenges to comprehension, with issues such as grammatical errors observed in lines 24–28 and unclear wording in lines 37–40 and 73–75 (Each trait represents a spectrum, with individuals adapting in the degree to which user reveals each trait according to the behavior). Thoroughly proofreading your manuscript is essential to ensure clarity, coherence, and the elimination of errors.

3. Another important point is that in Table 10, reference [38] did not conduct experiments on the MBTI dataset. This raises questions about the source of the reported 87% result and its relevance to the comparison.


4. The relationship between the NS dimension and openness within the Big Five personality framework should be further clarified. In particular, the rationale behind Table 1 requires stronger justification. Linking it to foundational works on MBTI and the conceptualization of openness in the Big Five framework would provide more depth and context.

5. More automatic personality recognition studies should be discussed to provide reader for a more comprehensive contexts. The below are some examples:

[1] Ghassemi, Sina, Tianyi Zhang, Ward Van Breda, Antonis Koutsoumpis, Janneke K. Oostrom, Djurre Holtrop, and Reinout E. de Vries. ""Unsupervised multimodal learning for dependency-free personality recognition."" IEEE transactions on affective computing (2023).

[2] Leekha, Maitree, Shahid Nawaz Khan, Harshita Srinivas, Rajiv Ratn Shah, and Jainendra Shukla. ""VyaktitvaNirdharan: Multimodal Assessment of Personality and Trait Emotional Intelligence."" IEEE Transactions on Affective Computing (2024).

[3] Song, Siyang, Shashank Jaiswal, Enrique Sanchez, Georgios Tzimiropoulos, Linlin Shen, and Michel Valstar. ""Self-supervised learning of person-specific facial dynamics for automatic personality recognition."" IEEE Transactions on Affective Computing 14, no. 1 (2021): 178-195.

[4] Song, Siyang, Zilong Shao, Shashank Jaiswal, Linlin Shen, Michel Valstar, and Hatice Gunes. ""Learning person-specific cognition from facial reactions for automatic personality recognition."" IEEE Transactions on Affective Computing 14, no. 4 (2022): 3048-3065.

[5] Junior, Julio CS Jacques, Yağmur Güçlütürk, Marc Pérez, Umut Güçlü, Carlos Andujar, Xavier Baró, Hugo Jair Escalante et al. ""First impressions: A survey on vision-based apparent personality trait analysis."" IEEE Transactions on Affective Computing 13, no. 1 (2019): 75-95.

**PeerJ Staff Note:** It is PeerJ policy that additional references suggested during the peer-review process should only be included if the authors are in agreement that they are relevant and useful.

Experimental design
The experimental design are generall comprehensive and good

Validity of the findings
no comment

Additional comments
no comment",Authentic,"Understanding human personality traits is significant as it helps in decision making related to consumers’ behavior, career counselling, team building and top candidates’ selection for recruitment. Among various traits, openness is essential as it shows both diverse aspects of sensitive nature or intuitive nature. The individuals having a sensing nature tends to be more practical and prefer to focus on concrete information whereas the users having intuitive trait type is characterized by a focus on abstract ideas, creative thinking and future-oriented perspectives. In this research work, we aim to explore diverse natural language processing (NLP) based features and apply state of the art deep learning algorithms for openness trait prediction. Using standard Myers-Briggs Type Indicator (MBTI) dataset, we propose the use of the latest deep features of sentence embeddings which captures contextual semantics of the content to be used with deep learning models. For comparison, we explore textual features of Frequency-Inverse Document (TF-IDF) and parts of speech (POS) tagging with machine learning models and deep features of word2vec and global vectors for word representation (GloVe) with deep learning models. The comprehensive empirical analysis reveals that TF-IDF used with gradient boosting achieves high accuracy of 90% whereas, the deep feature of sentence embeddings when used and with deep model bidirectional long short-term memory (Bi-LSTM) achieves 90.5% accuracy. The best results have been achieved using the latest Transformer-based DistilBERT, which achieves the highest accuracy of 92% outperforming the existing studies in relevant literature."
Anticancer drug synergy prediction based on CatBoost,"Basic reporting
The study presents a novel approach to predicting anticancer drug synergy using the CatBoost algorithm, leveraging features such as morgan fingerprints and gene expression profiles. The model demonstrated superior performance compared to other methods, and the use of SHAP provided insights into the biological relevance of the predictions.

Experimental design
Methods described lack details. How the new Catboost is constructed is not well defined. No details were provided about data preprocessing. The model selection methods were not adequately described.

Validity of the findings
The manuscript lacks an independent datasets for validation.
is it generic or authentic?",Generic,"Background
The research of cancer treatments has always been a hot topic in the medical field. Multi-targeted combination drugs have been considered as an ideal option for cancer treatment. Since it is not feasible to use clinical experience or high-throughput screening to identify the complete combinatorial space, methods such as machine learning models offer the possibility to explore the combinatorial space effectively.

Methods
In this work, we proposed a machine learning method based on CatBoost to predict the synergy scores of anticancer drug combinations on cancer cell lines, which utilized oblivious trees and ordered boosting technique to avoid overfitting and bias. The model was trained and tested using the data screened from NCI-ALMANAC dataset. The drugs were characterized with morgan fingerprints, drug target information, monotherapy information, and the cell lines were described with gene expression profiles.

Results
In the stratified 5-fold cross-validation, our method obtained excellent results, where, the receiver operating characteristic area under the curve (ROC AUC) is 0.9217, precision-recall area under the curve (PR AUC) is 0.4651, mean squared error (MSE) is 0.1365, and Pearson correlation coefficient is 0.5335. The performance is significantly better than three other advanced models. Additionally, when using SHapley Additive exPlanations (SHAP) to interpret the biological significance of the prediction results, we found that drug features played more prominent roles than cell line features, and genes associated with cancer development, such as PTK2, CCND1, and GNA11, played an important part in drug synergy prediction. Combining the experimental results, the model proposed in this study has a good prediction effect and can be used as an alternative method for predicting anticancer drug combinations."
Autonomous vehicle surveillance through fuzzy C-means segmentation and DeepSORT on aerial images,"Basic reporting
The paper focuses on developing a vehicle surveillance system using aerial images, employing a combination of fuzzy C-means (FCM) clustering for segmentation, YOLOv4 for vehicle detection, and DeepSORT for multi-vehicle tracking. Overall, the paper is interesting with detailed comments as follows.

The standards are mostly met. One question:
The significance of segmentation in image preprocessing is not sufficiently explained. A clearer explanation is needed to illustrate why segmentation is a critical step in the proposed system.

Experimental design
The approach is validated on two datasets (UAVDT and KIT-AIS) and demonstrates high precision in vehicle detection.

The standards are mostly met. One question:
The two datasets seem not big. What is the scalability for large-scale traffic scenarios of your proposed pipeline.

Validity of the findings
Key innovations include improved segmentation accuracy via FCM, enhanced vehicle tracking with ID assignment using Speed-Up Robust Features (SURF), and precise trajectory mapping.

The standards are mostly met.

Additional comments
1. The significance of segmentation in image preprocessing is not sufficiently explained. A clearer explanation is needed to illustrate why segmentation is a critical step in the proposed system.
2. The two datasets seem not big. What is the scalability for large-scale traffic scenarios of your proposed pipeline.
3. In Figure 7, why UAVDT and KIT-AIS dataset seem totally same. The detections results are also identical.
4. Can a UAV remain stationary at a single intersection for a period of time? Estimating traffic flow or conditions often requires observations over a defined time window to capture dynamic changes accurately",Authentic,"The high mobility of uncrewed aerial vehicles (UAVs) has led to their usage in various computer vision applications, notably in intelligent traffic surveillance, where it enhances productivity and simplifies the process. Yet, there are still several challenges that must be resolved to automate these systems. One significant challenge is the accurate extraction of vehicle foregrounds in complex traffic scenarios. As a result, this article proposes a novel vehicle detection and tracking system for autonomous vehicle surveillance, which employs Fuzzy C-mean clustering to segment the aerial images. After segmentation, we employed the YOLOv4 deep learning algorithm, which is efficient in detecting small-sized objects in vehicle detection. Furthermore, an ID assignment and recovery algorithm based on Speed-Up Robust Feature (SURF) is used for multi-vehicle tracking across image frames. Vehicles are determined by counting in each image to estimate the traffic density at different time intervals. Finally, these vehicles were tracked using DeepSORT, which combines the Kalman filter with deep learning to produce accurate results. Furthermore, to understand the traffic flow direction, the path trajectories of each tracked vehicle is projected. Our proposed model demonstrates a noteworthy vehicle detection and tracking rate during experimental validation, attaining precision scores of 0.82 and 0.80 over UAVDT and KIT-AIS datasets for vehicle detection. For vehicle tracking, the precision is 0.87 over the UAVDT dataset and 0.83 for the KIT-AIS dataset."
From grit to flourishing: physical literacy’s mediating role in enhancing well-being among college students with obesity,"Basic Reporting
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The abstract succinctly summarizes the study's objectives, methods, and key findings. The introduction provides a comprehensive background, justifying the research gap and hypotheses. The methods section is detailed, allowing for reproducibility, and the results are presented clearly with appropriate figures and tables. The discussion contextualizes the findings within existing literature and acknowledges limitations. However, the paper could benefit from a clearer distinction between the study's novel contributions and prior work, particularly in the discussion section.

Experimental Design
The study employs a cross-sectional design to investigate the mediating role of physical literacy in the relationship between grit and well-being among college students with obesity. The use of validated questionnaires (Grit-S, PPLI-SC, and PERMA-Profiler) strengthens the study's reliability. The inclusion of a parallel mediation analysis to examine the subdimensions of physical literacy (motivation, confidence and physical competence, and interaction with the environment) adds depth to the analysis.

However, the study could be improved by:

Longitudinal Design: The cross-sectional nature limits causal inferences. A longitudinal design would better establish temporal relationships.

Objective Measures: Reliance on self-reported data may introduce biases. Incorporating objective measures of physical activity (e.g., accelerometers) and physical competence (e.g., fitness tests) would enhance validity.

Sample Diversity: The sample is limited to students from Guangzhou, China, which may affect generalizability. Including participants from diverse cultural and geographical backgrounds would strengthen external validity.

Validity of the Findings
The findings are internally valid, supported by robust statistical analyses (mediation models with bootstrapping) and careful control of confounding variables. The significant mediation effects align with existing literature on grit, physical literacy, and well-being.

Key strengths include:

The identification of ""interaction with the environment"" as the strongest mediator, highlighting the importance of social and environmental engagement in well-being.

The use of the PERMA model to comprehensively assess well-being, capturing multiple dimensions (Positive Emotion, Engagement, Relationships, Meaning, and Accomplishment).

Limitations affecting external validity:

The study's focus on obese college students in China may limit generalizability to other populations or cultural contexts.

The reliance on self-reported data may not fully capture the constructs of interest.

Additional Comments
Cultural Context: The discussion of collectivist cultural norms is insightful but could be expanded. For example, how might these norms specifically influence the development of grit and physical literacy in Chinese students?

Practical Implications: The suggestions for campus-based interventions are valuable. However, more specific recommendations (e.g., types of group activities, frequency) would be helpful for practitioners.

Future Research: The call for longitudinal studies is appropriate. Additionally, exploring potential moderators (e.g., social support, mental health) could provide a more nuanced understanding of the mechanisms involved.

Overall Recommendation
The manuscript makes a valuable contribution to understanding the interplay between grit, physical literacy, and well-being in college students with obesity. With minor revisions (e.g., addressing limitations, clarifying practical implications), it would be suitable for publication. The study advances the field by highlighting the mediating role of physical literacy and the importance of environmental interaction.",AI-Generated,"Objective
To investigate whether physical literacy mediates the relationship between grit and well-being among college students with obesity.

Methods
A total of 385 students with obesity were recruited. Participants completed validated questionnaires measuring grit, physical literacy, and well-being. Mediation analyses were performed to estimate indirect effects and generate bias-corrected 95% confidence intervals (CI).

Results
Grit was positively associated with physical literacy, and physical literacy was positively associated with well-being. Physical literacy partially mediated the relationship between grit and well-being, with the indirect effect accounting for 26.32% of the total effect (indirect effect = 0.20, 95% CI [0.09–0.31]). In a parallel mediation model analyzing the subdimensions of physical literacy, the “interaction with the environment” emerged as the strongest mediator (indirect effect = 0.15, 95% CI [0.10–0.21]), accounting for 19.74% of the total effect. The indirect effects through “motivation” and “confidence and physical competence” were also significant but accounted for smaller proportions of the total effect (6.58% and 5.26%, respectively).

Conclusions
These findings serve as an initial step in understanding how physical literacy, particularly the ability to interact with the environment, partially mediates the relationship between grit and well-being among college students with obesity. Future interventional research aiming to enhance physical literacy—especially environmental engagement—is needed to confirm whether it can amplify the positive impact of grit on well-being. A multifaceted approach that fosters both psychological traits and physical competencies may prove beneficial in improving the psychological and physical health of this population."
Unconscious information processing of table tennis athletes in a masked priming paradigm: an event-related potentials (ERP) study,"Basic Reporting
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The abstract succinctly summarizes the study's objectives, methods, and key findings. The introduction provides a comprehensive background, justifying the research gap and hypotheses. The methods section is detailed, allowing for reproducibility, and the results are presented clearly with appropriate figures and tables. The discussion contextualizes the findings within existing literature and acknowledges limitations. However, the paper could benefit from a clearer distinction between the study's novel contributions and prior work, particularly in the discussion section.

Experimental Design
The study employs a robust 2 (group: athletes vs. non-athletes) × 2 (condition: congruent vs. incongruent) mixed factorial design, which is appropriate for addressing the research questions. The use of both behavioral (reaction times, error rates) and electrophysiological (P3 latency and amplitude) measures strengthens the validity of the findings. The masked priming paradigm is well-executed, and the control tasks (subjective and objective threshold tests) ensure that priming stimuli were processed unconsciously.

However, the study could be improved by:

Sample Size and Diversity: The sample size, while justified by a power analysis, is relatively small, especially for exploring potential sex differences or higher-level athletes. Future studies should aim for larger, more diverse samples.

Fitness Level Control: The lack of fitness level assessment between groups is a notable limitation, as fitness could confound cognitive performance. Including this as a covariate would strengthen the design.

Task Complexity: The simplicity of the arrow-direction task may have led to a ceiling effect, masking potential group differences. Incorporating more complex or sport-specific stimuli could better differentiate athletes' unconscious processing advantages.

Validity of the Findings
The findings are internally valid, supported by appropriate statistical analyses (ANOVAs, correlation tests) and careful control of confounding variables (e.g., prime visibility). The negative compatibility effect (NCE) and ERP results align with existing literature, reinforcing the credibility of the conclusions.

Key strengths include:

The correlation between years of training and NCE magnitude, suggesting training enhances inhibitory control.

The ERP data revealing neurocognitive adaptations in athletes (longer P3 latency, higher amplitude), supporting the role of expertise in resource allocation.

Limitations affecting external validity:

The generalizability of findings is limited to right-handed, college-aged table tennis athletes. Including left-handed athletes and broader age ranges would enhance applicability.

The absence of sport-specific stimuli limits conclusions about domain-specific vs. general processing advantages.

Additional Comments
Error Monitoring: The study did not analyze error rates or post-error slowing, which could provide deeper insights into cognitive control mechanisms. Future work should incorporate these measures.

Theoretical Implications: The discussion could better integrate the results with theories like event coding (Hommel et al., 2001) to explain why athletes showed no NCE advantage in general contexts.

Practical Applications: The implications for training (e.g., designing drills to enhance unconscious processing) could be expanded.

Overall Recommendation
The manuscript makes a valuable contribution to understanding unconscious information processing in athletes. With minor revisions (e.g., addressing limitations, clarifying theoretical links), it would be suitable for publication. The study advances the field by highlighting the context-dependent nature of athletes' cognitive advantages and the neuroplasticity induced by specialized training.",AI-Generated,"Background
Unconscious information processing is enhanced among athletes for sports-specific contexts. Whether this enhancement is transferable to general contexts is unknown. This study explored unconscious information processing and brain activity in highly trained table tennis athletes and non-athletes in general contexts.

Methods
Twenty table tennis athletes (six females, mean age = 20.38 ± 1.28, mean ± standard error) and 21 aged-matched college students (eight females, mean age = 19.81 ± 1.29) were recruited for this study. Each participant first performed a masked priming task. In this task, a prime stimulus (arrows pointing left or right) was presented, followed by a visual mask (arrows pointing in both directions) and then a target stimulus, the target stimulus consisted of arrows pointing in the same direction as the prime for congruent stimuli or in the opposite direction for incongruent trials, while the P3 component of the event-related potential was simultaneously recorded in the brain. As a control, participants then performed a prime identification task (the subjective threshold test and the objective threshold test) to determine whether they could consciously detect the priming arrows. Reaction times, error rates, P3 latency and P3 peak amplitude were analyzed to examine the unconscious information processing of table tennis athletes in general contexts.

Results
Participants responded with the direction of the target arrow and were not consciously aware of the priming stimulus. Athletes responded faster in comparison of non-athletes. Athletes and non-athletes responded faster and committed fewer errors in incongruent vs. congruent conditions. In addition, the years of table tennis training were negatively correlated with the magnitude of negative compatibility effect. Both groups displayed longer P3 latencies, a measure of inhibitory control, in the incongruent vs. congruent trials. However, athletes displayed higher P3 peak amplitudes, reflecting larger attention resource input, and longer P3 latencies than non-athletes in central brain sites.

Conclusion
Unconscious information processing among table tennis athletes is not prominent in general contexts, but may be limited to the sports-specific context or more complex cognitive tasks."
"Time perception and lived experience in personality disorders: differences across types, dimensions and severity","Basic Reporting
The manuscript is well-structured and clearly written, with a logical flow from introduction to discussion. The abstract succinctly summarizes the study's objectives, methods, and key findings. The introduction provides a comprehensive background on temporal experience in personality disorders (PD), particularly borderline personality disorder (BPD), and justifies the need for this study by highlighting gaps in the literature. The methods section is detailed, describing the sample, measures, and statistical analyses. The results are presented clearly, and the discussion effectively interprets the findings in the context of existing literature.

However, the manuscript could benefit from a clearer distinction between the study's primary and secondary objectives. Additionally, while the limitations are thoroughly discussed, some methodological details (e.g., exclusion criteria for the sample) could be expanded for greater transparency.
Experimental Design
The study employs a cross-sectional design to explore temporal experience in PD using both quantitative (PS-ZTPI, CT) and phenomenological approaches. The sample includes 63 participants with various PD diagnoses, primarily BPD and narcissistic PD (NPD), which aligns with the study's aims. The use of validated instruments (e.g., LPFS-BF 2.0, PICD) strengthens the methodological rigor.

Key strengths:

The combination of reflective (ZTPI) and pre-reflective (CT) measures provides a nuanced understanding of temporal experience.

The inclusion of both categorical (ICD-10) and dimensional (ICD-11) approaches to PD diagnosis is a notable contribution.

Areas for improvement:

The sample is skewed toward BPD and NPD, limiting generalizability to other PD types. A more balanced representation of PD categories would enhance the findings.

The lack of a healthy control group is a significant limitation, as comparisons rely on historical data. Future studies should include matched controls to strengthen validity.
Validity of the Findings
The findings are compelling and align with prior theoretical and empirical work on temporal disturbances in PD, particularly BPD. The results demonstrate that individuals with PD exhibit a heightened negative past orientation and diminished future perspective, consistent with phenomenological literature. The distinction between BPD and NPD in terms of present hedonism and past negativity is a valuable contribution.

However, several factors may affect the validity of the findings:

Depressive symptoms: The study did not control for comorbid depressive symptoms, which are known to influence time perspective (e.g., negative past bias). This confounding variable should be addressed in future research.

Instrument limitations: The Polish Short Version of ZTPI omits the present-fatalistic scale, and CT is a projective tool with inherent subjectivity. These limitations are acknowledged but warrant caution in interpretation.

Cross-sectional design: The study cannot establish causality or track changes over time. Longitudinal or interventional designs would provide stronger evidence for the relationship between PD severity and temporal experience.
Additional Comments
Clinical implications: The discussion could further elaborate on how these findings might inform therapeutic interventions (e.g., Time Perspective Therapy) for PD.

Phenomenological integration: While the study bridges quantitative and phenomenological approaches, a more explicit discussion of how the quantitative results map onto specific phenomenological constructs (e.g., ""intra-festum"" temporality) would enrich the interpretation.

Future directions: The manuscript identifies several avenues for future research (e.g., broader PD representation, longitudinal designs). Adding a call for studies using mixed-methods or experimental paradigms (e.g., temporal discounting tasks) could further strengthen this section.",AI-Generated,"Background
Altered temporal experience lies at the core of various psychiatric conditions, including borderline personality disorder (BPD). Mainstream research in psychopathology tends to explore BPD with scrutiny while neglecting other personality disorders (PD). At the same time, the dimensional approach to PD proposes looking through the disorders’ subtypes and tracing lived experience-based commonalities. This study is the first to explore the temporality of PD by investigating the relationship between symptom severity and lived time and combining objectified measures of time perception with phenomenological interpretation.

Methods
A total of 63 participants of various educational backgrounds, with personality disorders (36.5% male), following ICD-10 coding diagnosed with paranoid (3.2%), borderline (41.3%), narcissistic (33.3%), avoidant (4.8%), dependent (1.6%) and unspecified (15.9%) personality disorder. Levels of personality functioning and intensity of maladaptive trait domains were controlled with Level of Personality Functioning—Brief Scale 2.0 and Personality Inventory for ICD-11, respectively, resulting in the overall sample classification as comprising nine subclinical, 13 mild, 20 moderate, 16 severe, and five extremely severe conditions. Polish Short Version of the Zimbardo Time Perspective Inventory (PS-ZTPI) and Cottle’s Circles Test (CT) were used to assess the temporal experience.

Results
In comparison to healthy individuals, those with PD are more oriented toward past negative (4.01 vs. 2.98) and less toward past positive (2.31 vs. 3.71) and future (3.04 vs. 3.47), as measured with PS-ZTPI; their pre-reflective temporal experience, as measured with CT, is dominated either by the past or the future, while the present remains marginalized. BPD distinctiveness among other PD lies in higher orientation toward hedonistic present and lower orientation toward the future. While the general temporal profile of PD is independent of age and duration of hospitalization, it is related to the severity of the condition. The more severe the impairments in self-functioning, the higher the negative past perspective and pre-reflective past dominance, and the lower the positive and future perspective. The results of this study highlight temporality as an essential aspect of lived experience in PD, being possibly related to disturbed self-experience."
Impact of parental marital status on self-harm in Chinese primary school students: the mediating role of depression and the moderating effect of classmate relationships,"Basic Reporting
The paper is well-structured and provides clear reporting on the impact of parental marital status on self-harm in Chinese primary school students, focusing on the mediating role of depression and the moderating effect of classmate relationships. The introduction establishes the significance of the issue effectively, citing relevant literature. The figures and tables are informative and align with the study's objectives. However, some sections could benefit from improved clarity, particularly in explaining the statistical methods and results. The language is professional and adheres to academic standards.

Experimental Design
The study employs a cross-sectional survey with a robust sample size of 33,285 students, ensuring reliability and generalizability within the target population. The methodology is described comprehensively, detailing data collection, ethical considerations, and statistical analysis. The use of structural equation modeling (SEM) is appropriate for assessing mediating and moderating relationships. However, the study's reliance on self-reported data introduces potential bias, which is acknowledged but could be mitigated in future research with mixed methods.

Validity of the Findings
The findings are significant, demonstrating a clear relationship between parental marital status, depression, and self-harm, moderated by classmate relationships. The statistical analyses are rigorous, supporting the validity of the conclusions. Nonetheless, the cross-sectional design limits causal inferences. The discussion contextualizes the results effectively, comparing them with previous studies and offering plausible explanations for observed patterns. Suggestions for future research, including longitudinal studies, are well-justified.

Additional Comments
The study could expand on the implications of its findings for mental health interventions in schools and communities.

The discussion of the moderating role of classmate relationships could delve deeper into practical strategies for fostering supportive peer environments.

Consider rephrasing complex statistical explanations for broader accessibility to readers unfamiliar with SEM.",AI-Generated,"Background
Self-harm is an increasing global public health concern, with a growing prevalence in younger children. This study investigates the associations between parental marital status and self-harm behaviors among primary school students, with a focus on the mediating role of depressive symptoms and the moderating effect of classmate relationships.

Methods
A cross-sectional survey was conducted among 33,285 students (grades 3–6; mean age = 10.36 years) in the Shapingba District of Chongqing, China, from September to December 2020. Self-report measures included the Children’s Depression Inventory (CDI), general demographic data, self-harm behaviors, and parental marital status. Data were analyzed using SPSS 26.0 for descriptive statistics and Mplus 8.1 for structural equation modeling (SEM), assessing the effects of parental marital status on self-harm.

Results
The reporting rates of depression and self-harm in grades 3–6 of primary school are 16.3% and 12.7%, respectively. Parental separation exhibited a more pronounced overall impact on self-harm (β = 0.120) compared to divorce (β = 0.105). Positive classmate relationships mitigated the indirect effect of separation on self-harm mediated by depression, reducing it from 0.098 to 0.072. Additionally, these relationships attenuated the direct effect of divorce on self-harm, decreasing it from 0.088 to 0.043. Depression significantly mediates the relationship between parental separation and children’s self-harm, with direct and indirect effects accounting for 53% (β = 0.057) and 47% (β = 0.063) of the total effect, respectively.

Conclusion
The marital status of parents, especially in cases of separation, has a significant impact on self-harm behaviors among primary school students, with depression acting as a key mediating factor. Supportive classmate relationships can alleviate this effect, highlighting their importance in mental health interventions. These findings offer valuable insights for the development of policies aimed at reducing self-harm and enhancing psychological well-being among children."
Effect of hypoxia conditioning on physical fitness in middle-aged and older adults—a systematic review and meta-analysis,"Basic Reporting
Clarity and Organization:

The manuscript is well-structured and written clearly, with a focused introduction that effectively outlines the significance of hypoxic conditioning (HC) for middle-aged and older adults.

However, the discussion of contradictory findings (e.g., VO2peak improvements) could benefit from greater emphasis to provide a balanced overview.

Literature Contextualization:

The inclusion of relevant studies is thorough, and the discussion highlights key gaps in existing research.

The authors might strengthen the contextual foundation by further discussing the implications of variability in individual responses to HC, as mentioned briefly in the discussion.

Figures and Tables:

Figures and tables are well-prepared and informative. However, detailed captions should ensure standalone comprehension. For instance, clarify abbreviations used in Table 2 for non-specialist readers.

Supplementary Material:

The integration of supplementary material is excellent. The inclusion of a supplemental search strategy adds transparency to the methodology.

Experimental Design
Study Rationale:

The justification for the meta-analysis is well-articulated, particularly the focus on middle-aged and older adults who face unique physical fitness challenges.

Methodological Rigor:

The methodology adheres to PRISMA guidelines and employs robust data extraction and risk-of-bias evaluation techniques.

A notable strength is the comprehensive subgroup analysis, though the rationale for specific subgroup choices (e.g., <8 weeks vs. ≥8 weeks) could be explained further.

Suggestions for Improvement:

Consider adding a flowchart to depict the experimental processes for clarity.

A more detailed explanation of sensitivity analysis results (e.g., why certain studies impacted pooled outcomes significantly) would enhance the reader’s understanding.

Validity of the Findings
Robustness and Interpretation:

The statistical analysis is solid, with reasonable subgroup considerations and interpretation of heterogeneity. The inclusion of sensitivity analyses adds credibility to the findings.

The evidence supporting the benefit of HC for VO2peak is compelling, but the discussion should address the limitations of relying heavily on a single influential study for this conclusion.

Limitations and Future Research:

The manuscript acknowledges key limitations, such as the small number of included studies and high heterogeneity in some analyses. These points are critical and appropriately highlighted.

Expanding on potential biases due to unclear randomization methods in many included RCTs could add depth to the discussion.

Practical Implications:

The authors make a strong case for HC as a complementary intervention to normoxic training, particularly for improving aerobic capacity. Further discussion on its practical applications for clinicians or exercise physiologists would enhance the impact.

Additional Comments
Conclusion:

The conclusions are well-supported by the data and appropriately cautious regarding limitations.

Emphasizing specific practical guidelines for implementing HC in real-world scenarios would improve the manuscript's utility for practitioners.

Formatting:

Ensure consistent use of abbreviations (e.g., VO2max vs. VO2peak) and provide expanded definitions upon first use.

Overall Recommendation
The manuscript provides valuable insights into the effects of hypoxic conditioning on physical fitness in middle-aged and older adults. With minor revisions, particularly in contextualizing findings, discussing practical applications, and clarifying methodological details, this study would be a strong contribution to the field of geriatric exercise science and sports medicine.",AI-Generated,"Background
Hypoxic conditioning has emerged as a promising intervention for enhancing physiological adaptations. This systematic review and meta-analysis of randomized controlled trials aims to investigate the efficacy of hypoxic conditioning on physical fitness measures in aging populations.

Methods
The Embase, PubMed, Cochrane Library, and Web of Science were searched from inception to November 2024 (Prospero registration: CRD42023474570). The Cochrane Evaluation Tool and Grading of Recommendations Assessment, Development and Evaluation (GRADE) framework were used for risk of bias assessment and evidence certainty evaluation. Mean differences (MD) and standardized mean differences (SMD) and 95% confidence intervals (CI) were calculated using the Review Manager software. Subgroup analysis was performed to explore possible associations between the study characteristics and the effectiveness of the intervention.

Results
A total of 13 randomized controlled trials (RCTs) with 368 subjects were included in the meta-analysis. High certainty evidence found hypoxic conditioning (HC) significantly improved peak oxygen uptake (VO2peak) (SMD = 0.31, 95% CI [0.01–0.61]; P < 0.05), while very low to moderate certainty evidence shown that hypoxic conditioning (HC) have not induced greater changes on functional outcomes (SMD = −0.21, 95% CI [−0.66–0.24]; P > 0.05), muscle strength (SMD = −0.19, 95% CI [−0.63–0.26]; P > 0.05), maximal power output (SMD = 0.29, 95% CI [−0.17–0.76]; P > 0.05), VO2max (SMD = −0.39, 95% CI [−1.12–1.90]; P > 0.05), and exercise workload (MD = −10.07, 95% CI [−34.95–14.80]; P > 0.05).

Conclusion
This study suggests that hypoxia conditioning has a greater effect on enhancing VO2peak compared to equivalent normoxic training in the middle-aged and older population. More high-quality RCTs are needed in the future to explore the optimal oxygen concentration and exercise intensity during hypoxia conditioning."
Experimental study on the impact of Speed-Agility-Quickness Training method on the agility performance of collegiate sanda specialty students,"Basic Reporting
The study is well-structured and clearly written, adhering to standard scientific reporting. The introduction provides a comprehensive background on the Speed, Agility, and Quickness (SAQ) training method and its relevance to sanda athletes, supported by appropriate references. The figures and tables are clearly labeled and effectively summarize the findings. The language is clear, though minor grammatical corrections would enhance readability. The research objective is explicitly stated, and the manuscript adheres to PeerJ's standards.

Experimental Design
The research question is well-defined and aligns with the scope of the study. The methodology is robust, involving a clear division between experimental and control groups with detailed training regimens. The 8-week intervention period is reasonable, and the repeated measures design adds depth to the analysis. However, the manual timing used for certain tests introduces potential biases. Utilizing automated systems would enhance precision. While the sample size is acceptable, future studies should aim for a larger, more diverse cohort to improve generalizability.

Validity of the Findings
The results are compelling, demonstrating significant improvements in agility among the experimental group subjected to SAQ training. The statistical analyses are appropriate, and the findings are supported by effect size indicators. The discussion effectively contextualizes the results within the broader literature. Nonetheless, the reliance on a specific demographic (male collegiate sanda athletes) limits the applicability of the findings. Additionally, the use of stopwatches for timing introduces a notable limitation, acknowledged by the authors.

Additional Comments

The manuscript would benefit from a more extensive exploration of the psychological aspects of SAQ training, particularly how it impacts cognitive and decision-making skills in competitive environments.

Including more diverse agility test indicators could provide a comprehensive assessment of SAQ training's effects.

A discussion on how the findings can be translated into practical training regimens for coaches and athletes in other martial arts disciplines would enhance the study's impact.

Ethical considerations are well-addressed, ensuring compliance with research standards.

Overall, the study makes a significant contribution to understanding the impact of SAQ training in enhancing the agility of sanda athletes and sets a foundation for future research in this domain.",AI-Generated,"Research objective
This study investigates the effects of Speed, Agility, and Quickness (SAQ) training on the agility of collegiate sanda athletes at Henan Normal University.

Research methods
The experimental group (EG) (n = 12, Age: 19.58 ± 1.165, height: 176.592 ± 3.181 cm, weight: 71.38 ± 15.84 kg, training years: 2.92 ± 0.793) was trained by the SAQ training method, and the control group (CG) (n = 12, Age: 19.92 ± 1.084, 177.308 ± 2.171 cm, 71.63 ± 16.80 kg, training years: 2.75 ± 0.754) was trained by traditional agility. The data of the CG group and the EG group were compared by repeated measures of analysis of variance (ANOVA) in different periods of the test indexes (pro agility run, Illinois agility run, compass pointer run, cross quadrant jump, 15s repeated ring jump, and punch-kick combination test) in the CG group and EG group.

Research results
Post-experiment, the EG group showed significant differences in all six agility test indicators after adopting SAQ training. There were no significant differences in the Pro Agility Test, the Compass Pointer Test, and the 15s Repeated Ring Jump after the CG group used traditional agility performance training (P > 0.05). There were significant differences in the Illinois agility test (effect size D = 0.626), the cross quadrant jump test (effect size D = 0.558), and the punch-kick combination test (effect size D = 0.519) in the CG group after the experiment (P < 0.001). Similarly, the EG group showed significant differences in the Illinois agility test (effect size D = 0.894), the cross quadrant jump test (effect size D = 0.852), and the punch-kick combination test (effect size D = 0.896).

Research conclusion
SAQ training significantly enhances the agility performance of collegiate sanda specialty students. The effects of improving agility performance through SAQ training are superior to those achieved with traditional agility training methods."
The acute effects of simulated hypoxic training at different altitudes on oxidative stress and muscle damage in elite long-distance runners,"Basic Reporting:
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The title accurately reflects the study's focus, and the abstract provides a concise summary of the objectives, methods, results, and conclusions. The introduction effectively contextualizes the research within existing literature, highlighting the significance of oxidative stress and muscle damage in high-altitude training. The methods section is detailed, describing participant characteristics, study design, and procedures comprehensively. Results are presented clearly with appropriate figures and tables, and the discussion interprets the findings in relation to the hypothesis and prior research. The conclusion succinctly summarizes the key findings and their implications.

Experimental Design:
The study employs a robust quasi-experimental design with a single-group, four-treatment time series approach, which is appropriate for investigating the acute effects of different altitudes. The use of a crossover design and randomization of altitude conditions minimizes bias and enhances internal validity. The inclusion of elite long-distance runners ensures relevance to the research question, and the standardized diet and controlled environmental conditions (temperature, humidity) strengthen the study's reliability. However, the fixed order of altitude conditions (from sea level to the highest altitude) may introduce order effects, which could have been mitigated by counterbalancing. The sample size, though small, was justified by a power analysis, but generalizability may be limited due to the participants' residence at 1,700 m altitude.

Validity of the Findings:
The findings are valid and supported by appropriate statistical analyses, including repeated measures ANOVA and post-hoc tests. The use of well-established biomarkers (TAS, TOS, MDA, OSI, CK) and standardized measurement tools (e.g., Masimo Radical-7 Pulse Oximeter, Polar H10 heart rate monitor) enhances the credibility of the results. The significant altitude-dependent variations in oxidative stress markers and cardiovascular responses align with existing literature, reinforcing the study's conclusions. However, the lack of significant changes in CK levels, while plausibly explained by the nature of the exercise protocol and athlete adaptations, warrants further investigation. The immediate post-exercise measurement of biomarkers may also miss delayed peaks, suggesting a need for additional time-point assessments in future studies.

Additional Comments:

Strengths:

Comprehensive assessment of oxidative stress and muscle damage markers under controlled hypoxic conditions.

Clear documentation of methodological rigor, including randomization and blinding.

Practical implications for athletes and coaches in optimizing high-altitude training and recovery strategies.

Limitations:

The fixed order of altitude conditions may confound results due to potential acclimatization or fatigue effects.

Limited sample size and homogeneity (elite athletes residing at 1,700 m) may restrict generalizability.

Lack of follow-up measurements for biomarkers that may exhibit delayed responses (e.g., CK, MDA).

Suggestions for Improvement:

Randomize or counterbalance the order of altitude conditions to eliminate order effects.

Include additional time-point measurements (e.g., 24–48 hours post-exercise) to capture delayed biomarker responses.

Expand the participant pool to include athletes from sea-level or lower-altitude regions to enhance generalizability.

Overall Assessment:
The study makes a valuable contribution to understanding the acute effects of high-altitude training on oxidative stress and muscle damage in elite athletes. With minor methodological refinements, the findings could have broader applicability and deeper insights into adaptive mechanisms.",AI-Generated,"Background
Understanding the impact of altitude on muscle damage and oxidative stress is essential for optimizing training and recovery strategies for athletes exposed to high-altitude conditions. Therefore, this study aimed to investigate the effects of acute exercise at different altitudes on oxidative stress and muscle damage.

Methods
A total of twelve elite long-distance runners (mean age: 20.3 ± 1.5 years) from different branches participated in the study. The exercise protocol was the Bruce submaximal treadmill exercise test, which was conducted under three simulated hypoxic conditions (at 1,700 m, 2,450 m, and 3,200 m) and one normoxic condition (sea level). All measurements took place at the same time of the day. After the exercise protocol, 5 ml venous blood samples were taken from the participants, while heart rate and oxygen saturation were monitored at the 3rd, 6th, 9th, and 12th minutes during the exercise.

Results
Significant altitude-dependent variations were observed in oxidative stress markers, with total oxidant status (TOS) (p = 0.017) and malondialdehyde (MDA) (p < 0.001) levels increasing at higher altitudes, while total antioxidant status (TAS) (p < 0.001) exhibited an elevation and oxidative stress index (OSI) (p < 0.001) demonstrated a decline as altitude increased. However, no significant difference was found in creatine kinase (CK, p = 0.059) levels. Additionally, there were significant differences in the oxygen saturation measurement taken at the 3rd (p < 0.001), 6th (p < 0.001), 9th (p < 0.001), and 12th (p < 0.001), minutes following the exercise session. There was no difference in the pulse measurement taken at the 3rd and 12th minutes, but a difference was observed at the 6th and 9th minutes post-exercise (p < 0.01).

Conclusions
In conclusion, the study determined that endurance exercises performed under simulated normobaric hypoxia at different altitudes increased TAS and reduced OSI in elite long-distance runners. The increase in TAS and the reduction in OSI were more pronounced at higher altitudes, particularly at 2,450 m and 3,200 m, compared to sea level. These findings highlight the need for altitude-specific training and recovery strategies to minimize oxidative stress and muscle damage in athletes."
Acute effect of three functional fitness training designs with equalized load on inexperienced and experienced athletes,"Basic Reporting:
The manuscript is well-structured and clearly written, with a logical flow from introduction to conclusion. The title accurately reflects the study's focus, and the abstract provides a concise summary of the objectives, methods, results, and conclusions. The introduction effectively contextualizes the research within existing literature, emphasizing the importance of understanding different functional fitness training (FFT) modalities and their acute effects on athletes of varying experience levels. The methods section is detailed, describing participant characteristics, study design, and procedures comprehensively. Results are presented clearly with appropriate figures and tables, and the discussion interprets the findings in relation to the hypothesis and prior research. The conclusion succinctly summarizes the key findings and their practical implications.

Experimental Design:
The study employs a robust observational design with a crossover approach, where participants perform three different FFT modalities (AMRAP, FT, EMOM) with matched training loads. The inclusion of both inexperienced and experienced athletes adds depth to the analysis. Key strengths include:

Standardization: Workouts were homogenized based on AMRAP performance, ensuring consistent volume and intensity across modalities.

Control: Participants followed a standardized warm-up and rest periods (72 hours) between sessions to minimize carryover effects.

Measurement Tools: Validated tools (e.g., Borg CR-10 for RPE, Polar H10 for HR/HRV, Lactate Scout for LAC) were used to assess physiological and perceptual responses.

Limitations:

Order Effects: The fixed sequence of workouts (AMRAP → FT → EMOM) may introduce bias; randomization or counterbalancing would strengthen internal validity.

Sample Size: While the study met power criteria, a larger sample could enhance generalizability, especially for subgroup analyses (e.g., sex-specific responses).

Ecological Validity: The real-world setting (e.g., competition calendar constraints) limited protocol flexibility, such as reversing the workout order.

Validity of the Findings:
The findings are valid and supported by appropriate statistical analyses (two-way ANOVA, Bonferroni post-hoc tests). Key results align with existing literature:

FT elicited higher lactate and RPE than AMRAP, consistent with its time-pressured nature.

EMOM induced less neuromuscular fatigue (minimal CMJ decline) and lower lactate, likely due to structured rest intervals.

Experience-Level Differences: EG reported lower RPE in EMOM, highlighting the role of pacing strategies in advanced athletes.

Concerns:

Homogenization Method: Basing FT and EMOM workloads on AMRAP performance may not fully equate physiological demands, as FT was completed faster than AMRAP. Future studies could homogenize workloads using FT or EMOM as the reference.

HRV Interpretation: The significant drop in HRV post-exercise suggests high autonomic stress, but the clinical relevance of these acute changes warrants further exploration.

Additional Comments:

Strengths:

Novel comparison of AMRAP, FT, and EMOM with matched loads, addressing a gap in FFT literature.

Practical insights for coaches on tailoring workouts to athlete experience (e.g., EMOM for recovery-focused sessions).

Suggestions for Improvement:

Include longitudinal data to assess chronic adaptations to different modalities.

Explore sex-specific responses more thoroughly, given the unequal distribution (15 males, 10 females).

Clarify whether verbal encouragement was standardized across sessions to minimize motivational bias.

Overall Assessment:
The study provides valuable evidence on acute responses to FFT modalities, with clear implications for programming. Methodological refinements (e.g., randomized order, larger sample) could enhance future work.",AI-Generated,"Background
In the realm of functional fitness training (FFT), three common circuits—as many repetitions or round as possible (AMRAP), for time (FT), and every minute on a minute (EMOM)—are prevalent. We aimed to elucidate the immediate impacts on athletes, considering the experience, when performing three workout modalities with matched training loads.

Methods
Twenty-five healthy men and women, with at least three months of experience in FFT, were allocated into the Inexperienced group (IG) and Experienced group (EG). The cut point for allocating participant in each group was set at 24 months. All of them participated in three workouts (AMRAP, FT and EMOM) with three days of rest. A double comparison was performed between level of experience (IG and EG) and among kinds of training in rating of perceived exertion (RPE), lactate concentration (LAC), countermovement jump (CMJ), heart rate (HR) and heart rate variability (HRV) using ANOVA and post-hoc Bonferroni tests.

Results
Sex was initially analyzed but had no influence, leading to combined group analyses. The workout type significantly impacted performance, with AMRAP showing differences between expertise levels (ES = 0.81, p = .044). RPE varied by workout type (F(2,46) = 11.003; p < .001), with EG reporting FT as the most and EMOM as the least demanding. Lactate levels increased across all workouts, with FT showing the highest and EMOM the lowest levels (ES = 1.05, p < .001). CMJ performance declined post-AMRAP and FT in both groups, but not after EMOM. No expertise-level differences were found in HRmean or HRmax, but HRV changes were influenced by workout type (F(2,46) = 7.381; p < .01) and expertise (F(1,23) = 4.657; p = .034), with significant decreases in HRV after AMRAP and FT for IG.

Conclusion
The study demonstrates that FT produced greater LAC and RPE as compared to an AMRAP, whereas EMOM generated less neuromuscular fatigue and Lac, particularly in EG. These results underscore the importance of individualizing workout selection to expertise level to optimize performance. Future research should explore longitudinal adaptation to different workout types across diverse populations."
"The large mammal fossil fauna of the Cradle of Humankind, South Africa: a review","Basic Reporting
The manuscript provides a comprehensive review of the large mammal fossil fauna from eight key Plio-Pleistocene sites in the Cradle of Humankind, South Africa. The authors effectively summarize the history of faunal reporting, biochronology, and palaeoenvironmental reconstructions, while highlighting gaps and inconsistencies in existing literature. The structure is clear, with well-defined sections, and the inclusion of tables and figures aids in understanding the data. However, the manuscript could benefit from a more concise presentation of some repetitive points, particularly in the site-specific sections.

Experimental Design
The review is methodologically sound, drawing from a wide range of sources, including peer-reviewed articles, theses, and unpublished data. The focus on sites with uranium-lead (U-Pb) dating ensures a robust chronological framework. The authors’ decision to prioritize bovids due to their ecological significance is justified, though the limited discussion of micromammals and birds is a minor drawback. The inclusion of biochronological and geochronological comparisons strengthens the study’s interdisciplinary approach. A more explicit discussion of the limitations of biochronology (e.g., regional vs. local extinctions) would further enhance the design.

Validity of the Findings
The findings are well-supported by the cited literature, and the authors critically evaluate discrepancies between faunal and absolute dating methods. The conclusion that South Africa may have acted as a refugium is intriguing but would benefit from additional evidence, such as genetic or isotopic data, to strengthen the argument. The palaeoenvironmental reconstructions are plausible but occasionally rely heavily on presence/absence data, which can be biased by taphonomic factors. The authors acknowledge these limitations, but a deeper exploration of alternative proxies (e.g., stable isotopes, dental wear) would bolster the interpretations.

Additional Comments
Clarity and Flow: Some sections, particularly the site histories, are overly detailed and could be streamlined for better readability.

Figures and Tables: The figures are informative, but Figure 4 (proposed dates for deposits) is somewhat cluttered. Simplifying or splitting it into subplots might improve clarity.

Future Directions: The call for more robust faunal reporting is commendable. Suggesting specific techniques (e.g., ecomorphology, isotopic analysis) for future studies would be valuable.

Terminology: Ensure consistent use of terms like ""biotidal"" and ""refugium,"" which are central to the discussion but not always clearly defined.

Overall, this review is a significant contribution to paleoanthropology and paleoecology, providing a much-needed synthesis of the Cradle’s faunal record. With minor revisions, it will be a valuable resource for researchers in the field.",AI-Generated,"South Africa’s Cradle of Humankind UNESCO World Heritage Site has remained the single richest source of hominin fossils for over ninety years. While its hominin specimens have been the subject of extensive research, the same is not true for its abundant faunal assemblages, despite their value in Plio-Pleistocene palaeoenvironmental reconstructions. Moreover, precise ages and depositional histories have been historically difficult to assess, though advancements in both relative and absolute dating techniques are changing this. This review explores the history of non-hominin large mammal faunal reporting, palaeoenvironmental reconstructions based on these fauna, and dating histories (with a focus on biochronology) at the following eight fossil-bearing sites of the Cradle that have been radiometrically dated with uranium-lead: Bolt’s Farm, Cooper’s Cave, Drimolen, Haasgat, Hoogland, Malapa, Sterkfontein and Swartkrans. Continued efforts to provide more precise and direct ages for sites using a variety of methods indicate that the bulk of Cradle deposits date to between 3 and 1.4 Ma. We find that, across almost all eight sites, there is little discussion or debate surrounding faunal reports, with some sites described by a single publication. Many of the reports are decades old with little review or reanalysis in the years following, emphasising the need for reviews such as this one. Our analysis of the data indicates that faunal-based paleoenvironmental reconstructions across sites commonly show a trend of wooded landscapes giving way to grasslands. We find that these reconstructions are primarily based on faunal abundance data, despite the availability of many other informative analytical techniques. The findings of this review highlight a need for more extensive and robust faunal reporting, as this will aid in understanding the context of these Cradle sites."
"Phylogenetic relationships of Neogene hamsters (Mammalia, Rodentia, Cricetinae) revealed under Bayesian inference and maximum parsimony","Basic Reporting:
The manuscript is well-structured and clearly written, with a comprehensive introduction that contextualizes the study within the broader debate on cricetine systematics. The methods are described in sufficient detail, allowing for reproducibility, and the results are presented logically, supported by figures and supplementary materials. The discussion effectively interprets the findings in light of previous research, and the conclusion succinctly summarizes the key contributions of the study. The inclusion of supplemental materials (e.g., morphological matrix, phylogenetic trees) enhances transparency.

Experimental Design:
The study employs a robust phylogenetic framework, combining both maximum parsimony and Bayesian inference approaches, which strengthens the validity of the results. The authors conducted thorough model testing, including relaxed-clock Bayesian tip-dating, to estimate divergence times, and they addressed potential biases by identifying and excluding rogue taxa. The use of ancestral state reconstruction and stratigraphic congruence assessments further enriches the analysis. However, the exclusion of Pleistocene cricetine taxa and the limited number of extant species (only two) may introduce some uncertainty in the placement of extant lineages.

Validity of the Findings:
The findings are well-supported by the data and analyses. The congruence between Bayesian and parsimony topologies lends credibility to the proposed phylogenetic relationships. The identification of synapomorphies for key clades and the resolution of taxonomic uncertainties (e.g., the synonymy of Kowalskia with Neocricetodon) are convincing. The divergence time estimates are plausible and align with the fossil record. However, the low posterior probabilities for some clades (e.g., within Neocricetodon) suggest that certain relationships remain unresolved, which the authors appropriately acknowledge.

Additional Comments:

Strengths:

The study provides the first comprehensive phylogenetic analysis of Neogene cricetine hamsters using morphological data, filling a significant gap in the literature.

The combination of multiple analytical methods (Bayesian tip-dating, maximum parsimony, ancestral state reconstruction) is a major strength.

The discussion is thorough, addressing discrepancies with previous studies and proposing well-justified taxonomic revisions.

Suggestions for Improvement:

Include more extant taxa to better contextualize the fossil lineages and improve the accuracy of tip-dating.

Discuss potential limitations of the morphological dataset (e.g., homoplasy) and how they might affect the results.

Provide a clearer justification for the exclusion of Pleistocene taxa, as their inclusion could refine the placement of extant species.

Minor Points:

The dental terminology figure (Fig. 1) is helpful but could be improved with labels directly on the tooth diagrams for clarity.

Some node support values (e.g., bootstrap percentages) are relatively low; the authors could discuss potential reasons (e.g., incomplete sampling, homoplasy).

Overall Evaluation:
This is a significant contribution to the field of mammalian paleontology, offering new insights into the evolutionary history of cricetine rodents. The methodological rigor and thorough discussion make it a valuable reference for future studies.",AI-Generated,"There is an ongoing debate about the internal systematics of today’s group of hamsters (Cricetinae), following new insights that are gained based on molecular data. Regarding the closely related fossil cricetids, however, most studies deal with only a limited number of genera and statements about their possible relationships are rare. In this study, 41 fossil species from the Late Miocene to the Pliocene, belonging to seven extinct cricetine genera, Collimys, Rotundomys, Neocricetodon, Pseudocricetus, Cricetulodon, Apocricetus and Hattomys are analysed in a phylogenetic framework using traditional maximum parsimony and Bayesian inference approaches. Following thorough model testing, a relaxed-clock Bayesian inference analysis is performed under tip-dating to estimate divergence times simultaneously. Furthermore, so-called ‘rogue’ taxa are identified and excluded from the final trees to improve the informative value of the shown relationships. Based on these resulting trees, the fit of the topologies to the stratigraphy is assessed and the ancestral states of the characters are reconstructed under a parsimonious approach and stochastic character mapping. The overall topologies resulting from Bayesian and parsimonious approaches are largely congruent to each other and confirm the monophyly of most of the genera. Additionally, synapomorphies can be identified for each of these genera based on the ancestral state reconstructions. Only Cricetulodon turns out to be paraphyletic, while ‘Cricetulodon’ complicidens is a member of Neocricetodon. Lastly, this work makes a contribution to a debate that went on for decades, as the genus Kowalskia can be confirmed as junior synonym of Neocricetodon."
