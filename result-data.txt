XLNET:

"""
Starting XLNet model training with 5-fold cross-validation and Focal Loss
Model: xlnet-base-cased, Max Length: 384, Batch Size (per device): 16
Effective Batch Size: 32, Eval Batch Size: 32
Learning Rate: 5e-05, Epochs per fold: 6
Loading data...
Successfully loaded data with cp1252 encoding.
Loaded 399 samples with 3 classes: ['AI-Generated' 'Authentic' 'Generic']
Train/CV set size: 339, Test set size: 60
loading file spiece.model from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/spiece.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/tokenizer.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/config.json
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.52.2",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

\n--- Fold 1/5 ---
Map: 100%
 271/271 [00:01<00:00, 186.59 examples/s]
Map: 100%
 68/68 [00:00<00:00, 174.42 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/config.json
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.52.2",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/pytorch_model.bin
Attempting to create safetensors variant
Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
<ipython-input-2-b03fa68760c4>:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 271
  Num Epochs = 6
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 54
  Number of trainable parameters = 117,311,235
Starting training for fold 1...
Safetensors PR exists
 [54/54 06:22, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	0.898800	0.410657	0.426471	0.376614	0.338965	0.426471
2	0.700900	0.433762	0.647059	0.535006	0.472465	0.647059
3	0.555700	0.306249	0.661765	0.567351	0.760861	0.661765
4	0.400100	0.271019	0.676471	0.651852	0.671462	0.676471
5	0.272300	0.286453	0.691176	0.695482	0.704657	0.691176
6	0.197300	0.303150	0.676471	0.651852	0.671462	0.676471
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_1/checkpoint-9
Configuration saved in ./xlnet_results/fold_1/checkpoint-9/config.json
Model weights saved in ./xlnet_results/fold_1/checkpoint-9/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_1/checkpoint-9/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_1/checkpoint-9/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_1/checkpoint-18
Configuration saved in ./xlnet_results/fold_1/checkpoint-18/config.json
Model weights saved in ./xlnet_results/fold_1/checkpoint-18/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_1/checkpoint-18/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_1/checkpoint-18/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_1/checkpoint-27
Configuration saved in ./xlnet_results/fold_1/checkpoint-27/config.json
Model weights saved in ./xlnet_results/fold_1/checkpoint-27/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_1/checkpoint-27/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_1/checkpoint-27/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_1/checkpoint-36
Configuration saved in ./xlnet_results/fold_1/checkpoint-36/config.json
Model weights saved in ./xlnet_results/fold_1/checkpoint-36/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_1/checkpoint-36/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_1/checkpoint-36/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_1/checkpoint-45
Configuration saved in ./xlnet_results/fold_1/checkpoint-45/config.json
Model weights saved in ./xlnet_results/fold_1/checkpoint-45/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_1/checkpoint-45/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_1/checkpoint-45/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_1/checkpoint-54
Configuration saved in ./xlnet_results/fold_1/checkpoint-54/config.json
Model weights saved in ./xlnet_results/fold_1/checkpoint-54/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_1/checkpoint-54/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_1/checkpoint-54/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./xlnet_results/fold_1/checkpoint-45 (score: 0.6954824806449038).
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Evaluating fold 1 on its validation set...
 [3/3 00:01]
Fold 1 - Validation F1: 0.6955, Accuracy: 0.6912
New best model found in fold 1 with F1: 0.6955
\n--- Fold 2/5 ---
Map: 100%
 271/271 [00:00<00:00, 307.95 examples/s]
Map: 100%
 68/68 [00:00<00:00, 310.18 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/config.json
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.52.2",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/pytorch_model.bin
Attempting to create safetensors variant
Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
<ipython-input-2-b03fa68760c4>:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
Using auto half precision backend
Starting training for fold 2...
The following columns in the Training set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 271
  Num Epochs = 6
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 54
  Number of trainable parameters = 117,311,235
Safetensors PR exists
 [54/54 06:49, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	0.909000	0.459542	0.514706	0.349800	0.264922	0.514706
2	0.873600	0.403484	0.544118	0.409948	0.552362	0.544118
3	0.816900	0.425536	0.529412	0.455437	0.596069	0.529412
4	0.751000	0.397527	0.558824	0.475118	0.624036	0.558824
5	0.589600	0.379080	0.602941	0.610351	0.624231	0.602941
6	0.464700	0.378947	0.617647	0.611291	0.611520	0.617647
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_2/checkpoint-9
Configuration saved in ./xlnet_results/fold_2/checkpoint-9/config.json
Model weights saved in ./xlnet_results/fold_2/checkpoint-9/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_2/checkpoint-9/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_2/checkpoint-9/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_2/checkpoint-18
Configuration saved in ./xlnet_results/fold_2/checkpoint-18/config.json
Model weights saved in ./xlnet_results/fold_2/checkpoint-18/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_2/checkpoint-18/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_2/checkpoint-18/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_2/checkpoint-27
Configuration saved in ./xlnet_results/fold_2/checkpoint-27/config.json
Model weights saved in ./xlnet_results/fold_2/checkpoint-27/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_2/checkpoint-27/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_2/checkpoint-27/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_2/checkpoint-36
Configuration saved in ./xlnet_results/fold_2/checkpoint-36/config.json
Model weights saved in ./xlnet_results/fold_2/checkpoint-36/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_2/checkpoint-36/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_2/checkpoint-36/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_2/checkpoint-45
Configuration saved in ./xlnet_results/fold_2/checkpoint-45/config.json
Model weights saved in ./xlnet_results/fold_2/checkpoint-45/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_2/checkpoint-45/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_2/checkpoint-45/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_2/checkpoint-54
Configuration saved in ./xlnet_results/fold_2/checkpoint-54/config.json
Model weights saved in ./xlnet_results/fold_2/checkpoint-54/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_2/checkpoint-54/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_2/checkpoint-54/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./xlnet_results/fold_2/checkpoint-54 (score: 0.6112906701141995).
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Evaluating fold 2 on its validation set...
 [3/3 00:01]
Fold 2 - Validation F1: 0.6113, Accuracy: 0.6176
\n--- Fold 3/5 ---
Map: 100%
 271/271 [00:00<00:00, 304.98 examples/s]
Map: 100%
 68/68 [00:00<00:00, 296.17 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/config.json
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.52.2",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/pytorch_model.bin
Attempting to create safetensors variant
Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
<ipython-input-2-b03fa68760c4>:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 271
  Num Epochs = 6
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 54
  Number of trainable parameters = 117,311,235
Starting training for fold 3...
Safetensors PR exists
 [54/54 06:17, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	0.939500	0.423745	0.411765	0.373239	0.346609	0.411765
2	0.689500	0.404963	0.573529	0.462256	0.416916	0.573529
3	0.578900	0.375732	0.647059	0.632897	0.697041	0.647059
4	0.530600	0.360699	0.588235	0.499224	0.603190	0.588235
5	0.439100	0.393389	0.485294	0.464095	0.686029	0.485294
6	0.382800	0.322979	0.632353	0.626763	0.648734	0.632353
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_3/checkpoint-9
Configuration saved in ./xlnet_results/fold_3/checkpoint-9/config.json
Model weights saved in ./xlnet_results/fold_3/checkpoint-9/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_3/checkpoint-9/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_3/checkpoint-9/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_3/checkpoint-18
Configuration saved in ./xlnet_results/fold_3/checkpoint-18/config.json
Model weights saved in ./xlnet_results/fold_3/checkpoint-18/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_3/checkpoint-18/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_3/checkpoint-18/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_3/checkpoint-27
Configuration saved in ./xlnet_results/fold_3/checkpoint-27/config.json
Model weights saved in ./xlnet_results/fold_3/checkpoint-27/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_3/checkpoint-27/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_3/checkpoint-27/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_3/checkpoint-36
Configuration saved in ./xlnet_results/fold_3/checkpoint-36/config.json
Model weights saved in ./xlnet_results/fold_3/checkpoint-36/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_3/checkpoint-36/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_3/checkpoint-36/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_3/checkpoint-45
Configuration saved in ./xlnet_results/fold_3/checkpoint-45/config.json
Model weights saved in ./xlnet_results/fold_3/checkpoint-45/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_3/checkpoint-45/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_3/checkpoint-45/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_3/checkpoint-54
Configuration saved in ./xlnet_results/fold_3/checkpoint-54/config.json
Model weights saved in ./xlnet_results/fold_3/checkpoint-54/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_3/checkpoint-54/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_3/checkpoint-54/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./xlnet_results/fold_3/checkpoint-27 (score: 0.6328972909106253).
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Evaluating fold 3 on its validation set...
 [3/3 00:01]
Fold 3 - Validation F1: 0.6329, Accuracy: 0.6471
\n--- Fold 4/5 ---
Map: 100%
 271/271 [00:00<00:00, 311.41 examples/s]
Map: 100%
 68/68 [00:00<00:00, 294.08 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/config.json
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.52.2",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/pytorch_model.bin
Attempting to create safetensors variant
Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
<ipython-input-2-b03fa68760c4>:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
Using auto half precision backend
Starting training for fold 4...
The following columns in the Training set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 271
  Num Epochs = 6
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 54
  Number of trainable parameters = 117,311,235
Safetensors PR exists
 [54/54 06:24, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	0.933900	0.426753	0.500000	0.333333	0.250000	0.500000
2	0.848300	0.449921	0.470588	0.405308	0.359069	0.470588
3	0.841400	0.400281	0.602941	0.495150	0.484571	0.602941
4	0.665600	0.315500	0.544118	0.549208	0.570519	0.544118
5	0.564000	0.303607	0.617647	0.558618	0.600218	0.617647
6	0.457600	0.254265	0.647059	0.659165	0.705206	0.647059
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_4/checkpoint-9
Configuration saved in ./xlnet_results/fold_4/checkpoint-9/config.json
Model weights saved in ./xlnet_results/fold_4/checkpoint-9/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_4/checkpoint-9/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_4/checkpoint-9/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_4/checkpoint-18
Configuration saved in ./xlnet_results/fold_4/checkpoint-18/config.json
Model weights saved in ./xlnet_results/fold_4/checkpoint-18/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_4/checkpoint-18/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_4/checkpoint-18/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_4/checkpoint-27
Configuration saved in ./xlnet_results/fold_4/checkpoint-27/config.json
Model weights saved in ./xlnet_results/fold_4/checkpoint-27/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_4/checkpoint-27/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_4/checkpoint-27/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_4/checkpoint-36
Configuration saved in ./xlnet_results/fold_4/checkpoint-36/config.json
Model weights saved in ./xlnet_results/fold_4/checkpoint-36/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_4/checkpoint-36/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_4/checkpoint-36/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_4/checkpoint-45
Configuration saved in ./xlnet_results/fold_4/checkpoint-45/config.json
Model weights saved in ./xlnet_results/fold_4/checkpoint-45/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_4/checkpoint-45/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_4/checkpoint-45/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_4/checkpoint-54
Configuration saved in ./xlnet_results/fold_4/checkpoint-54/config.json
Model weights saved in ./xlnet_results/fold_4/checkpoint-54/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_4/checkpoint-54/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_4/checkpoint-54/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./xlnet_results/fold_4/checkpoint-54 (score: 0.6591645353793691).
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 68
  Batch size = 32
Evaluating fold 4 on its validation set...
 [3/3 00:01]
Fold 4 - Validation F1: 0.6592, Accuracy: 0.6471
\n--- Fold 5/5 ---
Map: 100%
 272/272 [00:00<00:00, 313.45 examples/s]
Map: 100%
 67/67 [00:00<00:00, 285.86 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/config.json
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.52.2",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/pytorch_model.bin
Attempting to create safetensors variant
Safetensors PR exists
Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
<ipython-input-2-b03fa68760c4>:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
Using auto half precision backend
Starting training for fold 5...
The following columns in the Training set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 272
  Num Epochs = 6
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 54
  Number of trainable parameters = 117,311,235
 [54/54 07:00, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	0.951700	0.436914	0.507463	0.391866	0.349861	0.507463
2	0.757700	0.372341	0.507463	0.498668	0.588825	0.507463
3	0.511800	0.367570	0.611940	0.562238	0.634605	0.611940
4	0.399700	0.396010	0.567164	0.570756	0.596042	0.567164
5	0.260800	0.344390	0.582090	0.594657	0.651427	0.582090
6	0.157900	0.297993	0.611940	0.618262	0.635732	0.611940
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 67
  Batch size = 32
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./xlnet_results/fold_5/checkpoint-9
Configuration saved in ./xlnet_results/fold_5/checkpoint-9/config.json
Model weights saved in ./xlnet_results/fold_5/checkpoint-9/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_5/checkpoint-9/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_5/checkpoint-9/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 67
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_5/checkpoint-18
Configuration saved in ./xlnet_results/fold_5/checkpoint-18/config.json
Model weights saved in ./xlnet_results/fold_5/checkpoint-18/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_5/checkpoint-18/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_5/checkpoint-18/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 67
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_5/checkpoint-27
Configuration saved in ./xlnet_results/fold_5/checkpoint-27/config.json
Model weights saved in ./xlnet_results/fold_5/checkpoint-27/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_5/checkpoint-27/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_5/checkpoint-27/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 67
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_5/checkpoint-36
Configuration saved in ./xlnet_results/fold_5/checkpoint-36/config.json
Model weights saved in ./xlnet_results/fold_5/checkpoint-36/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_5/checkpoint-36/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_5/checkpoint-36/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 67
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_5/checkpoint-45
Configuration saved in ./xlnet_results/fold_5/checkpoint-45/config.json
Model weights saved in ./xlnet_results/fold_5/checkpoint-45/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_5/checkpoint-45/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_5/checkpoint-45/special_tokens_map.json
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 67
  Batch size = 32
Saving model checkpoint to ./xlnet_results/fold_5/checkpoint-54
Configuration saved in ./xlnet_results/fold_5/checkpoint-54/config.json
Model weights saved in ./xlnet_results/fold_5/checkpoint-54/model.safetensors
tokenizer config file saved in ./xlnet_results/fold_5/checkpoint-54/tokenizer_config.json
Special tokens file saved in ./xlnet_results/fold_5/checkpoint-54/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./xlnet_results/fold_5/checkpoint-54 (score: 0.6182617567553353).
The following columns in the Evaluation set don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 67
  Batch size = 32
Evaluating fold 5 on its validation set...
 [3/3 00:01]
Fold 5 - Validation F1: 0.6183, Accuracy: 0.6119
\n--- Cross-Validation Summary (Average over folds) ---
avg_eval_loss: 0.3187
avg_eval_accuracy: 0.6430
avg_eval_f1: 0.6434
avg_eval_precision: 0.6708
avg_eval_recall: 0.6430
\nLoading the overall best model from cross-validation...
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/config.json
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.52.2",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlnet-base-cased/snapshots/ceaa69c7bc5e512b5007106a7ccbb7daf24b2c79/pytorch_model.bin
Attempting to create safetensors variant
Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Configuration saved in ./xlnet_classifier_cv/config.json
Saving the best model to ./xlnet_classifier_cv...
Model weights saved in ./xlnet_classifier_cv/model.safetensors
Safetensors PR exists
tokenizer config file saved in ./xlnet_classifier_cv/tokenizer_config.json
Special tokens file saved in ./xlnet_classifier_cv/special_tokens_map.json
Plotting training history of the best fold...
Training history plot saved to xlnet_training_history_cv_best_fold.png
\nEvaluating the best model on the hold-out test set...
Map: 100%
 60/60 [00:00<00:00, 256.38 examples/s]
PyTorch: setting up devices
<ipython-input-2-b03fa68760c4>:477: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  test_trainer = Trainer(
Using auto half precision backend

***** Running Prediction *****
  Num examples = 60
  Batch size = 32
\nTest Set Evaluation Report:
              precision    recall  f1-score   support

AI-Generated     0.9000    0.7500    0.8182        12
   Authentic     0.6667    0.6667    0.6667        30
     Generic     0.4500    0.5000    0.4737        18

    accuracy                         0.6333        60
   macro avg     0.6722    0.6389    0.6528        60
weighted avg     0.6483    0.6333    0.6391        60

Plotting confusion matrix for the test set...
Confusion matrix saved to xlnet_confusion_matrix_cv_test.png
"""

ROBERTA:

"""
Starting RoBERTa model training with 5-fold cross-validation
Model: roberta-base, Max Length: 384, Batch Size: 8 (Train), 16 (Eval)
Gradient Accumulation Steps: 2
Learning Rate: 4e-05, Epochs per fold: 8
Loading data...
Successfully loaded data with cp1252 encoding.
Loaded 399 samples with 3 classes: ['AI-Generated' 'Authentic' 'Generic']
Train/CV set size: 319, Test set size: 80
loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.52.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Using device: cuda
\n==================================================\nTraining Fold 1/5\n==================================================
Map: 100%
 255/255 [00:01<00:00, 166.36 examples/s]
Map: 100%
 64/64 [00:00<00:00, 126.83 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "AI-Generated",
    "1": "Authentic",
    "2": "Generic"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "AI-Generated": 0,
    "Authentic": 1,
    "Generic": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.52.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 255
  Num Epochs = 8
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 128
  Number of trainable parameters = 124,647,939
Starting training for Fold 1...
 [128/128 02:23, Epoch 8/8]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.108700	1.026283	0.515625	0.350838	0.265869	0.515625
2	0.973100	0.960197	0.515625	0.350838	0.265869	0.515625
3	0.934200	0.944366	0.609375	0.502211	0.440582	0.609375
4	0.839000	0.968018	0.593750	0.553586	0.555563	0.593750
5	0.760300	0.916008	0.609375	0.622862	0.671067	0.609375
6	0.648100	0.829347	0.625000	0.577860	0.591846	0.625000
7	0.558700	0.745094	0.703125	0.672632	0.732422	0.703125
8	0.425200	0.792099	0.656250	0.663091	0.683101	0.656250
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-16
Configuration saved in ./roberta_results/fold_1/checkpoint-16/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-16/model.safetensors
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-32
Configuration saved in ./roberta_results/fold_1/checkpoint-32/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-32/model.safetensors
Deleting older checkpoint [roberta_results/fold_1/checkpoint-112] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-48
Configuration saved in ./roberta_results/fold_1/checkpoint-48/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-48/model.safetensors
Deleting older checkpoint [roberta_results/fold_1/checkpoint-16] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-64
Configuration saved in ./roberta_results/fold_1/checkpoint-64/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-64/model.safetensors
Deleting older checkpoint [roberta_results/fold_1/checkpoint-32] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-80
Configuration saved in ./roberta_results/fold_1/checkpoint-80/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-80/model.safetensors
Deleting older checkpoint [roberta_results/fold_1/checkpoint-48] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-96
Configuration saved in ./roberta_results/fold_1/checkpoint-96/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-96/model.safetensors
Deleting older checkpoint [roberta_results/fold_1/checkpoint-64] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-112
Configuration saved in ./roberta_results/fold_1/checkpoint-112/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-112/model.safetensors
Deleting older checkpoint [roberta_results/fold_1/checkpoint-80] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_1/checkpoint-128
Configuration saved in ./roberta_results/fold_1/checkpoint-128/config.json
Model weights saved in ./roberta_results/fold_1/checkpoint-128/model.safetensors
Deleting older checkpoint [roberta_results/fold_1/checkpoint-96] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./roberta_results/fold_1/checkpoint-112 (score: 0.6726317663817664).
Deleting older checkpoint [roberta_results/fold_1/checkpoint-128] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Evaluating best model of Fold 1 on its validation set...
 [4/4 00:00]
Fold 1 Validation: F1=0.6726, Accuracy=0.7031
New best model found in Fold 1 with Val F1: 0.6726
\n==================================================\nTraining Fold 2/5\n==================================================
Map: 100%
 255/255 [00:01<00:00, 171.54 examples/s]
Map: 100%
 64/64 [00:00<00:00, 78.61 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "AI-Generated",
    "1": "Authentic",
    "2": "Generic"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "AI-Generated": 0,
    "Authentic": 1,
    "Generic": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.52.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 255
  Num Epochs = 8
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 128
  Number of trainable parameters = 124,647,939
Starting training for Fold 2...
 [112/128 01:59 < 00:17, 0.92 it/s, Epoch 7/8]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.086400	1.042168	0.500000	0.333333	0.250000	0.500000
2	1.022200	0.975628	0.500000	0.333333	0.250000	0.500000
3	0.928400	0.847935	0.578125	0.576206	0.596719	0.578125
4	0.729900	0.719978	0.656250	0.612397	0.651292	0.656250
5	0.674600	0.849169	0.593750	0.554330	0.556380	0.593750
6	0.546400	0.789369	0.625000	0.600879	0.598916	0.625000
7	0.471000	0.776360	0.609375	0.581355	0.622303	0.609375
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_2/checkpoint-16
Configuration saved in ./roberta_results/fold_2/checkpoint-16/config.json
Model weights saved in ./roberta_results/fold_2/checkpoint-16/model.safetensors
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_2/checkpoint-32
Configuration saved in ./roberta_results/fold_2/checkpoint-32/config.json
Model weights saved in ./roberta_results/fold_2/checkpoint-32/model.safetensors
Deleting older checkpoint [roberta_results/fold_2/checkpoint-64] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_2/checkpoint-48
Configuration saved in ./roberta_results/fold_2/checkpoint-48/config.json
Model weights saved in ./roberta_results/fold_2/checkpoint-48/model.safetensors
Deleting older checkpoint [roberta_results/fold_2/checkpoint-16] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_2/checkpoint-64
Configuration saved in ./roberta_results/fold_2/checkpoint-64/config.json
Model weights saved in ./roberta_results/fold_2/checkpoint-64/model.safetensors
Deleting older checkpoint [roberta_results/fold_2/checkpoint-32] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_2/checkpoint-80
Configuration saved in ./roberta_results/fold_2/checkpoint-80/config.json
Model weights saved in ./roberta_results/fold_2/checkpoint-80/model.safetensors
Deleting older checkpoint [roberta_results/fold_2/checkpoint-48] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_2/checkpoint-96
Configuration saved in ./roberta_results/fold_2/checkpoint-96/config.json
Model weights saved in ./roberta_results/fold_2/checkpoint-96/model.safetensors
Deleting older checkpoint [roberta_results/fold_2/checkpoint-80] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_2/checkpoint-112
Configuration saved in ./roberta_results/fold_2/checkpoint-112/config.json
Model weights saved in ./roberta_results/fold_2/checkpoint-112/model.safetensors
Deleting older checkpoint [roberta_results/fold_2/checkpoint-96] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./roberta_results/fold_2/checkpoint-64 (score: 0.6123973371152788).
Deleting older checkpoint [roberta_results/fold_2/checkpoint-112] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Evaluating best model of Fold 2 on its validation set...
 [4/4 00:00]
Fold 2 Validation: F1=0.6124, Accuracy=0.6562
\n==================================================\nTraining Fold 3/5\n==================================================
Map: 100%
 255/255 [00:01<00:00, 170.18 examples/s]
Map: 100%
 64/64 [00:00<00:00, 134.13 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "AI-Generated",
    "1": "Authentic",
    "2": "Generic"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "AI-Generated": 0,
    "Authentic": 1,
    "Generic": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.52.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 255
  Num Epochs = 8
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 128
  Number of trainable parameters = 124,647,939
Starting training for Fold 3...
 [128/128 02:09, Epoch 8/8]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.075800	1.038597	0.500000	0.333333	0.250000	0.500000
2	1.030400	1.011078	0.500000	0.333333	0.250000	0.500000
3	0.981800	0.916765	0.593750	0.483845	0.478987	0.593750
4	0.785500	0.705103	0.578125	0.500431	0.462813	0.578125
5	0.609500	0.742791	0.640625	0.643792	0.650029	0.640625
6	0.430100	0.830479	0.640625	0.640625	0.660833	0.640625
7	0.269900	1.032565	0.593750	0.591775	0.635345	0.593750
8	0.175900	1.079882	0.593750	0.594499	0.605497	0.593750
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-16
Configuration saved in ./roberta_results/fold_3/checkpoint-16/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-16/model.safetensors
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-32
Configuration saved in ./roberta_results/fold_3/checkpoint-32/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-32/model.safetensors
Deleting older checkpoint [roberta_results/fold_3/checkpoint-128] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-48
Configuration saved in ./roberta_results/fold_3/checkpoint-48/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-48/model.safetensors
Deleting older checkpoint [roberta_results/fold_3/checkpoint-16] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-64
Configuration saved in ./roberta_results/fold_3/checkpoint-64/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-64/model.safetensors
Deleting older checkpoint [roberta_results/fold_3/checkpoint-32] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-80
Configuration saved in ./roberta_results/fold_3/checkpoint-80/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-80/model.safetensors
Deleting older checkpoint [roberta_results/fold_3/checkpoint-48] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-96
Configuration saved in ./roberta_results/fold_3/checkpoint-96/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-96/model.safetensors
Deleting older checkpoint [roberta_results/fold_3/checkpoint-64] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-112
Configuration saved in ./roberta_results/fold_3/checkpoint-112/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-112/model.safetensors
Deleting older checkpoint [roberta_results/fold_3/checkpoint-96] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_3/checkpoint-128
Configuration saved in ./roberta_results/fold_3/checkpoint-128/config.json
Model weights saved in ./roberta_results/fold_3/checkpoint-128/model.safetensors
Deleting older checkpoint [roberta_results/fold_3/checkpoint-112] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./roberta_results/fold_3/checkpoint-80 (score: 0.6437924830067973).
Deleting older checkpoint [roberta_results/fold_3/checkpoint-128] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Evaluating best model of Fold 3 on its validation set...
 [4/4 00:00]
Fold 3 Validation: F1=0.6438, Accuracy=0.6406
\n==================================================\nTraining Fold 4/5\n==================================================
Map: 100%
 255/255 [00:01<00:00, 169.68 examples/s]
Map: 100%
 64/64 [00:00<00:00, 143.81 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "AI-Generated",
    "1": "Authentic",
    "2": "Generic"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "AI-Generated": 0,
    "Authentic": 1,
    "Generic": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.52.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 255
  Num Epochs = 8
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 128
  Number of trainable parameters = 124,647,939
Starting training for Fold 4...
 [128/128 02:10, Epoch 8/8]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.074700	1.035461	0.500000	0.333333	0.250000	0.500000
2	1.047500	1.019745	0.500000	0.333333	0.250000	0.500000
3	1.024700	0.971546	0.500000	0.333333	0.250000	0.500000
4	0.908900	0.825207	0.609375	0.501738	0.483827	0.609375
5	0.703200	0.785315	0.515625	0.503465	0.639805	0.515625
6	0.504200	0.849427	0.578125	0.571343	0.644553	0.578125
7	0.412600	0.790448	0.703125	0.698351	0.700372	0.703125
8	0.297900	0.843606	0.656250	0.657790	0.668909	0.656250
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-16
Configuration saved in ./roberta_results/fold_4/checkpoint-16/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-16/model.safetensors
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-32
Configuration saved in ./roberta_results/fold_4/checkpoint-32/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-32/model.safetensors
Deleting older checkpoint [roberta_results/fold_4/checkpoint-80] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-48
Configuration saved in ./roberta_results/fold_4/checkpoint-48/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-48/model.safetensors
Deleting older checkpoint [roberta_results/fold_4/checkpoint-32] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-64
Configuration saved in ./roberta_results/fold_4/checkpoint-64/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-64/model.safetensors
Deleting older checkpoint [roberta_results/fold_4/checkpoint-16] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-80
Configuration saved in ./roberta_results/fold_4/checkpoint-80/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-80/model.safetensors
Deleting older checkpoint [roberta_results/fold_4/checkpoint-48] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-96
Configuration saved in ./roberta_results/fold_4/checkpoint-96/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-96/model.safetensors
Deleting older checkpoint [roberta_results/fold_4/checkpoint-64] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-112
Configuration saved in ./roberta_results/fold_4/checkpoint-112/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-112/model.safetensors
Deleting older checkpoint [roberta_results/fold_4/checkpoint-80] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_4/checkpoint-128
Configuration saved in ./roberta_results/fold_4/checkpoint-128/config.json
Model weights saved in ./roberta_results/fold_4/checkpoint-128/model.safetensors
Deleting older checkpoint [roberta_results/fold_4/checkpoint-96] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./roberta_results/fold_4/checkpoint-112 (score: 0.6983505674243163).
Deleting older checkpoint [roberta_results/fold_4/checkpoint-128] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 64
  Batch size = 16
Evaluating best model of Fold 4 on its validation set...
 [4/4 00:00]
Fold 4 Validation: F1=0.6984, Accuracy=0.7031
New best model found in Fold 4 with Val F1: 0.6984
\n==================================================\nTraining Fold 5/5\n==================================================
Map: 100%
 256/256 [00:01<00:00, 167.62 examples/s]
Map: 100%
 63/63 [00:00<00:00, 127.83 examples/s]
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "AI-Generated",
    "1": "Authentic",
    "2": "Generic"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "AI-Generated": 0,
    "Authentic": 1,
    "Generic": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.52.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 256
  Num Epochs = 8
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 128
  Number of trainable parameters = 124,647,939
Starting training for Fold 5...
 [128/128 02:28, Epoch 8/8]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.085700	1.035249	0.507937	0.342189	0.257999	0.507937
2	1.044200	0.968525	0.507937	0.342189	0.257999	0.507937
3	0.966400	0.963080	0.507937	0.342189	0.257999	0.507937
4	0.852100	0.758161	0.587302	0.530945	0.535053	0.587302
5	0.650900	0.816545	0.523810	0.520427	0.520377	0.523810
6	0.541100	1.108818	0.523810	0.495712	0.641876	0.523810
7	0.619600	0.847349	0.571429	0.565494	0.614278	0.571429
8	0.368800	0.905620	0.571429	0.569866	0.568947	0.571429
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-16
Configuration saved in ./roberta_results/fold_5/checkpoint-16/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-16/model.safetensors
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-32
Configuration saved in ./roberta_results/fold_5/checkpoint-32/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-32/model.safetensors
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-48
Configuration saved in ./roberta_results/fold_5/checkpoint-48/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-48/model.safetensors
Deleting older checkpoint [roberta_results/fold_5/checkpoint-32] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-64
Configuration saved in ./roberta_results/fold_5/checkpoint-64/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-64/model.safetensors
Deleting older checkpoint [roberta_results/fold_5/checkpoint-16] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-80
Configuration saved in ./roberta_results/fold_5/checkpoint-80/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-80/model.safetensors
Deleting older checkpoint [roberta_results/fold_5/checkpoint-48] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-96
Configuration saved in ./roberta_results/fold_5/checkpoint-96/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-96/model.safetensors
Deleting older checkpoint [roberta_results/fold_5/checkpoint-80] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-112
Configuration saved in ./roberta_results/fold_5/checkpoint-112/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-112/model.safetensors
Deleting older checkpoint [roberta_results/fold_5/checkpoint-64] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
Saving model checkpoint to ./roberta_results/fold_5/checkpoint-128
Configuration saved in ./roberta_results/fold_5/checkpoint-128/config.json
Model weights saved in ./roberta_results/fold_5/checkpoint-128/model.safetensors
Deleting older checkpoint [roberta_results/fold_5/checkpoint-96] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./roberta_results/fold_5/checkpoint-128 (score: 0.5698664651045603).
Deleting older checkpoint [roberta_results/fold_5/checkpoint-112] due to args.save_total_limit
The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Paper Title, text, Review Type, Abstract, Review Text. If Paper Title, text, Review Type, Abstract, Review Text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.

***** Running Evaluation *****
  Num examples = 63
  Batch size = 16
Evaluating best model of Fold 5 on its validation set...
 [4/4 00:00]
Fold 5 Validation: F1=0.5699, Accuracy=0.5714
\nAverage Cross-Validation Results:
  Average eval_loss: 0.7808
  Average eval_accuracy: 0.6549
  Average eval_f1: 0.6394
  Average eval_precision: 0.6606
  Average eval_recall: 0.6549
\nLoading the overall best model for final evaluation...
loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "AI-Generated",
    "1": "Authentic",
    "2": "Generic"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "AI-Generated": 0,
    "Authentic": 1,
    "Generic": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.52.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training history plot saved to roberta_training_history_cv_best_fold.png
\nPreparing test set for final evaluation...
Map: 100%
 80/80 [00:00<00:00, 125.68 examples/s]
Final Test Set Evaluation: 100%
 5/5 [00:01<00:00,  2.87it/s]
  Test Set: Accuracy=0.7125, F1=0.7059, Precision=0.7117, Recall=0.7125
Configuration saved in ./roberta_classifier_cv/config.json
Confusion matrix plot saved to roberta_confusion_matrix_cv_test.png
\nTest Set Performance Report (Overall Best Model):
              precision    recall  f1-score   support

AI-Generated       0.87      0.81      0.84        16
   Authentic       0.69      0.80      0.74        41
     Generic       0.65      0.48      0.55        23

    accuracy                           0.71        80
   macro avg       0.73      0.70      0.71        80
weighted avg       0.71      0.71      0.71        80

\nTest Set Performance Summary (Overall Best Model):
  AI-Generated: F1=0.8387, Precision=0.8667, Recall=0.8125
  Authentic: F1=0.7416, Precision=0.6875, Recall=0.8049
  Generic: F1=0.5500, Precision=0.6471, Recall=0.4783
\n  Overall Test: F1=0.7059, Accuracy=0.7125
\nSaving overall best model and tokenizer...
Model weights saved in ./roberta_classifier_cv/model.safetensors
tokenizer config file saved in ./roberta_classifier_cv/tokenizer_config.json
Special tokens file saved in ./roberta_classifier_cv/special_tokens_map.json
"""


DISTIL-BERT:

"""
Starting DistilBERT model training with 5-fold cross-validation.
Model: distilbert-base-uncased, Max Length: 384, Batch Size: 16
Learning Rate: 3e-05, Epochs per fold: 6
Loading and preprocessing data...
Loaded 399 samples with 3 classes: ['AI-Generated' 'Authentic' 'Generic']
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Using device: cuda
\n==================================================\nTraining Fold 1/5\n==================================================
Tokenizing datasets for Fold 1...
Map: 100%
 255/255 [00:00<00:00, 528.23 examples/s]
Map: 100%
 64/64 [00:00<00:00, 469.99 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Starting training for Fold 1...
 [96/96 00:44, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.034900	0.977623	0.515625	0.350838	0.265869	0.515625
2	0.934300	0.866596	0.625000	0.520615	0.468056	0.625000
3	0.803300	0.788883	0.640625	0.552462	0.740196	0.640625
4	0.687100	0.776808	0.640625	0.571296	0.730329	0.640625
5	0.612700	0.757717	0.671875	0.613364	0.758977	0.671875
6	0.578200	0.760914	0.718750	0.688138	0.747494	0.718750
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Evaluating best model of Fold 1 on its validation set...
 [2/2 00:00]
Fold 1 Validation: F1=0.6881, Accuracy=0.7188
New best model found in Fold 1 with Val F1: 0.6881
\n==================================================\nTraining Fold 2/5\n==================================================
Tokenizing datasets for Fold 2...
Map: 100%
 255/255 [00:00<00:00, 447.75 examples/s]
Map: 100%
 64/64 [00:00<00:00, 463.74 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Starting training for Fold 2...
 [96/96 01:05, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.011500	0.954399	0.500000	0.333333	0.250000	0.500000
2	0.887000	0.886215	0.578125	0.464496	0.474311	0.578125
3	0.771700	0.792347	0.687500	0.628111	0.807692	0.687500
4	0.662400	0.738750	0.687500	0.640642	0.736639	0.687500
5	0.583900	0.718414	0.671875	0.627207	0.655320	0.671875
6	0.535400	0.703655	0.703125	0.673899	0.722183	0.703125
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Evaluating best model of Fold 2 on its validation set...
 [2/2 00:00]
Fold 2 Validation: F1=0.6739, Accuracy=0.7031
\n==================================================\nTraining Fold 3/5\n==================================================
Tokenizing datasets for Fold 3...
Map: 100%
 255/255 [00:00<00:00, 501.67 examples/s]
Map: 100%
 64/64 [00:00<00:00, 476.79 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Starting training for Fold 3...
 [96/96 01:33, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.029100	0.970848	0.500000	0.333333	0.250000	0.500000
2	0.930700	0.882786	0.562500	0.443414	0.469792	0.562500
3	0.829900	0.831921	0.640625	0.598633	0.695159	0.640625
4	0.719700	0.773867	0.671875	0.646294	0.679167	0.671875
5	0.621000	0.748499	0.671875	0.662678	0.680054	0.671875
6	0.565700	0.742212	0.640625	0.635658	0.654167	0.640625
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Evaluating best model of Fold 3 on its validation set...
 [2/2 00:00]
Fold 3 Validation: F1=0.6627, Accuracy=0.6719
\n==================================================\nTraining Fold 4/5\n==================================================
Tokenizing datasets for Fold 4...
Map: 100%
 255/255 [00:00<00:00, 469.22 examples/s]
Map: 100%
 64/64 [00:00<00:00, 465.69 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Starting training for Fold 4...
 [96/96 01:19, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.011200	0.981846	0.500000	0.333333	0.250000	0.500000
2	0.910200	0.898193	0.625000	0.550325	0.590545	0.625000
3	0.789800	0.918638	0.609375	0.517528	0.780702	0.609375
4	0.682900	0.809565	0.656250	0.634264	0.658381	0.656250
5	0.594500	0.803768	0.640625	0.629562	0.629220	0.640625
6	0.541600	0.796362	0.640625	0.622255	0.640394	0.640625
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Evaluating best model of Fold 4 on its validation set...
 [2/2 00:00]
Fold 4 Validation: F1=0.6343, Accuracy=0.6562
\n==================================================\nTraining Fold 5/5\n==================================================
Tokenizing datasets for Fold 5...
Map: 100%
 256/256 [00:00<00:00, 467.78 examples/s]
Map: 100%
 63/63 [00:00<00:00, 448.94 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Starting training for Fold 5...
 [96/96 01:14, Epoch 6/6]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	1.010300	0.973710	0.507937	0.342189	0.257999	0.507937
2	0.908900	0.887796	0.650794	0.575795	0.793063	0.650794
3	0.776600	0.836000	0.634921	0.538370	0.747497	0.634921
4	0.672700	0.788001	0.634921	0.579191	0.691643	0.634921
5	0.591300	0.785594	0.634921	0.579191	0.691643	0.634921
6	0.540000	0.771967	0.619048	0.567619	0.605820	0.619048
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Evaluating best model of Fold 5 on its validation set...
 [2/2 00:00]
Fold 5 Validation: F1=0.5792, Accuracy=0.6349
\nAverage Cross-Validation Results:
  Average eval_loss: 0.7621
  Average eval_accuracy: 0.6770
  Average eval_f1: 0.6476
  Average eval_precision: 0.7000
  Average eval_recall: 0.6770
\nLoading the overall best model for final evaluation...
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
\nPreparing test set for final evaluation...
Map: 100%
 80/80 [00:00<00:00, 444.99 examples/s]
Evaluating overall best model on the Test set...
Final Test Set Evaluation: 100%
 3/3 [00:00<00:00,  3.60it/s]
  Test Set: Accuracy=0.6750, F1=0.6106, Precision=0.7033, Recall=0.6750
\nTest Set Performance Report (Overall Best Model):
              precision    recall  f1-score   support

AI-Generated       0.80      0.75      0.77        16
   Authentic       0.64      0.95      0.76        41
     Generic       0.75      0.13      0.22        23

    accuracy                           0.68        80
   macro avg       0.73      0.61      0.59        80
weighted avg       0.70      0.68      0.61        80

\nTest Set Performance Summary (Overall Best Model):
  AI-Generated: F1=0.7742, Precision=0.8000, Recall=0.7500
  Authentic: F1=0.7647, Precision=0.6393, Recall=0.9512
  Generic: F1=0.2222, Precision=0.7500, Recall=0.1304
\n  Overall Test: F1=0.6106, Accuracy=0.6750
\nSaving overall best model and tokenizer...
\nTraining, cross-validation, and final test evaluation complete.
Best model and tokenizer saved to ./distilbert_classifier_cv
Visualization files saved: distilbert_training_history_cv_best_fold.png, distilbert_confusion_matrix_cv_test.png
"""

BERT:

"""
Starting BERT model training with 5-fold cross-validation.
Model: bert-base-uncased, Max Length: 384, Batch Size: 32
Learning Rate: 4e-05, Epochs per fold: 7
Using device: cuda
\n==================================================\nTraining Fold 1/5\n==================================================
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Fold 1, Epoch 1/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 1, Epoch 1: Train Loss: 1.0318, Val Loss: 1.0052, Val Acc: 0.5156, Val F1: 0.3508
Fold 1, Epoch 2/7: 100%|██████████| 8/8 [00:15<00:00,  1.96s/it]
Fold 1, Epoch 2: Train Loss: 1.0068, Val Loss: 0.9798, Val Acc: 0.5156, Val F1: 0.3508
Fold 1, Epoch 3/7: 100%|██████████| 8/8 [00:15<00:00,  1.94s/it]
Fold 1, Epoch 3: Train Loss: 0.9565, Val Loss: 0.9426, Val Acc: 0.5156, Val F1: 0.3508
Fold 1, Epoch 4/7: 100%|██████████| 8/8 [00:15<00:00,  1.91s/it]
Fold 1, Epoch 4: Train Loss: 0.8123, Val Loss: 0.8775, Val Acc: 0.6094, Val F1: 0.5135
Fold 1, Epoch 5/7: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it]
Fold 1, Epoch 5: Train Loss: 0.6564, Val Loss: 0.8432, Val Acc: 0.6406, Val F1: 0.5807
Fold 1, Epoch 6/7: 100%|██████████| 8/8 [00:14<00:00,  1.87s/it]
Fold 1, Epoch 6: Train Loss: 0.4927, Val Loss: 0.8426, Val Acc: 0.6562, Val F1: 0.6384
Fold 1, Epoch 7/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 1, Epoch 7: Train Loss: 0.3169, Val Loss: 0.8729, Val Acc: 0.6250, Val F1: 0.6171
New best model found in Fold 1 with F1: 0.6171
\n==================================================\nTraining Fold 2/5\n==================================================
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Fold 2, Epoch 1/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 2, Epoch 1: Train Loss: 1.0473, Val Loss: 1.0238, Val Acc: 0.5000, Val F1: 0.3333
Fold 2, Epoch 2/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 2, Epoch 2: Train Loss: 1.0208, Val Loss: 1.0082, Val Acc: 0.5000, Val F1: 0.3333
Fold 2, Epoch 3/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 2, Epoch 3: Train Loss: 0.9518, Val Loss: 0.9393, Val Acc: 0.5625, Val F1: 0.4514
Fold 2, Epoch 4/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 2, Epoch 4: Train Loss: 0.8367, Val Loss: 0.9001, Val Acc: 0.5938, Val F1: 0.4838
Fold 2, Epoch 5/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 2, Epoch 5: Train Loss: 0.6845, Val Loss: 0.8222, Val Acc: 0.6094, Val F1: 0.5018
Fold 2, Epoch 6/7: 100%|██████████| 8/8 [00:15<00:00,  1.90s/it]
Fold 2, Epoch 6: Train Loss: 0.5146, Val Loss: 0.9688, Val Acc: 0.5312, Val F1: 0.4554
Fold 2, Epoch 7/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 2, Epoch 7: Train Loss: 0.3372, Val Loss: 0.9919, Val Acc: 0.5625, Val F1: 0.5563
\n==================================================\nTraining Fold 3/5\n==================================================
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Fold 3, Epoch 1/7: 100%|██████████| 8/8 [00:15<00:00,  1.90s/it]
Fold 3, Epoch 1: Train Loss: 1.0386, Val Loss: 1.0012, Val Acc: 0.5000, Val F1: 0.3333
Fold 3, Epoch 2/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 3, Epoch 2: Train Loss: 0.9414, Val Loss: 0.9971, Val Acc: 0.4688, Val F1: 0.3889
Fold 3, Epoch 3/7: 100%|██████████| 8/8 [00:15<00:00,  1.90s/it]
Fold 3, Epoch 3: Train Loss: 0.8137, Val Loss: 0.8341, Val Acc: 0.5156, Val F1: 0.4574
Fold 3, Epoch 4/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 3, Epoch 4: Train Loss: 0.6570, Val Loss: 0.8075, Val Acc: 0.5625, Val F1: 0.5338
Fold 3, Epoch 5/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 3, Epoch 5: Train Loss: 0.5503, Val Loss: 0.9428, Val Acc: 0.5156, Val F1: 0.4585
Fold 3, Epoch 6/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 3, Epoch 6: Train Loss: 0.4197, Val Loss: 0.9366, Val Acc: 0.5156, Val F1: 0.5157
Fold 3, Epoch 7/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 3, Epoch 7: Train Loss: 0.2657, Val Loss: 1.1558, Val Acc: 0.5312, Val F1: 0.5372
\n==================================================\nTraining Fold 4/5\n==================================================
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Fold 4, Epoch 1/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 4, Epoch 1: Train Loss: 1.0551, Val Loss: 1.0317, Val Acc: 0.5000, Val F1: 0.3333
Fold 4, Epoch 2/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 4, Epoch 2: Train Loss: 1.0235, Val Loss: 1.0121, Val Acc: 0.5000, Val F1: 0.3333
Fold 4, Epoch 3/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 4, Epoch 3: Train Loss: 0.9565, Val Loss: 0.9706, Val Acc: 0.5156, Val F1: 0.4279
Fold 4, Epoch 4/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 4, Epoch 4: Train Loss: 0.8249, Val Loss: 0.9331, Val Acc: 0.4531, Val F1: 0.4201
Fold 4, Epoch 5/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 4, Epoch 5: Train Loss: 0.6405, Val Loss: 0.8880, Val Acc: 0.4688, Val F1: 0.4745
Fold 4, Epoch 6/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 4, Epoch 6: Train Loss: 0.4313, Val Loss: 1.1462, Val Acc: 0.5000, Val F1: 0.4750
Fold 4, Epoch 7/7: 100%|██████████| 8/8 [00:15<00:00,  1.90s/it]
Fold 4, Epoch 7: Train Loss: 0.2719, Val Loss: 1.2836, Val Acc: 0.5156, Val F1: 0.4919
\n==================================================\nTraining Fold 5/5\n==================================================
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Fold 5, Epoch 1/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 5, Epoch 1: Train Loss: 1.0318, Val Loss: 1.0067, Val Acc: 0.5079, Val F1: 0.3422
Fold 5, Epoch 2/7: 100%|██████████| 8/8 [00:15<00:00,  1.91s/it]
Fold 5, Epoch 2: Train Loss: 0.9898, Val Loss: 1.0190, Val Acc: 0.5397, Val F1: 0.4797
Fold 5, Epoch 3/7: 100%|██████████| 8/8 [00:15<00:00,  1.90s/it]
Fold 5, Epoch 3: Train Loss: 0.8952, Val Loss: 0.9895, Val Acc: 0.5397, Val F1: 0.4265
Fold 5, Epoch 4/7: 100%|██████████| 8/8 [00:15<00:00,  1.91s/it]
Fold 5, Epoch 4: Train Loss: 0.7213, Val Loss: 0.9490, Val Acc: 0.5714, Val F1: 0.5633
Fold 5, Epoch 5/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 5, Epoch 5: Train Loss: 0.6076, Val Loss: 0.9715, Val Acc: 0.6032, Val F1: 0.5878
Fold 5, Epoch 6/7: 100%|██████████| 8/8 [00:15<00:00,  1.88s/it]
Fold 5, Epoch 6: Train Loss: 0.4451, Val Loss: 1.0946, Val Acc: 0.6349, Val F1: 0.5650
Fold 5, Epoch 7/7: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]
Fold 5, Epoch 7: Train Loss: 0.3768, Val Loss: 1.0727, Val Acc: 0.5873, Val F1: 0.5713
\nAverage Cross-Validation Results:
  Average val_loss: 1.0754
  Average val_accuracy: 0.5643
  Average val_f1: 0.5548
  Average val_precision: 0.5936
  Average val_recall: 0.5643
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
\nSaving best model and tokenizer...
\nPerforming final evaluation on the Test set...
Evaluating on Test Set: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
\nTest Set Performance Report:
              precision    recall  f1-score   support

AI-Generated       0.90      0.56      0.69        16
   Authentic       0.68      0.78      0.73        41
     Generic       0.43      0.43      0.43        23

    accuracy                           0.64        80
   macro avg       0.67      0.59      0.62        80
weighted avg       0.65      0.64      0.64        80

\nTest Set Performance Summary:
  AI-Generated: F1=0.6923, Precision=0.9000, Recall=0.5625
  Authentic: F1=0.7273, Precision=0.6809, Recall=0.7805
  Generic: F1=0.4348, Precision=0.4348, Recall=0.4348
\n  Overall Test: F1=0.6362, Accuracy=0.6375
\nTraining, cross-validation, and final test evaluation complete.
Best model and tokenizer saved to ./bert_classifier_cv
Visualization files saved: bert_training_history_cv_best_fold.png, bert_confusion_matrix_cv_test.png
"""



